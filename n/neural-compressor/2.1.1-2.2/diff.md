# Comparing `tmp/neural_compressor-2.1.1.tar.gz` & `tmp/neural_compressor-2.2.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "neural_compressor-2.1.1.tar", last modified: Thu May 11 02:51:24 2023, max compression
+gzip compressed data, was "neural_compressor-2.2.tar", last modified: Wed Jun 21 12:28:59 2023, max compression
```

## Comparing `neural_compressor-2.1.1.tar` & `neural_compressor-2.2.tar`

### file list

```diff
@@ -1,516 +1,577 @@
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.764222 neural_compressor-2.1.1/
--rw-r--r--   0 root         (0) root         (0)    11360 2023-05-11 02:51:06.000000 neural_compressor-2.1.1/LICENSE
--rw-r--r--   0 root         (0) root         (0)    11458 2023-05-11 02:51:24.764222 neural_compressor-2.1.1/PKG-INFO
--rw-r--r--   0 root         (0) root         (0)    10704 2023-05-11 02:51:06.000000 neural_compressor-2.1.1/README.md
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.720218 neural_compressor-2.1.1/neural_coder/
--rw-r--r--   0 root         (0) root         (0)      748 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/__init__.py
--rw-r--r--   0 root         (0) root         (0)      668 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/__main__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.724218 neural_compressor-2.1.1/neural_coder/backends/
--rw-r--r--   0 root         (0) root         (0)      583 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1382 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/intel_extension_for_transformers.yaml
--rw-r--r--   0 root         (0) root         (0)     1114 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/keras_inc.yaml
--rw-r--r--   0 root         (0) root         (0)     1038 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_bf16.yaml
--rw-r--r--   0 root         (0) root         (0)     1058 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_bf16_channels_last.yaml
--rw-r--r--   0 root         (0) root         (0)     1053 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_bf16_ipex.yaml
--rw-r--r--   0 root         (0) root         (0)     1073 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_bf16_ipex_channels_last.yaml
--rw-r--r--   0 root         (0) root         (0)     1037 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_fp32_channels_last.yaml
--rw-r--r--   0 root         (0) root         (0)     1032 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_fp32_ipex.yaml
--rw-r--r--   0 root         (0) root         (0)     1052 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_fp32_ipex_channels_last.yaml
--rw-r--r--   0 root         (0) root         (0)      849 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_gpu_to_cpu.yaml
--rw-r--r--   0 root         (0) root         (0)     1038 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_int8.yaml
--rw-r--r--   0 root         (0) root         (0)     1057 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_jit_bf16.yaml
--rw-r--r--   0 root         (0) root         (0)     1077 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_jit_bf16_channels_last.yaml
--rw-r--r--   0 root         (0) root         (0)     1072 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_jit_bf16_ipex.yaml
--rw-r--r--   0 root         (0) root         (0)     1092 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_jit_bf16_ipex_channels_last.yaml
--rw-r--r--   0 root         (0) root         (0)     1038 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_jit_fp32.yaml
--rw-r--r--   0 root         (0) root         (0)     1056 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_jit_fp32_channels_last.yaml
--rw-r--r--   0 root         (0) root         (0)     1053 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_jit_fp32_ipex.yaml
--rw-r--r--   0 root         (0) root         (0)     1071 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_jit_fp32_ipex_channels_last.yaml
--rw-r--r--   0 root         (0) root         (0)     1044 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_onnxruntime_fp32.yaml
--rw-r--r--   0 root         (0) root         (0)     1065 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_onnxruntime_int8_qlinear.yaml
--rw-r--r--   0 root         (0) root         (0)     1041 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_openvino_fp32.yaml
--rw-r--r--   0 root         (0) root         (0)     1062 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/nano_openvino_int8.yaml
--rw-r--r--   0 root         (0) root         (0)     1138 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/onnx_inc_dynamic_quant.yaml
--rw-r--r--   0 root         (0) root         (0)     1188 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/onnx_inc_static_quant_qdq.yaml
--rw-r--r--   0 root         (0) root         (0)     1192 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/onnx_inc_static_quant_qlinear.yaml
--rw-r--r--   0 root         (0) root         (0)      922 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_aliblade.yaml
--rw-r--r--   0 root         (0) root         (0)     2655 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_benchmark.yaml
--rw-r--r--   0 root         (0) root         (0)     1246 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_channels_last.yaml
--rw-r--r--   0 root         (0) root         (0)     1363 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_bf16.yaml
--rw-r--r--   0 root         (0) root         (0)     1859 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_dynamic_quant.yaml
--rw-r--r--   0 root         (0) root         (0)     2033 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_dynamic_quant_fp8.yaml
--rw-r--r--   0 root         (0) root         (0)     1453 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_huggingface_optimum_dynamic.yaml
--rw-r--r--   0 root         (0) root         (0)     1486 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_huggingface_optimum_static.yaml
--rw-r--r--   0 root         (0) root         (0)     1886 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_static_quant_fx.yaml
--rw-r--r--   0 root         (0) root         (0)     1958 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_static_quant_fx_fp8.yaml
--rw-r--r--   0 root         (0) root         (0)     1510 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_static_quant_ipex.yaml
--rw-r--r--   0 root         (0) root         (0)     1306 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_ipex_bf16.yaml
--rw-r--r--   0 root         (0) root         (0)     1076 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_ipex_fp32.yaml
--rw-r--r--   0 root         (0) root         (0)     1540 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_ipex_int8_dynamic_quant.yaml
--rw-r--r--   0 root         (0) root         (0)     1539 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_ipex_int8_static_quant.yaml
--rw-r--r--   0 root         (0) root         (0)     1260 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_jit_script.yaml
--rw-r--r--   0 root         (0) root         (0)     1234 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_jit_script_ofi.yaml
--rw-r--r--   0 root         (0) root         (0)     1347 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_jit_trace.yaml
--rw-r--r--   0 root         (0) root         (0)     1321 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_jit_trace_ofi.yaml
--rw-r--r--   0 root         (0) root         (0)      860 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_mixed_precision_cpu.yaml
--rw-r--r--   0 root         (0) root         (0)      861 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_mixed_precision_cuda.yaml
--rw-r--r--   0 root         (0) root         (0)      843 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_mixed_precision_intel_gpu.yaml
--rw-r--r--   0 root         (0) root         (0)     1201 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_torchdynamo_jit_script.yaml
--rw-r--r--   0 root         (0) root         (0)     1235 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_torchdynamo_jit_script_ofi.yaml
--rw-r--r--   0 root         (0) root         (0)     1216 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_torchdynamo_jit_trace.yaml
--rw-r--r--   0 root         (0) root         (0)     1250 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/pytorch_torchdynamo_jit_trace_ofi.yaml
--rw-r--r--   0 root         (0) root         (0)     1118 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/backends/template.yaml
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.724218 neural_compressor-2.1.1/neural_coder/coders/
--rw-r--r--   0 root         (0) root         (0)      583 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.724218 neural_compressor-2.1.1/neural_coder/coders/autoinc/
--rw-r--r--   0 root         (0) root         (0)      583 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/autoinc/__init__.py
--rw-r--r--   0 root         (0) root         (0)    26057 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/autoinc/autoinc_harness.py
--rw-r--r--   0 root         (0) root         (0)     1744 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/autoinc/calib_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     1335 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/autoinc/domain.py
--rw-r--r--   0 root         (0) root         (0)     4793 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/autoinc/eval_func.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.728219 neural_compressor-2.1.1/neural_coder/coders/pytorch/
--rw-r--r--   0 root         (0) root         (0)      583 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/pytorch/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3204 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/pytorch/batch_size.py
--rw-r--r--   0 root         (0) root         (0)     1457 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/pytorch/change_trainer_to_nlptrainer.py
--rw-r--r--   0 root         (0) root         (0)     2992 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/pytorch/cuda_to_cpu.py
--rw-r--r--   0 root         (0) root         (0)     6755 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/pytorch/dummy_dataloader.py
--rw-r--r--   0 root         (0) root         (0)    23101 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/pytorch/harness.py
--rw-r--r--   0 root         (0) root         (0)     2890 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/pytorch/lightning.py
--rw-r--r--   0 root         (0) root         (0)     3667 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/pytorch/reclaim_inference_transformers_trainer.py
--rw-r--r--   0 root         (0) root         (0)     5291 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/pytorch/reclaim_inputs.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.728219 neural_compressor-2.1.1/neural_coder/coders/tensorflow/
--rw-r--r--   0 root         (0) root         (0)      583 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/tensorflow/__init__.py
--rw-r--r--   0 root         (0) root         (0)     2678 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/tensorflow/amp.py
--rw-r--r--   0 root         (0) root         (0)     2361 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/tensorflow/inc.py
--rw-r--r--   0 root         (0) root         (0)     3716 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/coders/transform.py
--rw-r--r--   0 root         (0) root         (0)     3401 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/globals.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.728219 neural_compressor-2.1.1/neural_coder/graphers/
--rw-r--r--   0 root         (0) root         (0)      583 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/graphers/__init__.py
--rw-r--r--   0 root         (0) root         (0)    11123 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/graphers/code_line.py
--rw-r--r--   0 root         (0) root         (0)     7296 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/graphers/function.py
--rw-r--r--   0 root         (0) root         (0)    12402 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/graphers/model.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.728219 neural_compressor-2.1.1/neural_coder/graphers/preloads/
--rw-r--r--   0 root         (0) root         (0)      583 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/graphers/preloads/__init__.py
--rw-r--r--   0 root         (0) root         (0)    58208 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/graphers/preloads/transformers.yaml
--rw-r--r--   0 root         (0) root         (0)    52440 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/interface.py
--rw-r--r--   0 root         (0) root         (0)     3722 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/launcher.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.728219 neural_compressor-2.1.1/neural_coder/utils/
--rw-r--r--   0 root         (0) root         (0)      583 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)      900 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/utils/common.py
--rw-r--r--   0 root         (0) root         (0)     1756 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/utils/cpu_info.py
--rw-r--r--   0 root         (0) root         (0)     3077 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/utils/device.py
--rw-r--r--   0 root         (0) root         (0)     7527 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/utils/handle_user_input.py
--rw-r--r--   0 root         (0) root         (0)     5116 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/utils/line_operation.py
--rw-r--r--   0 root         (0) root         (0)    38671 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/utils/numa_launcher.py
--rw-r--r--   0 root         (0) root         (0)    18664 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/utils/pdf_report.py
--rw-r--r--   0 root         (0) root         (0)      604 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_coder/version.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.728219 neural_compressor-2.1.1/neural_compressor/
--rw-r--r--   0 root         (0) root         (0)     1254 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.732219 neural_compressor-2.1.1/neural_compressor/adaptor/
--rw-r--r--   0 root         (0) root         (0)      973 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/__init__.py
--rw-r--r--   0 root         (0) root         (0)     7730 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/adaptor.py
--rw-r--r--   0 root         (0) root         (0)    36382 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/keras.py
--rw-r--r--   0 root         (0) root         (0)     3590 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/keras.yaml
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.732219 neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/
--rw-r--r--   0 root         (0) root         (0)      632 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3281 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/conv2d.py
--rw-r--r--   0 root         (0) root         (0)     2861 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/dense.py
--rw-r--r--   0 root         (0) root         (0)     4456 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/depthwise_conv2d.py
--rw-r--r--   0 root         (0) root         (0)     3950 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/quantizer.py
--rw-r--r--   0 root         (0) root         (0)     4248 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/separable_conv2d.py
--rw-r--r--   0 root         (0) root         (0)    22313 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/mxnet.py
--rw-r--r--   0 root         (0) root         (0)    10620 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/mxnet.yaml
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.732219 neural_compressor-2.1.1/neural_compressor/adaptor/mxnet_utils/
--rw-r--r--   0 root         (0) root         (0)      655 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/mxnet_utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)    31176 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/mxnet_utils/util.py
--rw-r--r--   0 root         (0) root         (0)    73017 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/onnxrt.py
--rw-r--r--   0 root         (0) root         (0)    14506 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/onnxrt.yaml
--rw-r--r--   0 root         (0) root         (0)    15257 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/onnxrt_cuda.yaml
--rw-r--r--   0 root         (0) root         (0)     4832 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/onnxrt_trt.yaml
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.732219 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/
--rw-r--r--   0 root         (0) root         (0)      656 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)    32644 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/calibration.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.736219 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/
--rw-r--r--   0 root         (0) root         (0)     1026 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/__init__.py
--rw-r--r--   0 root         (0) root         (0)     5100 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/activation.py
--rw-r--r--   0 root         (0) root         (0)     1802 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/argmax.py
--rw-r--r--   0 root         (0) root         (0)     4737 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/attention.py
--rw-r--r--   0 root         (0) root         (0)     5054 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/binary_op.py
--rw-r--r--   0 root         (0) root         (0)     5946 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/concat.py
--rw-r--r--   0 root         (0) root         (0)    10775 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/conv.py
--rw-r--r--   0 root         (0) root         (0)     3671 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/direct_q8.py
--rw-r--r--   0 root         (0) root         (0)     4360 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/embed_layernorm.py
--rw-r--r--   0 root         (0) root         (0)     4390 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/gather.py
--rw-r--r--   0 root         (0) root         (0)     3662 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/gavgpool.py
--rw-r--r--   0 root         (0) root         (0)     6655 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/gemm.py
--rw-r--r--   0 root         (0) root         (0)     5776 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/lstm.py
--rw-r--r--   0 root         (0) root         (0)     7546 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/matmul.py
--rw-r--r--   0 root         (0) root         (0)     3319 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/maxpool.py
--rw-r--r--   0 root         (0) root         (0)     5577 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/ops.py
--rw-r--r--   0 root         (0) root         (0)     4996 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/pad.py
--rw-r--r--   0 root         (0) root         (0)     4726 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/pooling.py
--rw-r--r--   0 root         (0) root         (0)     3444 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/resize.py
--rw-r--r--   0 root         (0) root         (0)     6146 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/split.py
--rw-r--r--   0 root         (0) root         (0)    60047 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/quantizer.py
--rw-r--r--   0 root         (0) root         (0)    30132 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/util.py
--rw-r--r--   0 root         (0) root         (0)   200728 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/pytorch.py
--rw-r--r--   0 root         (0) root         (0)    15795 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/pytorch_cpu.yaml
--rw-r--r--   0 root         (0) root         (0)     2627 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/pytorch_gpu.yaml
--rw-r--r--   0 root         (0) root         (0)     4937 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/pytorch_ipex.yaml
--rw-r--r--   0 root         (0) root         (0)     2384 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/query.py
--rw-r--r--   0 root         (0) root         (0)   111831 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tensorflow.py
--rw-r--r--   0 root         (0) root         (0)     9844 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tensorflow.yaml
--rw-r--r--   0 root         (0) root         (0)     6941 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tensorflow_itex.yaml
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.736219 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/
--rw-r--r--   0 root         (0) root         (0)      657 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)    41758 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_converter.py
--rw-r--r--   0 root         (0) root         (0)    16274 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_converter_without_calib.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.736219 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/
--rw-r--r--   0 root         (0) root         (0)      665 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.736219 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/
--rw-r--r--   0 root         (0) root         (0)      669 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/__init__.py
--rw-r--r--   0 root         (0) root         (0)    13692 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/bf16_convert.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.740220 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/
--rw-r--r--   0 root         (0) root         (0)      673 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3238 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_add_to_biasadd.py
--rw-r--r--   0 root         (0) root         (0)     3985 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_layout.py
--rw-r--r--   0 root         (0) root         (0)     3191 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_leakyrelu.py
--rw-r--r--   0 root         (0) root         (0)     1871 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_nan_to_random.py
--rw-r--r--   0 root         (0) root         (0)     4330 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_placeholder_to_const.py
--rw-r--r--   0 root         (0) root         (0)     3076 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dequantize_cast_optimizer.py
--rw-r--r--   0 root         (0) root         (0)     4157 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dilated_contraction.py
--rw-r--r--   0 root         (0) root         (0)     5522 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dummy_biasadd.py
--rw-r--r--   0 root         (0) root         (0)     3398 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/expanddims_optimizer.py
--rw-r--r--   0 root         (0) root         (0)     3912 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fetch_weight_from_reshape.py
--rw-r--r--   0 root         (0) root         (0)    13288 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fold_batch_norm.py
--rw-r--r--   0 root         (0) root         (0)     7500 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fold_constant.py
--rw-r--r--   0 root         (0) root         (0)     3011 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_biasadd_add.py
--rw-r--r--   0 root         (0) root         (0)     3813 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_column_wise_mul.py
--rw-r--r--   0 root         (0) root         (0)     5035 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_conv_with_math.py
--rw-r--r--   0 root         (0) root         (0)    17017 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_decomposed_bn.py
--rw-r--r--   0 root         (0) root         (0)    16070 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_decomposed_in.py
--rw-r--r--   0 root         (0) root         (0)     9500 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_gelu.py
--rw-r--r--   0 root         (0) root         (0)     9885 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_layer_norm.py
--rw-r--r--   0 root         (0) root         (0)     5473 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_pad_with_conv.py
--rw-r--r--   0 root         (0) root         (0)     5588 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_pad_with_fp32_conv.py
--rw-r--r--   0 root         (0) root         (0)     5941 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_reshape_transpose.py
--rw-r--r--   0 root         (0) root         (0)     5998 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/graph_cse_optimizer.py
--rw-r--r--   0 root         (0) root         (0)     3201 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/grappler_pass.py
--rw-r--r--   0 root         (0) root         (0)    12728 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/insert_print_node.py
--rw-r--r--   0 root         (0) root         (0)     5980 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/move_squeeze_after_relu.py
--rw-r--r--   0 root         (0) root         (0)    12441 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/pre_optimize.py
--rw-r--r--   0 root         (0) root         (0)     2843 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/remove_training_nodes.py
--rw-r--r--   0 root         (0) root         (0)     2346 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/rename_batch_norm.py
--rw-r--r--   0 root         (0) root         (0)     2674 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/split_shared_input.py
--rw-r--r--   0 root         (0) root         (0)     2219 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/strip_equivalent_nodes.py
--rw-r--r--   0 root         (0) root         (0)     1631 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/strip_unused_nodes.py
--rw-r--r--   0 root         (0) root         (0)     3420 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/switch_optimizer.py
--rw-r--r--   0 root         (0) root         (0)     1224 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/graph_base.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.740220 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/
--rw-r--r--   0 root         (0) root         (0)      670 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/__init__.py
--rw-r--r--   0 root         (0) root         (0)     7820 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_fake_quant.py
--rw-r--r--   0 root         (0) root         (0)    18068 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_value.py
--rw-r--r--   0 root         (0) root         (0)     6116 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_value_without_calib.py
--rw-r--r--   0 root         (0) root         (0)     7028 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_conv_redundant_dequantize.py
--rw-r--r--   0 root         (0) root         (0)    39443 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_conv_requantize.py
--rw-r--r--   0 root         (0) root         (0)     7817 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_matmul_redundant_dequantize.py
--rw-r--r--   0 root         (0) root         (0)    41642 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_matmul_requantize.py
--rw-r--r--   0 root         (0) root         (0)     5409 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/meta_op_optimizer.py
--rw-r--r--   0 root         (0) root         (0)     1736 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/post_hostconst_converter.py
--rw-r--r--   0 root         (0) root         (0)     5449 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/post_quantized_op_cse.py
--rw-r--r--   0 root         (0) root         (0)    17254 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/rnn_convert.py
--rw-r--r--   0 root         (0) root         (0)     4912 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/scale_propagation.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.740220 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/
--rw-r--r--   0 root         (0) root         (0)      692 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/__init__.py
--rw-r--r--   0 root         (0) root         (0)    55321 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_graph.py
--rw-r--r--   0 root         (0) root         (0)    13537 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_node.py
--rw-r--r--   0 root         (0) root         (0)     4606 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_schema.py
--rw-r--r--   0 root         (0) root         (0)    22286 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/tf2onnx_utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.740220 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/
--rw-r--r--   0 root         (0) root         (0)      669 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/__init__.py
--rw-r--r--   0 root         (0) root         (0)    36544 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/insert_qdq_pattern.py
--rw-r--r--   0 root         (0) root         (0)     4320 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/merge_duplicated_qdq.py
--rw-r--r--   0 root         (0) root         (0)     2506 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/share_qdq_y_pattern.py
--rw-r--r--   0 root         (0) root         (0)    40223 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_util.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.740220 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/
--rw-r--r--   0 root         (0) root         (0)      666 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.744220 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/
--rw-r--r--   0 root         (0) root         (0)      670 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/__init__.py
--rw-r--r--   0 root         (0) root         (0)     8721 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/fake_quantize.py
--rw-r--r--   0 root         (0) root         (0)     4620 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_config.py
--rw-r--r--   0 root         (0) root         (0)     2973 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_helper.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.744220 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/
--rw-r--r--   0 root         (0) root         (0)      675 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1285 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/optimize_layer.py
--rw-r--r--   0 root         (0) root         (0)     3103 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_add.py
--rw-r--r--   0 root         (0) root         (0)     3134 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_base.py
--rw-r--r--   0 root         (0) root         (0)     2102 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_bn.py
--rw-r--r--   0 root         (0) root         (0)    10309 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_wrapper.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.744220 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/
--rw-r--r--   0 root         (0) root         (0)      670 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/__init__.py
--rw-r--r--   0 root         (0) root         (0)    13718 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_bn.py
--rw-r--r--   0 root         (0) root         (0)    12386 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_concatv2.py
--rw-r--r--   0 root         (0) root         (0)   112996 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_conv.py
--rw-r--r--   0 root         (0) root         (0)    27174 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_deconv.py
--rw-r--r--   0 root         (0) root         (0)     8266 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_in.py
--rw-r--r--   0 root         (0) root         (0)    55564 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_matmul.py
--rw-r--r--   0 root         (0) root         (0)     6075 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_pooling.py
--rw-r--r--   0 root         (0) root         (0)     7074 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/optimize_qdq.py
--rw-r--r--   0 root         (0) root         (0)    39207 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_base.py
--rw-r--r--   0 root         (0) root         (0)    13443 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_bn.py
--rw-r--r--   0 root         (0) root         (0)     4867 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_concatv2.py
--rw-r--r--   0 root         (0) root         (0)    21256 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_conv.py
--rw-r--r--   0 root         (0) root         (0)     5481 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_for_intel_cpu.py
--rw-r--r--   0 root         (0) root         (0)    17273 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_matmul.py
--rw-r--r--   0 root         (0) root         (0)     3261 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_pooling.py
--rw-r--r--   0 root         (0) root         (0)    19194 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph_common.py
--rw-r--r--   0 root         (0) root         (0)    14567 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/tf2onnx_converter.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.744220 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/
--rw-r--r--   0 root         (0) root         (0)      662 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/__init__.py
--rw-r--r--   0 root         (0) root         (0)     6270 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/bias_correction.py
--rw-r--r--   0 root         (0) root         (0)     3619 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/graph_transform_base.py
--rw-r--r--   0 root         (0) root         (0)     9682 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/insert_logging.py
--rw-r--r--   0 root         (0) root         (0)    15143 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/rerange_quantized_concat.py
--rw-r--r--   0 root         (0) root         (0)    24096 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/util.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.744220 neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/
--rw-r--r--   0 root         (0) root         (0)      657 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3278 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/bf16_convert.py
--rw-r--r--   0 root         (0) root         (0)    25653 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/hawq_metric.py
--rw-r--r--   0 root         (0) root         (0)     4018 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/model_wrapper.py
--rw-r--r--   0 root         (0) root         (0)    40384 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/smooth_quant.py
--rw-r--r--   0 root         (0) root         (0)     2732 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/symbolic_trace.py
--rw-r--r--   0 root         (0) root         (0)    39439 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/util.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.744220 neural_compressor-2.1.1/neural_compressor/algorithm/
--rw-r--r--   0 root         (0) root         (0)     1133 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/algorithm/__init__.py
--rw-r--r--   0 root         (0) root         (0)     6577 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/algorithm/algorithm.py
--rw-r--r--   0 root         (0) root         (0)     6191 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/algorithm/fast_bias_correction.py
--rw-r--r--   0 root         (0) root         (0)     3583 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/algorithm/smooth_quant.py
--rw-r--r--   0 root         (0) root         (0)     6136 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/algorithm/weight_correction.py
--rw-r--r--   0 root         (0) root         (0)    26398 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/benchmark.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.744220 neural_compressor-2.1.1/neural_compressor/compression/
--rw-r--r--   0 root         (0) root         (0)      820 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/__init__.py
--rw-r--r--   0 root         (0) root         (0)    32708 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/callbacks.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.744220 neural_compressor-2.1.1/neural_compressor/compression/distillation/
--rw-r--r--   0 root         (0) root         (0)      656 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/distillation/__init__.py
--rw-r--r--   0 root         (0) root         (0)    65142 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/distillation/criterions.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.748220 neural_compressor-2.1.1/neural_compressor/compression/pruner/
--rw-r--r--   0 root         (0) root         (0)      649 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/pruner/__init__.py
--rw-r--r--   0 root         (0) root         (0)     7272 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/pruner/criteria.py
--rw-r--r--   0 root         (0) root         (0)    55148 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/pruner/patterns.py
--rw-r--r--   0 root         (0) root         (0)    24370 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/pruner/pruners.py
--rw-r--r--   0 root         (0) root         (0)     5094 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/pruner/regs.py
--rw-r--r--   0 root         (0) root         (0)     6369 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/pruner/schedulers.py
--rw-r--r--   0 root         (0) root         (0)    18398 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/compression/pruner/utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.748220 neural_compressor-2.1.1/neural_compressor/conf/
--rw-r--r--   0 root         (0) root         (0)      632 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/conf/__init__.py
--rw-r--r--   0 root         (0) root         (0)    73646 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/conf/config.py
--rw-r--r--   0 root         (0) root         (0)     3054 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/conf/dotdict.py
--rw-r--r--   0 root         (0) root         (0)     9784 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/conf/pythonic_config.py
--rw-r--r--   0 root         (0) root         (0)    75392 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/config.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.748220 neural_compressor-2.1.1/neural_compressor/contrib/
--rw-r--r--   0 root         (0) root         (0)      712 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/contrib/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.748220 neural_compressor-2.1.1/neural_compressor/contrib/strategy/
--rw-r--r--   0 root         (0) root         (0)      973 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/contrib/strategy/__init__.py
--rw-r--r--   0 root         (0) root         (0)    14037 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/contrib/strategy/sigopt.py
--rw-r--r--   0 root         (0) root         (0)    25688 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/contrib/strategy/tpe.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.748220 neural_compressor-2.1.1/neural_compressor/data/
--rw-r--r--   0 root         (0) root         (0)     2351 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.748220 neural_compressor-2.1.1/neural_compressor/data/dataloaders/
--rw-r--r--   0 root         (0) root         (0)      811 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/dataloaders/__init__.py
--rw-r--r--   0 root         (0) root         (0)     4838 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/dataloaders/base_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     5843 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/dataloaders/dataloader.py
--rw-r--r--   0 root         (0) root         (0)     6119 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/dataloaders/default_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     4892 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/dataloaders/fetcher.py
--rw-r--r--   0 root         (0) root         (0)     1757 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/dataloaders/mxnet_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     3703 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/dataloaders/onnxrt_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     2559 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/dataloaders/pytorch_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     5097 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/dataloaders/sampler.py
--rw-r--r--   0 root         (0) root         (0)    14844 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/dataloaders/tensorflow_dataloader.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.748220 neural_compressor-2.1.1/neural_compressor/data/datasets/
--rw-r--r--   0 root         (0) root         (0)     1253 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/datasets/__init__.py
--rw-r--r--   0 root         (0) root         (0)    19629 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/datasets/bert_dataset.py
--rw-r--r--   0 root         (0) root         (0)    12251 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/datasets/coco_dataset.py
--rw-r--r--   0 root         (0) root         (0)    42946 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/datasets/dataset.py
--rw-r--r--   0 root         (0) root         (0)     6584 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/datasets/dummy_dataset.py
--rw-r--r--   0 root         (0) root         (0)    13461 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/datasets/dummy_dataset_v2.py
--rw-r--r--   0 root         (0) root         (0)     9400 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/datasets/imagenet_dataset.py
--rw-r--r--   0 root         (0) root         (0)     3650 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/datasets/style_transfer_dataset.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.748220 neural_compressor-2.1.1/neural_compressor/data/filters/
--rw-r--r--   0 root         (0) root         (0)     1131 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/filters/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1930 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/filters/coco_filter.py
--rw-r--r--   0 root         (0) root         (0)     6147 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/filters/filter.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.752221 neural_compressor-2.1.1/neural_compressor/data/transforms/
--rw-r--r--   0 root         (0) root         (0)     1929 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/transforms/__init__.py
--rw-r--r--   0 root         (0) root         (0)     2031 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/transforms/coco_transform.py
--rw-r--r--   0 root         (0) root         (0)    17692 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/transforms/imagenet_transform.py
--rw-r--r--   0 root         (0) root         (0)      997 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/transforms/postprocess.py
--rw-r--r--   0 root         (0) root         (0)    12109 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/transforms/tokenization.py
--rw-r--r--   0 root         (0) root         (0)   106216 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/data/transforms/transform.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.752221 neural_compressor-2.1.1/neural_compressor/experimental/
--rw-r--r--   0 root         (0) root         (0)     1325 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/__init__.py
--rw-r--r--   0 root         (0) root         (0)    30416 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/benchmark.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.752221 neural_compressor-2.1.1/neural_compressor/experimental/common/
--rw-r--r--   0 root         (0) root         (0)     1034 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/common/__init__.py
--rw-r--r--   0 root         (0) root         (0)    68967 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/common/criterion.py
--rw-r--r--   0 root         (0) root         (0)     5122 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/common/dataloader.py
--rw-r--r--   0 root         (0) root         (0)     1535 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/common/metric.py
--rw-r--r--   0 root         (0) root         (0)     2500 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/common/model.py
--rw-r--r--   0 root         (0) root         (0)     7839 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/common/optimizer.py
--rw-r--r--   0 root         (0) root         (0)      997 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/common/postprocess.py
--rw-r--r--   0 root         (0) root         (0)     2182 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/common/torch_utils.py
--rw-r--r--   0 root         (0) root         (0)    25100 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/component.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.752221 neural_compressor-2.1.1/neural_compressor/experimental/compression/
--rw-r--r--   0 root         (0) root         (0)      700 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/compression/__init__.py
--rw-r--r--   0 root         (0) root         (0)     5628 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/compression/pruning.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.752221 neural_compressor-2.1.1/neural_compressor/experimental/data/
--rw-r--r--   0 root         (0) root         (0)     1269 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.752221 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/
--rw-r--r--   0 root         (0) root         (0)      836 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/__init__.py
--rw-r--r--   0 root         (0) root         (0)     4816 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/base_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     1582 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/dataloader.py
--rw-r--r--   0 root         (0) root         (0)     6075 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/default_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     4826 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/fetcher.py
--rw-r--r--   0 root         (0) root         (0)     1735 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/mxnet_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     3659 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/onnxrt_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     2537 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/pytorch_dataloader.py
--rw-r--r--   0 root         (0) root         (0)     5009 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/sampler.py
--rw-r--r--   0 root         (0) root         (0)    14814 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/tensorflow_dataloader.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.756221 neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/
--rw-r--r--   0 root         (0) root         (0)     1128 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/__init__.py
--rw-r--r--   0 root         (0) root         (0)    19430 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/bert_dataset.py
--rw-r--r--   0 root         (0) root         (0)    12163 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/coco_dataset.py
--rw-r--r--   0 root         (0) root         (0)    42301 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/dataset.py
--rw-r--r--   0 root         (0) root         (0)     6561 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/dummy_dataset.py
--rw-r--r--   0 root         (0) root         (0)    13417 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/dummy_dataset_v2.py
--rw-r--r--   0 root         (0) root         (0)     9268 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/imagenet_dataset.py
--rw-r--r--   0 root         (0) root         (0)     3627 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/style_transfer_dataset.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.756221 neural_compressor-2.1.1/neural_compressor/experimental/data/filters/
--rw-r--r--   0 root         (0) root         (0)     1045 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/filters/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1884 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/filters/coco_filter.py
--rw-r--r--   0 root         (0) root         (0)     5962 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/filters/filter.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.756221 neural_compressor-2.1.1/neural_compressor/experimental/data/transforms/
--rw-r--r--   0 root         (0) root         (0)     1208 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/transforms/__init__.py
--rw-r--r--   0 root         (0) root         (0)    17489 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/transforms/imagenet_transform.py
--rw-r--r--   0 root         (0) root         (0)    12109 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/transforms/tokenization.py
--rw-r--r--   0 root         (0) root         (0)   106216 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/data/transforms/transform.py
--rw-r--r--   0 root         (0) root         (0)    21921 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/distillation.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.756221 neural_compressor-2.1.1/neural_compressor/experimental/export/
--rw-r--r--   0 root         (0) root         (0)      834 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/export/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3079 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/export/qlinear2qdq.py
--rw-r--r--   0 root         (0) root         (0)     4687 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/export/tf2onnx.py
--rw-r--r--   0 root         (0) root         (0)    36419 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/export/torch2onnx.py
--rw-r--r--   0 root         (0) root         (0)     2289 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/export/utils.py
--rw-r--r--   0 root         (0) root         (0)    20016 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/graph_optimization.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.756221 neural_compressor-2.1.1/neural_compressor/experimental/metric/
--rw-r--r--   0 root         (0) root         (0)     1069 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/metric/__init__.py
--rw-r--r--   0 root         (0) root         (0)     4874 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/metric/bleu.py
--rw-r--r--   0 root         (0) root         (0)     5230 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/metric/bleu_util.py
--rw-r--r--   0 root         (0) root         (0)     2188 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/metric/coco_label_map.py
--rw-r--r--   0 root         (0) root         (0)    32578 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/metric/coco_tools.py
--rw-r--r--   0 root         (0) root         (0)     4339 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/metric/evaluate_squad.py
--rw-r--r--   0 root         (0) root         (0)     5342 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/metric/f1.py
--rw-r--r--   0 root         (0) root         (0)    57890 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/metric/metric.py
--rw-r--r--   0 root         (0) root         (0)    10133 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/mixed_precision.py
--rw-r--r--   0 root         (0) root         (0)    15676 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/model_conversion.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.756221 neural_compressor-2.1.1/neural_compressor/experimental/nas/
--rw-r--r--   0 root         (0) root         (0)      728 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/nas/__init__.py
--rw-r--r--   0 root         (0) root         (0)     6086 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/nas/basic_nas.py
--rw-r--r--   0 root         (0) root         (0)     4022 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/nas/dynas.py
--rw-r--r--   0 root         (0) root         (0)    15982 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/nas/nas.py
--rw-r--r--   0 root         (0) root         (0)     2797 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/nas/nas_utils.py
--rw-r--r--   0 root         (0) root         (0)     5893 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/nas/search_algorithms.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.756221 neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/
--rw-r--r--   0 root         (0) root         (0)      995 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/__init__.py
--rw-r--r--   0 root         (0) root         (0)    12216 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/gradient_sensitivity.py
--rw-r--r--   0 root         (0) root         (0)     2729 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/group_lasso.py
--rw-r--r--   0 root         (0) root         (0)     4565 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/magnitude.py
--rw-r--r--   0 root         (0) root         (0)     2158 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/pattern_lock.py
--rw-r--r--   0 root         (0) root         (0)     4724 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/pruner.py
--rw-r--r--   0 root         (0) root         (0)    22318 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruning.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.756221 neural_compressor-2.1.1/neural_compressor/experimental/pruning_recipes/
--rw-r--r--   0 root         (0) root         (0)      736 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruning_recipes/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.756221 neural_compressor-2.1.1/neural_compressor/experimental/pruning_recipes/patterns/
--rw-r--r--   0 root         (0) root         (0)     1017 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruning_recipes/patterns/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3735 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruning_recipes/patterns/pattern.py
--rw-r--r--   0 root         (0) root         (0)     2835 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruning_recipes/patterns/tile_pattern.py
--rw-r--r--   0 root         (0) root         (0)    23225 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pruning_v2.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.760221 neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/
--rw-r--r--   0 root         (0) root         (0)      660 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/__init__.py
--rw-r--r--   0 root         (0) root         (0)      681 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/logger.py
--rw-r--r--   0 root         (0) root         (0)    24675 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/patterns.py
--rw-r--r--   0 root         (0) root         (0)     9285 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/prune_utils.py
--rw-r--r--   0 root         (0) root         (0)    12505 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/pruner.py
--rw-r--r--   0 root         (0) root         (0)     6382 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/pruning.py
--rw-r--r--   0 root         (0) root         (0)     5421 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/scheduler.py
--rw-r--r--   0 root         (0) root         (0)    24028 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/quantization.py
--rw-r--r--   0 root         (0) root         (0)    18510 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/experimental/scheduler.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.760221 neural_compressor-2.1.1/neural_compressor/metric/
--rw-r--r--   0 root         (0) root         (0)     1204 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/metric/__init__.py
--rw-r--r--   0 root         (0) root         (0)     4874 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/metric/bleu.py
--rw-r--r--   0 root         (0) root         (0)     5230 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/metric/bleu_util.py
--rw-r--r--   0 root         (0) root         (0)     2188 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/metric/coco_label_map.py
--rw-r--r--   0 root         (0) root         (0)    32578 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/metric/coco_tools.py
--rw-r--r--   0 root         (0) root         (0)     4339 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/metric/evaluate_squad.py
--rw-r--r--   0 root         (0) root         (0)     5342 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/metric/f1.py
--rw-r--r--   0 root         (0) root         (0)    58474 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/metric/metric.py
--rw-r--r--   0 root         (0) root         (0)    19915 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/mix_precision.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.760221 neural_compressor-2.1.1/neural_compressor/model/
--rw-r--r--   0 root         (0) root         (0)      800 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/model/__init__.py
--rw-r--r--   0 root         (0) root         (0)     2098 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/model/base_model.py
--rw-r--r--   0 root         (0) root         (0)     4388 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/model/keras_model.py
--rw-r--r--   0 root         (0) root         (0)     7172 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/model/model.py
--rw-r--r--   0 root         (0) root         (0)     2471 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/model/mxnet_model.py
--rw-r--r--   0 root         (0) root         (0)     4945 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/model/nets_factory.py
--rw-r--r--   0 root         (0) root         (0)    25267 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/model/onnx_model.py
--rw-r--r--   0 root         (0) root         (0)    50828 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/model/tensorflow_model.py
--rw-r--r--   0 root         (0) root         (0)    16101 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/model/torch_model.py
--rw-r--r--   0 root         (0) root         (0)    21247 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/objective.py
--rw-r--r--   0 root         (0) root         (0)    24079 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/quantization.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.760221 neural_compressor-2.1.1/neural_compressor/strategy/
--rw-r--r--   0 root         (0) root         (0)     1015 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/__init__.py
--rw-r--r--   0 root         (0) root         (0)     4847 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/auto.py
--rw-r--r--   0 root         (0) root         (0)     7623 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/auto_mixed_precision.py
--rw-r--r--   0 root         (0) root         (0)    17640 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/basic.py
--rw-r--r--   0 root         (0) root         (0)    15754 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/bayesian.py
--rw-r--r--   0 root         (0) root         (0)     9569 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/conservative.py
--rw-r--r--   0 root         (0) root         (0)     2297 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/exhaustive.py
--rw-r--r--   0 root         (0) root         (0)     5787 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/hawq_v2.py
--rw-r--r--   0 root         (0) root         (0)    10489 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/mse.py
--rw-r--r--   0 root         (0) root         (0)    12459 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/mse_v2.py
--rw-r--r--   0 root         (0) root         (0)     2550 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/random.py
--rw-r--r--   0 root         (0) root         (0)    79180 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/strategy.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.760221 neural_compressor-2.1.1/neural_compressor/strategy/utils/
--rw-r--r--   0 root         (0) root         (0)      905 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)     1427 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/utils/constant.py
--rw-r--r--   0 root         (0) root         (0)    21546 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/utils/tuning_sampler.py
--rw-r--r--   0 root         (0) root         (0)    31984 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/utils/tuning_space.py
--rw-r--r--   0 root         (0) root         (0)     3808 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/utils/tuning_structs.py
--rw-r--r--   0 root         (0) root         (0)     1760 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/strategy/utils/utility.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.764222 neural_compressor-2.1.1/neural_compressor/template/
--rw-r--r--   0 root         (0) root         (0)        0 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/template/__init__.py
--rw-r--r--   0 root         (0) root         (0)     4411 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/template/api_doc_example.py
--rw-r--r--   0 root         (0) root         (0)     5001 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/template/graph_optimization.yaml
--rw-r--r--   0 root         (0) root         (0)     5133 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/template/pruning.yaml
--rw-r--r--   0 root         (0) root         (0)     7611 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/template/ptq.yaml
--rw-r--r--   0 root         (0) root         (0)     7003 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/template/qat.yaml
--rw-r--r--   0 root         (0) root         (0)    18610 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/training.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.764222 neural_compressor-2.1.1/neural_compressor/utils/
--rw-r--r--   0 root         (0) root         (0)     1034 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)     2869 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/utils/collect_layer_histogram.py
--rw-r--r--   0 root         (0) root         (0)     3607 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/utils/constant.py
--rw-r--r--   0 root         (0) root         (0)     9668 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/utils/create_obj_from_config.py
--rw-r--r--   0 root         (0) root         (0)     6085 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/utils/kl_divergence.py
--rw-r--r--   0 root         (0) root         (0)    11294 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/utils/load_huggingface.py
--rw-r--r--   0 root         (0) root         (0)     4722 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/utils/logger.py
--rw-r--r--   0 root         (0) root         (0)     1343 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/utils/options.py
--rw-r--r--   0 root         (0) root         (0)    18173 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/utils/pytorch.py
--rw-r--r--   0 root         (0) root         (0)    19279 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/utils/utility.py
--rw-r--r--   0 root         (0) root         (0)      766 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/neural_compressor/version.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-11 02:51:24.728219 neural_compressor-2.1.1/neural_compressor.egg-info/
--rw-r--r--   0 root         (0) root         (0)    11458 2023-05-11 02:51:24.000000 neural_compressor-2.1.1/neural_compressor.egg-info/PKG-INFO
--rw-r--r--   0 root         (0) root         (0)    24022 2023-05-11 02:51:24.000000 neural_compressor-2.1.1/neural_compressor.egg-info/SOURCES.txt
--rw-r--r--   0 root         (0) root         (0)        1 2023-05-11 02:51:24.000000 neural_compressor-2.1.1/neural_compressor.egg-info/dependency_links.txt
--rw-r--r--   0 root         (0) root         (0)      123 2023-05-11 02:51:24.000000 neural_compressor-2.1.1/neural_compressor.egg-info/requires.txt
--rw-r--r--   0 root         (0) root         (0)       31 2023-05-11 02:51:24.000000 neural_compressor-2.1.1/neural_compressor.egg-info/top_level.txt
--rw-r--r--   0 root         (0) root         (0)       38 2023-05-11 02:51:24.764222 neural_compressor-2.1.1/setup.cfg
--rw-r--r--   0 root         (0) root         (0)     3109 2023-05-11 02:51:07.000000 neural_compressor-2.1.1/setup.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.632919 neural_compressor-2.2/
+-rw-r--r--   0 root         (0) root         (0)    11360 2023-06-21 12:28:42.000000 neural_compressor-2.2/LICENSE
+-rw-r--r--   0 root         (0) root         (0)    11139 2023-06-21 12:28:59.632919 neural_compressor-2.2/PKG-INFO
+-rw-r--r--   0 root         (0) root         (0)    10403 2023-06-21 12:28:42.000000 neural_compressor-2.2/README.md
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.561916 neural_compressor-2.2/neural_coder/
+-rw-r--r--   0 root         (0) root         (0)      748 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/__init__.py
+-rw-r--r--   0 root         (0) root         (0)      668 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/__main__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.568917 neural_compressor-2.2/neural_coder/backends/
+-rw-r--r--   0 root         (0) root         (0)      583 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1382 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/intel_extension_for_transformers.yaml
+-rw-r--r--   0 root         (0) root         (0)     1121 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/keras_inc.yaml
+-rw-r--r--   0 root         (0) root         (0)     1038 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_bf16.yaml
+-rw-r--r--   0 root         (0) root         (0)     1058 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_bf16_channels_last.yaml
+-rw-r--r--   0 root         (0) root         (0)     1053 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_bf16_ipex.yaml
+-rw-r--r--   0 root         (0) root         (0)     1073 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_bf16_ipex_channels_last.yaml
+-rw-r--r--   0 root         (0) root         (0)     1037 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_fp32_channels_last.yaml
+-rw-r--r--   0 root         (0) root         (0)     1032 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_fp32_ipex.yaml
+-rw-r--r--   0 root         (0) root         (0)     1052 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_fp32_ipex_channels_last.yaml
+-rw-r--r--   0 root         (0) root         (0)      849 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_gpu_to_cpu.yaml
+-rw-r--r--   0 root         (0) root         (0)     1038 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_int8.yaml
+-rw-r--r--   0 root         (0) root         (0)     1057 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_jit_bf16.yaml
+-rw-r--r--   0 root         (0) root         (0)     1077 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_jit_bf16_channels_last.yaml
+-rw-r--r--   0 root         (0) root         (0)     1072 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_jit_bf16_ipex.yaml
+-rw-r--r--   0 root         (0) root         (0)     1092 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_jit_bf16_ipex_channels_last.yaml
+-rw-r--r--   0 root         (0) root         (0)     1038 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_jit_fp32.yaml
+-rw-r--r--   0 root         (0) root         (0)     1056 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_jit_fp32_channels_last.yaml
+-rw-r--r--   0 root         (0) root         (0)     1053 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_jit_fp32_ipex.yaml
+-rw-r--r--   0 root         (0) root         (0)     1071 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_jit_fp32_ipex_channels_last.yaml
+-rw-r--r--   0 root         (0) root         (0)     1044 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_onnxruntime_fp32.yaml
+-rw-r--r--   0 root         (0) root         (0)     1065 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_onnxruntime_int8_qlinear.yaml
+-rw-r--r--   0 root         (0) root         (0)     1041 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_openvino_fp32.yaml
+-rw-r--r--   0 root         (0) root         (0)     1062 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/nano_openvino_int8.yaml
+-rw-r--r--   0 root         (0) root         (0)     1082 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/onnx_inc_dynamic_quant.yaml
+-rw-r--r--   0 root         (0) root         (0)     1082 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/onnx_inc_static_quant_qdq.yaml
+-rw-r--r--   0 root         (0) root         (0)     1086 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/onnx_inc_static_quant_qlinear.yaml
+-rw-r--r--   0 root         (0) root         (0)      922 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_aliblade.yaml
+-rw-r--r--   0 root         (0) root         (0)     2655 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_benchmark.yaml
+-rw-r--r--   0 root         (0) root         (0)     1246 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_channels_last.yaml
+-rw-r--r--   0 root         (0) root         (0)     1380 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_inc_bf16.yaml
+-rw-r--r--   0 root         (0) root         (0)     1651 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_inc_dynamic_quant.yaml
+-rw-r--r--   0 root         (0) root         (0)     1761 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_inc_dynamic_quant_fp8.yaml
+-rw-r--r--   0 root         (0) root         (0)     1515 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_inc_huggingface_optimum_dynamic.yaml
+-rw-r--r--   0 root         (0) root         (0)     1548 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_inc_huggingface_optimum_static.yaml
+-rw-r--r--   0 root         (0) root         (0)     1719 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_inc_static_quant_fx.yaml
+-rw-r--r--   0 root         (0) root         (0)     1746 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_inc_static_quant_fx_fp8.yaml
+-rw-r--r--   0 root         (0) root         (0)     1274 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_inc_static_quant_ipex.yaml
+-rw-r--r--   0 root         (0) root         (0)     1306 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_ipex_bf16.yaml
+-rw-r--r--   0 root         (0) root         (0)     1076 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_ipex_fp32.yaml
+-rw-r--r--   0 root         (0) root         (0)     1540 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_ipex_int8_dynamic_quant.yaml
+-rw-r--r--   0 root         (0) root         (0)     1539 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_ipex_int8_static_quant.yaml
+-rw-r--r--   0 root         (0) root         (0)     1260 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_jit_script.yaml
+-rw-r--r--   0 root         (0) root         (0)     1234 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_jit_script_ofi.yaml
+-rw-r--r--   0 root         (0) root         (0)     1347 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_jit_trace.yaml
+-rw-r--r--   0 root         (0) root         (0)     1321 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_jit_trace_ofi.yaml
+-rw-r--r--   0 root         (0) root         (0)      860 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_mixed_precision_cpu.yaml
+-rw-r--r--   0 root         (0) root         (0)      861 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_mixed_precision_cuda.yaml
+-rw-r--r--   0 root         (0) root         (0)      843 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_mixed_precision_intel_gpu.yaml
+-rw-r--r--   0 root         (0) root         (0)     1201 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_torchdynamo_jit_script.yaml
+-rw-r--r--   0 root         (0) root         (0)     1235 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_torchdynamo_jit_script_ofi.yaml
+-rw-r--r--   0 root         (0) root         (0)     1216 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_torchdynamo_jit_trace.yaml
+-rw-r--r--   0 root         (0) root         (0)     1250 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/pytorch_torchdynamo_jit_trace_ofi.yaml
+-rw-r--r--   0 root         (0) root         (0)     1118 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/backends/template.yaml
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.569917 neural_compressor-2.2/neural_coder/coders/
+-rw-r--r--   0 root         (0) root         (0)      583 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.569917 neural_compressor-2.2/neural_coder/coders/autoinc/
+-rw-r--r--   0 root         (0) root         (0)      583 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/autoinc/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    26057 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/autoinc/autoinc_harness.py
+-rw-r--r--   0 root         (0) root         (0)     1744 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/autoinc/calib_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     1335 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/autoinc/domain.py
+-rw-r--r--   0 root         (0) root         (0)     4793 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/autoinc/eval_func.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.570917 neural_compressor-2.2/neural_coder/coders/pytorch/
+-rw-r--r--   0 root         (0) root         (0)      583 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/pytorch/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3204 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/pytorch/batch_size.py
+-rw-r--r--   0 root         (0) root         (0)     1457 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/pytorch/change_trainer_to_nlptrainer.py
+-rw-r--r--   0 root         (0) root         (0)     2992 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/pytorch/cuda_to_cpu.py
+-rw-r--r--   0 root         (0) root         (0)     6755 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/pytorch/dummy_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)    23101 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/pytorch/harness.py
+-rw-r--r--   0 root         (0) root         (0)     2890 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/pytorch/lightning.py
+-rw-r--r--   0 root         (0) root         (0)     3667 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/pytorch/reclaim_inference_transformers_trainer.py
+-rw-r--r--   0 root         (0) root         (0)     5291 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/pytorch/reclaim_inputs.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.571917 neural_compressor-2.2/neural_coder/coders/tensorflow/
+-rw-r--r--   0 root         (0) root         (0)      583 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/tensorflow/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2678 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/tensorflow/amp.py
+-rw-r--r--   0 root         (0) root         (0)     2170 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/tensorflow/inc.py
+-rw-r--r--   0 root         (0) root         (0)     3716 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/coders/transform.py
+-rw-r--r--   0 root         (0) root         (0)     3401 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/globals.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.571917 neural_compressor-2.2/neural_coder/graphers/
+-rw-r--r--   0 root         (0) root         (0)      583 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/graphers/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    11123 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/graphers/code_line.py
+-rw-r--r--   0 root         (0) root         (0)     7296 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/graphers/function.py
+-rw-r--r--   0 root         (0) root         (0)    12402 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/graphers/model.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.572917 neural_compressor-2.2/neural_coder/graphers/preloads/
+-rw-r--r--   0 root         (0) root         (0)      583 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/graphers/preloads/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    58208 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/graphers/preloads/transformers.yaml
+-rw-r--r--   0 root         (0) root         (0)    52440 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/interface.py
+-rw-r--r--   0 root         (0) root         (0)     3722 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/launcher.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.573917 neural_compressor-2.2/neural_coder/utils/
+-rw-r--r--   0 root         (0) root         (0)      583 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)      900 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/utils/common.py
+-rw-r--r--   0 root         (0) root         (0)     1756 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/utils/cpu_info.py
+-rw-r--r--   0 root         (0) root         (0)     3077 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/utils/device.py
+-rw-r--r--   0 root         (0) root         (0)     7527 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/utils/handle_user_input.py
+-rw-r--r--   0 root         (0) root         (0)     5116 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/utils/line_operation.py
+-rw-r--r--   0 root         (0) root         (0)    38671 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/utils/numa_launcher.py
+-rw-r--r--   0 root         (0) root         (0)    18664 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/utils/pdf_report.py
+-rw-r--r--   0 root         (0) root         (0)      604 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_coder/version.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.574917 neural_compressor-2.2/neural_compressor/
+-rw-r--r--   0 root         (0) root         (0)     1209 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.578917 neural_compressor-2.2/neural_compressor/adaptor/
+-rw-r--r--   0 root         (0) root         (0)      973 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     7730 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/adaptor.py
+-rw-r--r--   0 root         (0) root         (0)    45064 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/keras.py
+-rw-r--r--   0 root         (0) root         (0)     5659 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/keras.yaml
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.579917 neural_compressor-2.2/neural_compressor/adaptor/keras_utils/
+-rw-r--r--   0 root         (0) root         (0)      632 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/keras_utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3281 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/keras_utils/conv2d.py
+-rw-r--r--   0 root         (0) root         (0)     2861 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/keras_utils/dense.py
+-rw-r--r--   0 root         (0) root         (0)     4456 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/keras_utils/depthwise_conv2d.py
+-rw-r--r--   0 root         (0) root         (0)     2290 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/keras_utils/pool2d.py
+-rw-r--r--   0 root         (0) root         (0)     3950 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/keras_utils/quantizer.py
+-rw-r--r--   0 root         (0) root         (0)     4248 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/keras_utils/separable_conv2d.py
+-rw-r--r--   0 root         (0) root         (0)    22316 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/mxnet.py
+-rw-r--r--   0 root         (0) root         (0)    10620 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/mxnet.yaml
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.579917 neural_compressor-2.2/neural_compressor/adaptor/mxnet_utils/
+-rw-r--r--   0 root         (0) root         (0)      655 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/mxnet_utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    31176 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/mxnet_utils/util.py
+-rw-r--r--   0 root         (0) root         (0)    77117 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/onnxrt.py
+-rw-r--r--   0 root         (0) root         (0)    15262 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/onnxrt.yaml
+-rw-r--r--   0 root         (0) root         (0)    15699 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/onnxrt_cuda.yaml
+-rw-r--r--   0 root         (0) root         (0)     4872 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/onnxrt_trt.yaml
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.580917 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/
+-rw-r--r--   0 root         (0) root         (0)      656 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    36485 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/calibration.py
+-rw-r--r--   0 root         (0) root         (0)    15844 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/calibrator.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.583917 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/
+-rw-r--r--   0 root         (0) root         (0)     1026 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     5434 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/activation.py
+-rw-r--r--   0 root         (0) root         (0)     1802 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/argmax.py
+-rw-r--r--   0 root         (0) root         (0)     4737 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/attention.py
+-rw-r--r--   0 root         (0) root         (0)     5475 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/binary_op.py
+-rw-r--r--   0 root         (0) root         (0)     6055 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/concat.py
+-rw-r--r--   0 root         (0) root         (0)    10775 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/conv.py
+-rw-r--r--   0 root         (0) root         (0)     3736 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/direct_q8.py
+-rw-r--r--   0 root         (0) root         (0)     4360 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/embed_layernorm.py
+-rw-r--r--   0 root         (0) root         (0)     5694 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/gather.py
+-rw-r--r--   0 root         (0) root         (0)     3662 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/gavgpool.py
+-rw-r--r--   0 root         (0) root         (0)     6655 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/gemm.py
+-rw-r--r--   0 root         (0) root         (0)     5776 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/lstm.py
+-rw-r--r--   0 root         (0) root         (0)     8860 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/matmul.py
+-rw-r--r--   0 root         (0) root         (0)     3345 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/maxpool.py
+-rw-r--r--   0 root         (0) root         (0)     1719 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/norm.py
+-rw-r--r--   0 root         (0) root         (0)     5638 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/ops.py
+-rw-r--r--   0 root         (0) root         (0)     4996 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/pad.py
+-rw-r--r--   0 root         (0) root         (0)     4726 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/pooling.py
+-rw-r--r--   0 root         (0) root         (0)     1100 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/reduce.py
+-rw-r--r--   0 root         (0) root         (0)     3444 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/resize.py
+-rw-r--r--   0 root         (0) root         (0)     6206 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/split.py
+-rw-r--r--   0 root         (0) root         (0)     1005 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/unary_op.py
+-rw-r--r--   0 root         (0) root         (0)    61182 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/quantizer.py
+-rw-r--r--   0 root         (0) root         (0)    30141 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/smooth_quant.py
+-rw-r--r--   0 root         (0) root         (0)    20482 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/ox_utils/util.py
+-rw-r--r--   0 root         (0) root         (0)   220311 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/pytorch.py
+-rw-r--r--   0 root         (0) root         (0)    15795 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/pytorch_cpu.yaml
+-rw-r--r--   0 root         (0) root         (0)     2627 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/pytorch_gpu.yaml
+-rw-r--r--   0 root         (0) root         (0)     4958 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/pytorch_ipex.yaml
+-rw-r--r--   0 root         (0) root         (0)     2384 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/query.py
+-rw-r--r--   0 root         (0) root         (0)   113916 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tensorflow.py
+-rw-r--r--   0 root         (0) root         (0)     9968 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tensorflow.yaml
+-rw-r--r--   0 root         (0) root         (0)     6941 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tensorflow_itex.yaml
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.584917 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/
+-rw-r--r--   0 root         (0) root         (0)      657 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    42716 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_converter.py
+-rw-r--r--   0 root         (0) root         (0)    16274 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_converter_without_calib.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.585917 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/
+-rw-r--r--   0 root         (0) root         (0)      665 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.585917 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/
+-rw-r--r--   0 root         (0) root         (0)      669 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    13846 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/bf16_convert.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.590918 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/
+-rw-r--r--   0 root         (0) root         (0)      673 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3254 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_add_to_biasadd.py
+-rw-r--r--   0 root         (0) root         (0)     3985 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_layout.py
+-rw-r--r--   0 root         (0) root         (0)     3191 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_leakyrelu.py
+-rw-r--r--   0 root         (0) root         (0)     1871 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_nan_to_random.py
+-rw-r--r--   0 root         (0) root         (0)     4330 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_placeholder_to_const.py
+-rw-r--r--   0 root         (0) root         (0)     3305 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dequantize_cast_optimizer.py
+-rw-r--r--   0 root         (0) root         (0)     4157 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dilated_contraction.py
+-rw-r--r--   0 root         (0) root         (0)     5522 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dummy_biasadd.py
+-rw-r--r--   0 root         (0) root         (0)     3398 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/expanddims_optimizer.py
+-rw-r--r--   0 root         (0) root         (0)     3912 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fetch_weight_from_reshape.py
+-rw-r--r--   0 root         (0) root         (0)    13288 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fold_batch_norm.py
+-rw-r--r--   0 root         (0) root         (0)     7500 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fold_constant.py
+-rw-r--r--   0 root         (0) root         (0)     3011 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_biasadd_add.py
+-rw-r--r--   0 root         (0) root         (0)     3813 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_column_wise_mul.py
+-rw-r--r--   0 root         (0) root         (0)     5035 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_conv_with_math.py
+-rw-r--r--   0 root         (0) root         (0)    17017 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_decomposed_bn.py
+-rw-r--r--   0 root         (0) root         (0)    16070 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_decomposed_in.py
+-rw-r--r--   0 root         (0) root         (0)     9500 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_gelu.py
+-rw-r--r--   0 root         (0) root         (0)     9885 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_layer_norm.py
+-rw-r--r--   0 root         (0) root         (0)     5473 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_pad_with_conv.py
+-rw-r--r--   0 root         (0) root         (0)     5588 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_pad_with_fp32_conv.py
+-rw-r--r--   0 root         (0) root         (0)     5941 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_reshape_transpose.py
+-rw-r--r--   0 root         (0) root         (0)     5998 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/graph_cse_optimizer.py
+-rw-r--r--   0 root         (0) root         (0)     3201 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/grappler_pass.py
+-rw-r--r--   0 root         (0) root         (0)    12728 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/insert_print_node.py
+-rw-r--r--   0 root         (0) root         (0)     5980 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/move_squeeze_after_relu.py
+-rw-r--r--   0 root         (0) root         (0)    13918 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/pre_optimize.py
+-rw-r--r--   0 root         (0) root         (0)     2843 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/remove_training_nodes.py
+-rw-r--r--   0 root         (0) root         (0)     2346 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/rename_batch_norm.py
+-rw-r--r--   0 root         (0) root         (0)     2674 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/split_shared_input.py
+-rw-r--r--   0 root         (0) root         (0)     2219 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/strip_equivalent_nodes.py
+-rw-r--r--   0 root         (0) root         (0)     1631 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/strip_unused_nodes.py
+-rw-r--r--   0 root         (0) root         (0)     3420 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/switch_optimizer.py
+-rw-r--r--   0 root         (0) root         (0)     1224 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/graph_base.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.592918 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/
+-rw-r--r--   0 root         (0) root         (0)      670 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     7820 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_fake_quant.py
+-rw-r--r--   0 root         (0) root         (0)    18210 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_value.py
+-rw-r--r--   0 root         (0) root         (0)     6116 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_value_without_calib.py
+-rw-r--r--   0 root         (0) root         (0)     7028 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_conv_redundant_dequantize.py
+-rw-r--r--   0 root         (0) root         (0)    39443 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_conv_requantize.py
+-rw-r--r--   0 root         (0) root         (0)     8188 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_matmul_redundant_dequantize.py
+-rw-r--r--   0 root         (0) root         (0)    41642 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_matmul_requantize.py
+-rw-r--r--   0 root         (0) root         (0)     5409 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/meta_op_optimizer.py
+-rw-r--r--   0 root         (0) root         (0)     1769 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/post_hostconst_converter.py
+-rw-r--r--   0 root         (0) root         (0)     5449 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/post_quantized_op_cse.py
+-rw-r--r--   0 root         (0) root         (0)    17254 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/rnn_convert.py
+-rw-r--r--   0 root         (0) root         (0)     4912 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/scale_propagation.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.592918 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/
+-rw-r--r--   0 root         (0) root         (0)      692 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    55654 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_graph.py
+-rw-r--r--   0 root         (0) root         (0)    13537 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_node.py
+-rw-r--r--   0 root         (0) root         (0)     4606 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_schema.py
+-rw-r--r--   0 root         (0) root         (0)    22286 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/tf2onnx_utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.593918 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/
+-rw-r--r--   0 root         (0) root         (0)      669 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    37367 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/insert_qdq_pattern.py
+-rw-r--r--   0 root         (0) root         (0)     4320 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/merge_duplicated_qdq.py
+-rw-r--r--   0 root         (0) root         (0)     2506 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/share_qdq_y_pattern.py
+-rw-r--r--   0 root         (0) root         (0)    40440 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_util.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.594918 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/
+-rw-r--r--   0 root         (0) root         (0)      666 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.595918 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/
+-rw-r--r--   0 root         (0) root         (0)      670 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     8721 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/fake_quantize.py
+-rw-r--r--   0 root         (0) root         (0)     4620 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_config.py
+-rw-r--r--   0 root         (0) root         (0)     2973 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_helper.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.596918 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/
+-rw-r--r--   0 root         (0) root         (0)      675 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1285 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/optimize_layer.py
+-rw-r--r--   0 root         (0) root         (0)     3103 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_add.py
+-rw-r--r--   0 root         (0) root         (0)     3134 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_base.py
+-rw-r--r--   0 root         (0) root         (0)     2102 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_bn.py
+-rw-r--r--   0 root         (0) root         (0)    10309 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_wrapper.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.597918 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/
+-rw-r--r--   0 root         (0) root         (0)      670 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    13718 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_bn.py
+-rw-r--r--   0 root         (0) root         (0)    12386 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_concatv2.py
+-rw-r--r--   0 root         (0) root         (0)   112996 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_conv.py
+-rw-r--r--   0 root         (0) root         (0)    27174 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_deconv.py
+-rw-r--r--   0 root         (0) root         (0)     8266 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_in.py
+-rw-r--r--   0 root         (0) root         (0)    55564 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_matmul.py
+-rw-r--r--   0 root         (0) root         (0)     6075 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_pooling.py
+-rw-r--r--   0 root         (0) root         (0)     7074 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/optimize_qdq.py
+-rw-r--r--   0 root         (0) root         (0)    39207 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_base.py
+-rw-r--r--   0 root         (0) root         (0)    13443 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_bn.py
+-rw-r--r--   0 root         (0) root         (0)     4867 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_concatv2.py
+-rw-r--r--   0 root         (0) root         (0)    21256 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_conv.py
+-rw-r--r--   0 root         (0) root         (0)     5481 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_for_intel_cpu.py
+-rw-r--r--   0 root         (0) root         (0)    17273 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_matmul.py
+-rw-r--r--   0 root         (0) root         (0)     3261 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_pooling.py
+-rw-r--r--   0 root         (0) root         (0)    19188 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph_common.py
+-rw-r--r--   0 root         (0) root         (0)    10384 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/smooth_quant_calibration.py
+-rw-r--r--   0 root         (0) root         (0)     7752 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/smooth_quant_scaler.py
+-rw-r--r--   0 root         (0) root         (0)    14567 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/tf2onnx_converter.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.598918 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/
+-rw-r--r--   0 root         (0) root         (0)      662 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     6270 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/bias_correction.py
+-rw-r--r--   0 root         (0) root         (0)     3619 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/graph_transform_base.py
+-rw-r--r--   0 root         (0) root         (0)     9682 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/insert_logging.py
+-rw-r--r--   0 root         (0) root         (0)    15301 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/rerange_quantized_concat.py
+-rw-r--r--   0 root         (0) root         (0)    25851 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/tf_utils/util.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.599918 neural_compressor-2.2/neural_compressor/adaptor/torch_utils/
+-rw-r--r--   0 root         (0) root         (0)      657 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/torch_utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3278 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/torch_utils/bf16_convert.py
+-rw-r--r--   0 root         (0) root         (0)    25631 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/torch_utils/hawq_metric.py
+-rw-r--r--   0 root         (0) root         (0)     1743 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/torch_utils/mixed_precision.py
+-rw-r--r--   0 root         (0) root         (0)     5472 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/torch_utils/model_wrapper.py
+-rw-r--r--   0 root         (0) root         (0)     7127 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/torch_utils/pattern_detector.py
+-rw-r--r--   0 root         (0) root         (0)    43673 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/torch_utils/smooth_quant.py
+-rw-r--r--   0 root         (0) root         (0)     2732 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/torch_utils/symbolic_trace.py
+-rw-r--r--   0 root         (0) root         (0)    38166 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/adaptor/torch_utils/util.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.600918 neural_compressor-2.2/neural_compressor/algorithm/
+-rw-r--r--   0 root         (0) root         (0)     1133 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/algorithm/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     6513 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/algorithm/algorithm.py
+-rw-r--r--   0 root         (0) root         (0)     6191 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/algorithm/fast_bias_correction.py
+-rw-r--r--   0 root         (0) root         (0)     3590 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/algorithm/smooth_quant.py
+-rw-r--r--   0 root         (0) root         (0)     6136 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/algorithm/weight_correction.py
+-rw-r--r--   0 root         (0) root         (0)    22632 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/benchmark.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.600918 neural_compressor-2.2/neural_compressor/compression/
+-rw-r--r--   0 root         (0) root         (0)      730 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    19938 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/callbacks.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.600918 neural_compressor-2.2/neural_compressor/compression/distillation/
+-rw-r--r--   0 root         (0) root         (0)      656 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/distillation/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    65142 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/distillation/criterions.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.602918 neural_compressor-2.2/neural_compressor/compression/pruner/
+-rw-r--r--   0 root         (0) root         (0)      773 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    12467 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/criteria.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.602918 neural_compressor-2.2/neural_compressor/compression/pruner/model_slim/
+-rw-r--r--   0 root         (0) root         (0)      746 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/model_slim/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     5110 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/model_slim/auto_slim.py
+-rw-r--r--   0 root         (0) root         (0)    36319 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/model_slim/pattern_analyzer.py
+-rw-r--r--   0 root         (0) root         (0)    17870 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/model_slim/weight_slim.py
+-rw-r--r--   0 root         (0) root         (0)    69780 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/patterns.py
+-rw-r--r--   0 root         (0) root         (0)    52361 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/pruners.py
+-rw-r--r--   0 root         (0) root         (0)     5094 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/regs.py
+-rw-r--r--   0 root         (0) root         (0)     6369 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/schedulers.py
+-rw-r--r--   0 root         (0) root         (0)    24017 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/compression/pruner/utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.603918 neural_compressor-2.2/neural_compressor/conf/
+-rw-r--r--   0 root         (0) root         (0)      632 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/conf/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    73446 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/conf/config.py
+-rw-r--r--   0 root         (0) root         (0)     3054 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/conf/dotdict.py
+-rw-r--r--   0 root         (0) root         (0)    52392 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/conf/pythonic_config.py
+-rw-r--r--   0 root         (0) root         (0)    88862 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/config.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.603918 neural_compressor-2.2/neural_compressor/contrib/
+-rw-r--r--   0 root         (0) root         (0)      712 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/contrib/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.603918 neural_compressor-2.2/neural_compressor/contrib/strategy/
+-rw-r--r--   0 root         (0) root         (0)      973 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/contrib/strategy/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    15661 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/contrib/strategy/sigopt.py
+-rw-r--r--   0 root         (0) root         (0)    26054 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/contrib/strategy/tpe.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.604918 neural_compressor-2.2/neural_compressor/data/
+-rw-r--r--   0 root         (0) root         (0)     2428 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.605918 neural_compressor-2.2/neural_compressor/data/dataloaders/
+-rw-r--r--   0 root         (0) root         (0)      811 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/dataloaders/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     4838 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/dataloaders/base_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     6141 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/dataloaders/dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     6119 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/dataloaders/default_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     4892 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/dataloaders/fetcher.py
+-rw-r--r--   0 root         (0) root         (0)     1757 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/dataloaders/mxnet_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     3703 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/dataloaders/onnxrt_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     2559 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/dataloaders/pytorch_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     5097 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/dataloaders/sampler.py
+-rw-r--r--   0 root         (0) root         (0)    14844 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/dataloaders/tensorflow_dataloader.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.606918 neural_compressor-2.2/neural_compressor/data/datasets/
+-rw-r--r--   0 root         (0) root         (0)     1253 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/datasets/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    19629 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/datasets/bert_dataset.py
+-rw-r--r--   0 root         (0) root         (0)    12251 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/datasets/coco_dataset.py
+-rw-r--r--   0 root         (0) root         (0)    42946 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/datasets/dataset.py
+-rw-r--r--   0 root         (0) root         (0)     6584 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/datasets/dummy_dataset.py
+-rw-r--r--   0 root         (0) root         (0)    13461 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/datasets/dummy_dataset_v2.py
+-rw-r--r--   0 root         (0) root         (0)     9400 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/datasets/imagenet_dataset.py
+-rw-r--r--   0 root         (0) root         (0)     3650 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/datasets/style_transfer_dataset.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.607918 neural_compressor-2.2/neural_compressor/data/filters/
+-rw-r--r--   0 root         (0) root         (0)     1131 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/filters/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1930 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/filters/coco_filter.py
+-rw-r--r--   0 root         (0) root         (0)     6147 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/filters/filter.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.607918 neural_compressor-2.2/neural_compressor/data/transforms/
+-rw-r--r--   0 root         (0) root         (0)     1929 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/transforms/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2031 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/transforms/coco_transform.py
+-rw-r--r--   0 root         (0) root         (0)    17692 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/transforms/imagenet_transform.py
+-rw-r--r--   0 root         (0) root         (0)      997 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/transforms/postprocess.py
+-rw-r--r--   0 root         (0) root         (0)    12109 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/transforms/tokenization.py
+-rw-r--r--   0 root         (0) root         (0)   106309 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/data/transforms/transform.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.609918 neural_compressor-2.2/neural_compressor/experimental/
+-rw-r--r--   0 root         (0) root         (0)     1348 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    30413 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/benchmark.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.610918 neural_compressor-2.2/neural_compressor/experimental/common/
+-rw-r--r--   0 root         (0) root         (0)     1034 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/common/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    68967 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/common/criterion.py
+-rw-r--r--   0 root         (0) root         (0)     5122 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/common/dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     1535 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/common/metric.py
+-rw-r--r--   0 root         (0) root         (0)     2656 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/common/model.py
+-rw-r--r--   0 root         (0) root         (0)     7839 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/common/optimizer.py
+-rw-r--r--   0 root         (0) root         (0)      997 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/common/postprocess.py
+-rw-r--r--   0 root         (0) root         (0)     2182 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/common/torch_utils.py
+-rw-r--r--   0 root         (0) root         (0)    25100 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/component.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.611919 neural_compressor-2.2/neural_compressor/experimental/compression/
+-rw-r--r--   0 root         (0) root         (0)      700 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/compression/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     5762 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/compression/pruning.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.611919 neural_compressor-2.2/neural_compressor/experimental/contrib/
+-rw-r--r--   0 root         (0) root         (0)      712 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/contrib/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.611919 neural_compressor-2.2/neural_compressor/experimental/contrib/strategy/
+-rw-r--r--   0 root         (0) root         (0)      972 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/contrib/strategy/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    14075 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/contrib/strategy/sigopt.py
+-rw-r--r--   0 root         (0) root         (0)    25727 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/contrib/strategy/tpe.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.611919 neural_compressor-2.2/neural_compressor/experimental/data/
+-rw-r--r--   0 root         (0) root         (0)     1269 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.613918 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/
+-rw-r--r--   0 root         (0) root         (0)      836 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     4816 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/base_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     1582 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     6075 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/default_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     4826 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/fetcher.py
+-rw-r--r--   0 root         (0) root         (0)     1735 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/mxnet_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     3659 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/onnxrt_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     2537 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/pytorch_dataloader.py
+-rw-r--r--   0 root         (0) root         (0)     5009 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/sampler.py
+-rw-r--r--   0 root         (0) root         (0)    14814 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/tensorflow_dataloader.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.614919 neural_compressor-2.2/neural_compressor/experimental/data/datasets/
+-rw-r--r--   0 root         (0) root         (0)     1128 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/datasets/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    19430 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/datasets/bert_dataset.py
+-rw-r--r--   0 root         (0) root         (0)    12163 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/datasets/coco_dataset.py
+-rw-r--r--   0 root         (0) root         (0)    42301 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/datasets/dataset.py
+-rw-r--r--   0 root         (0) root         (0)     6561 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/datasets/dummy_dataset.py
+-rw-r--r--   0 root         (0) root         (0)    13417 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/datasets/dummy_dataset_v2.py
+-rw-r--r--   0 root         (0) root         (0)     9268 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/datasets/imagenet_dataset.py
+-rw-r--r--   0 root         (0) root         (0)     3627 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/datasets/style_transfer_dataset.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.614919 neural_compressor-2.2/neural_compressor/experimental/data/filters/
+-rw-r--r--   0 root         (0) root         (0)     1045 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/filters/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1884 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/filters/coco_filter.py
+-rw-r--r--   0 root         (0) root         (0)     5962 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/filters/filter.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.615918 neural_compressor-2.2/neural_compressor/experimental/data/transforms/
+-rw-r--r--   0 root         (0) root         (0)     1208 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/transforms/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    17489 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/transforms/imagenet_transform.py
+-rw-r--r--   0 root         (0) root         (0)    12109 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/transforms/tokenization.py
+-rw-r--r--   0 root         (0) root         (0)   106216 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/data/transforms/transform.py
+-rw-r--r--   0 root         (0) root         (0)    22101 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/distillation.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.615918 neural_compressor-2.2/neural_compressor/experimental/export/
+-rw-r--r--   0 root         (0) root         (0)      834 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/export/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3079 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/export/qlinear2qdq.py
+-rw-r--r--   0 root         (0) root         (0)     4687 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/export/tf2onnx.py
+-rw-r--r--   0 root         (0) root         (0)     8191 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/export/torch2onnx.py
+-rw-r--r--   0 root         (0) root         (0)    20027 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/graph_optimization.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.617919 neural_compressor-2.2/neural_compressor/experimental/metric/
+-rw-r--r--   0 root         (0) root         (0)     1069 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/metric/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     4874 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/metric/bleu.py
+-rw-r--r--   0 root         (0) root         (0)     5230 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/metric/bleu_util.py
+-rw-r--r--   0 root         (0) root         (0)     2188 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/metric/coco_label_map.py
+-rw-r--r--   0 root         (0) root         (0)    32578 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/metric/coco_tools.py
+-rw-r--r--   0 root         (0) root         (0)     4339 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/metric/evaluate_squad.py
+-rw-r--r--   0 root         (0) root         (0)     5342 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/metric/f1.py
+-rw-r--r--   0 root         (0) root         (0)    57878 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/metric/metric.py
+-rw-r--r--   0 root         (0) root         (0)    10144 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/mixed_precision.py
+-rw-r--r--   0 root         (0) root         (0)    15676 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/model_conversion.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.617919 neural_compressor-2.2/neural_compressor/experimental/nas/
+-rw-r--r--   0 root         (0) root         (0)      728 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/nas/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     6086 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/nas/basic_nas.py
+-rw-r--r--   0 root         (0) root         (0)     4022 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/nas/dynas.py
+-rw-r--r--   0 root         (0) root         (0)    15982 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/nas/nas.py
+-rw-r--r--   0 root         (0) root         (0)     2797 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/nas/nas_utils.py
+-rw-r--r--   0 root         (0) root         (0)     5893 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/nas/search_algorithms.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.618919 neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/
+-rw-r--r--   0 root         (0) root         (0)      995 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    12216 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/gradient_sensitivity.py
+-rw-r--r--   0 root         (0) root         (0)     2729 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/group_lasso.py
+-rw-r--r--   0 root         (0) root         (0)     4565 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/magnitude.py
+-rw-r--r--   0 root         (0) root         (0)     2158 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/pattern_lock.py
+-rw-r--r--   0 root         (0) root         (0)     4724 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/pruner.py
+-rw-r--r--   0 root         (0) root         (0)    22318 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruning.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.618919 neural_compressor-2.2/neural_compressor/experimental/pruning_recipes/
+-rw-r--r--   0 root         (0) root         (0)      736 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruning_recipes/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.619919 neural_compressor-2.2/neural_compressor/experimental/pruning_recipes/patterns/
+-rw-r--r--   0 root         (0) root         (0)     1017 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruning_recipes/patterns/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3735 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruning_recipes/patterns/pattern.py
+-rw-r--r--   0 root         (0) root         (0)     2835 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruning_recipes/patterns/tile_pattern.py
+-rw-r--r--   0 root         (0) root         (0)    23225 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pruning_v2.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.620919 neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/
+-rw-r--r--   0 root         (0) root         (0)      660 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/__init__.py
+-rw-r--r--   0 root         (0) root         (0)      681 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/logger.py
+-rw-r--r--   0 root         (0) root         (0)    24675 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/patterns.py
+-rw-r--r--   0 root         (0) root         (0)     9285 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/prune_utils.py
+-rw-r--r--   0 root         (0) root         (0)    12505 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/pruner.py
+-rw-r--r--   0 root         (0) root         (0)     6382 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/pruning.py
+-rw-r--r--   0 root         (0) root         (0)     5421 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/scheduler.py
+-rw-r--r--   0 root         (0) root         (0)    22544 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/quantization.py
+-rw-r--r--   0 root         (0) root         (0)    18510 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/scheduler.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.621919 neural_compressor-2.2/neural_compressor/experimental/strategy/
+-rw-r--r--   0 root         (0) root         (0)     1023 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     7634 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/auto_mixed_precision.py
+-rw-r--r--   0 root         (0) root         (0)     9744 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/basic.py
+-rw-r--r--   0 root         (0) root         (0)    15658 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/bayesian.py
+-rw-r--r--   0 root         (0) root         (0)     2137 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/exhaustive.py
+-rw-r--r--   0 root         (0) root         (0)    10490 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/mse.py
+-rw-r--r--   0 root         (0) root         (0)    11869 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/mse_v2.py
+-rw-r--r--   0 root         (0) root         (0)     2551 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/random.py
+-rw-r--r--   0 root         (0) root         (0)    65327 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/strategy.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.622919 neural_compressor-2.2/neural_compressor/experimental/strategy/utils/
+-rw-r--r--   0 root         (0) root         (0)      905 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1358 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/utils/constant.py
+-rw-r--r--   0 root         (0) root         (0)    22965 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/utils/tuning_sampler.py
+-rw-r--r--   0 root         (0) root         (0)    30566 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/utils/tuning_space.py
+-rw-r--r--   0 root         (0) root         (0)     3808 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/utils/tuning_structs.py
+-rw-r--r--   0 root         (0) root         (0)     1760 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/experimental/strategy/utils/utility.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.623919 neural_compressor-2.2/neural_compressor/metric/
+-rw-r--r--   0 root         (0) root         (0)     1281 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/metric/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     4874 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/metric/bleu.py
+-rw-r--r--   0 root         (0) root         (0)     5230 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/metric/bleu_util.py
+-rw-r--r--   0 root         (0) root         (0)     2188 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/metric/coco_label_map.py
+-rw-r--r--   0 root         (0) root         (0)    32578 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/metric/coco_tools.py
+-rw-r--r--   0 root         (0) root         (0)     4339 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/metric/evaluate_squad.py
+-rw-r--r--   0 root         (0) root         (0)     5342 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/metric/f1.py
+-rw-r--r--   0 root         (0) root         (0)    60811 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/metric/metric.py
+-rw-r--r--   0 root         (0) root         (0)     7985 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/mix_precision.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.624919 neural_compressor-2.2/neural_compressor/model/
+-rw-r--r--   0 root         (0) root         (0)      800 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/model/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2098 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/model/base_model.py
+-rw-r--r--   0 root         (0) root         (0)     4388 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/model/keras_model.py
+-rw-r--r--   0 root         (0) root         (0)    11263 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/model/model.py
+-rw-r--r--   0 root         (0) root         (0)     2466 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/model/mxnet_model.py
+-rw-r--r--   0 root         (0) root         (0)     4945 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/model/nets_factory.py
+-rw-r--r--   0 root         (0) root         (0)    29905 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/model/onnx_model.py
+-rw-r--r--   0 root         (0) root         (0)    48924 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/model/tensorflow_model.py
+-rw-r--r--   0 root         (0) root         (0)    16394 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/model/torch_model.py
+-rw-r--r--   0 root         (0) root         (0)    21247 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/objective.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.624919 neural_compressor-2.2/neural_compressor/profiling/
+-rw-r--r--   0 root         (0) root         (0)      663 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.625919 neural_compressor-2.2/neural_compressor/profiling/parser/
+-rw-r--r--   0 root         (0) root         (0)      621 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/parser/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1782 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/parser/factory.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.625919 neural_compressor-2.2/neural_compressor/profiling/parser/onnx_parser/
+-rw-r--r--   0 root         (0) root         (0)      621 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/parser/onnx_parser/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1300 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/parser/onnx_parser/factory.py
+-rw-r--r--   0 root         (0) root         (0)     2923 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/parser/onnx_parser/parser.py
+-rw-r--r--   0 root         (0) root         (0)     2202 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/parser/parser.py
+-rw-r--r--   0 root         (0) root         (0)     2226 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/parser/result.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.626919 neural_compressor-2.2/neural_compressor/profiling/parser/tensorflow_parser/
+-rw-r--r--   0 root         (0) root         (0)      621 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/parser/tensorflow_parser/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1334 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/parser/tensorflow_parser/factory.py
+-rw-r--r--   0 root         (0) root         (0)     4345 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/parser/tensorflow_parser/parser.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.626919 neural_compressor-2.2/neural_compressor/profiling/profiler/
+-rw-r--r--   0 root         (0) root         (0)      621 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2110 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/factory.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.627919 neural_compressor-2.2/neural_compressor/profiling/profiler/onnxrt_profiler/
+-rw-r--r--   0 root         (0) root         (0)      621 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/onnxrt_profiler/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1565 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/onnxrt_profiler/factory.py
+-rw-r--r--   0 root         (0) root         (0)     3337 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/onnxrt_profiler/profiler.py
+-rw-r--r--   0 root         (0) root         (0)     1284 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/onnxrt_profiler/utils.py
+-rw-r--r--   0 root         (0) root         (0)     1052 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/profiler.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.627919 neural_compressor-2.2/neural_compressor/profiling/profiler/tensorflow_profiler/
+-rw-r--r--   0 root         (0) root         (0)      621 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/tensorflow_profiler/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1995 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/tensorflow_profiler/factory.py
+-rw-r--r--   0 root         (0) root         (0)     5470 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/tensorflow_profiler/profiler.py
+-rw-r--r--   0 root         (0) root         (0)     2684 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/profiling/profiler/tensorflow_profiler/utils.py
+-rw-r--r--   0 root         (0) root         (0)    11891 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/quantization.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.629919 neural_compressor-2.2/neural_compressor/strategy/
+-rw-r--r--   0 root         (0) root         (0)     1015 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     4779 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/auto.py
+-rw-r--r--   0 root         (0) root         (0)     8664 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/auto_mixed_precision.py
+-rw-r--r--   0 root         (0) root         (0)    21817 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/basic.py
+-rw-r--r--   0 root         (0) root         (0)    17120 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/bayesian.py
+-rw-r--r--   0 root         (0) root         (0)    11122 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/conservative.py
+-rw-r--r--   0 root         (0) root         (0)     2106 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/exhaustive.py
+-rw-r--r--   0 root         (0) root         (0)     5899 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/hawq_v2.py
+-rw-r--r--   0 root         (0) root         (0)    12039 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/mse.py
+-rw-r--r--   0 root         (0) root         (0)    11397 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/mse_v2.py
+-rw-r--r--   0 root         (0) root         (0)     2541 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/random.py
+-rw-r--r--   0 root         (0) root         (0)    86097 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/strategy.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.630919 neural_compressor-2.2/neural_compressor/strategy/utils/
+-rw-r--r--   0 root         (0) root         (0)      905 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     1402 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/utils/constant.py
+-rw-r--r--   0 root         (0) root         (0)    27701 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/utils/tuning_sampler.py
+-rw-r--r--   0 root         (0) root         (0)    32824 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/utils/tuning_space.py
+-rw-r--r--   0 root         (0) root         (0)     3736 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/utils/tuning_structs.py
+-rw-r--r--   0 root         (0) root         (0)     2613 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/strategy/utils/utility.py
+-rw-r--r--   0 root         (0) root         (0)    21592 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/training.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.631919 neural_compressor-2.2/neural_compressor/utils/
+-rw-r--r--   0 root         (0) root         (0)     1026 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2869 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/collect_layer_histogram.py
+-rw-r--r--   0 root         (0) root         (0)     3646 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/constant.py
+-rw-r--r--   0 root         (0) root         (0)     9668 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/create_obj_from_config.py
+-rw-r--r--   0 root         (0) root         (0)     6085 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/kl_divergence.py
+-rw-r--r--   0 root         (0) root         (0)    11294 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/load_huggingface.py
+-rw-r--r--   0 root         (0) root         (0)     4722 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/logger.py
+-rw-r--r--   0 root         (0) root         (0)     4446 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/neural_insights_utils.py
+-rw-r--r--   0 root         (0) root         (0)     1341 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/options.py
+-rw-r--r--   0 root         (0) root         (0)    18736 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/pytorch.py
+-rw-r--r--   0 root         (0) root         (0)    35375 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/utility.py
+-rw-r--r--   0 root         (0) root         (0)     2638 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/utils/weights_details.py
+-rw-r--r--   0 root         (0) root         (0)      764 2023-06-21 12:28:42.000000 neural_compressor-2.2/neural_compressor/version.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-06-21 12:28:59.575917 neural_compressor-2.2/neural_compressor.egg-info/
+-rw-r--r--   0 root         (0) root         (0)    11139 2023-06-21 12:28:59.000000 neural_compressor-2.2/neural_compressor.egg-info/PKG-INFO
+-rw-r--r--   0 root         (0) root         (0)    26940 2023-06-21 12:28:59.000000 neural_compressor-2.2/neural_compressor.egg-info/SOURCES.txt
+-rw-r--r--   0 root         (0) root         (0)        1 2023-06-21 12:28:59.000000 neural_compressor-2.2/neural_compressor.egg-info/dependency_links.txt
+-rw-r--r--   0 root         (0) root         (0)      207 2023-06-21 12:28:59.000000 neural_compressor-2.2/neural_compressor.egg-info/requires.txt
+-rw-r--r--   0 root         (0) root         (0)       31 2023-06-21 12:28:59.000000 neural_compressor-2.2/neural_compressor.egg-info/top_level.txt
+-rw-r--r--   0 root         (0) root         (0)       38 2023-06-21 12:28:59.632919 neural_compressor-2.2/setup.cfg
+-rw-r--r--   0 root         (0) root         (0)     3720 2023-06-21 12:28:42.000000 neural_compressor-2.2/setup.py
```

### Comparing `neural_compressor-2.1.1/LICENSE` & `neural_compressor-2.2/LICENSE`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/PKG-INFO` & `neural_compressor-2.2/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 Metadata-Version: 2.1
 Name: neural_compressor
-Version: 2.1.1
+Version: 2.2
 Summary: Repository of Intel Neural Compressor
 Home-page: https://github.com/intel/neural-compressor
 Author: Intel AIA Team
 Author-email: feng.tian@intel.com, haihao.shen@intel.com, suyue.chen@intel.com
 License: Apache 2.0
-Keywords: quantization,auto-tuning,post-training static quantization,post-training dynamic quantization,quantization-aware training,tuning strategy
+Keywords: quantization,auto-tuning,post-training static quantization,post-training dynamic quantization,quantization-aware training
 Classifier: Intended Audience :: Science/Research
 Classifier: Programming Language :: Python :: 3
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: License :: OSI Approved :: Apache Software License
 Requires-Python: >=3.6.0
 Description-Content-Type: text/markdown
 License-File: LICENSE
 
 <div align="center">
-  
+
 Intel Neural Compressor
 ===========================
 <h3> An open-source Python library supporting popular model compression techniques on all mainstream deep learning frameworks (TensorFlow, PyTorch, ONNX Runtime, and MXNet)</h3>
 
 [![python](https://img.shields.io/badge/python-3.7%2B-blue)](https://github.com/intel/neural-compressor)
-[![version](https://img.shields.io/badge/release-2.1-green)](https://github.com/intel/neural-compressor/releases)
+[![version](https://img.shields.io/badge/release-2.2-green)](https://github.com/intel/neural-compressor/releases)
 [![license](https://img.shields.io/badge/license-Apache%202-blue)](https://github.com/intel/neural-compressor/blob/master/LICENSE)
 [![coverage](https://img.shields.io/badge/coverage-85%25-green)](https://github.com/intel/neural-compressor)
 [![Downloads](https://static.pepy.tech/personalized-badge/neural-compressor?period=total&units=international_system&left_color=grey&right_color=green&left_text=downloads)](https://pepy.tech/project/neural-compressor)
 
 [Architecture](./docs/source/design.md#architecture)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Workflow](./docs/source/design.md#workflow)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Results](./docs/source/validated_model_list.md)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Examples](./examples/README.md)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Documentations](https://intel.github.io/neural-compressor)
 </div>
 
@@ -48,19 +48,19 @@
 ### Install from pypi
 ```Shell
 pip install neural-compressor
 ```
 > More installation methods can be found at [Installation Guide](./docs/source/installation_guide.md). Please check out our [FAQ](./docs/source/faq.md) for more details.
 
 ## Getting Started
-### Quantization with Python API    
+### Quantization with Python API
 
 ```shell
 # Install Intel Neural Compressor and TensorFlow
-pip install neural-compressor 
+pip install neural-compressor
 pip install tensorflow
 # Prepare fp32 model
 wget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6/mobilenet_v1_1.0_224_frozen.pb
 ```
 ```python
 from neural_compressor.config import PostTrainingQuantConfig
 from neural_compressor.data import DataLoader
@@ -72,51 +72,45 @@
 from neural_compressor.quantization import fit
 q_model = fit(
     model="./mobilenet_v1_1.0_224_frozen.pb",
     conf=PostTrainingQuantConfig(),
     calib_dataloader=dataloader,
     eval_dataloader=dataloader)
 ```
-> More quick samples can be found in [Get Started Page](./docs/source/get_started.md).
 
 ## Documentation
 
 <table class="docutils">
   <thead>
   <tr>
     <th colspan="8">Overview</th>
   </tr>
   </thead>
   <tbody>
     <tr>
       <td colspan="2" align="center"><a href="./docs/source/design.md#architecture">Architecture</a></td>
       <td colspan="2" align="center"><a href="./docs/source/design.md#workflow">Workflow</a></td>
-      <td colspan="2" align="center"><a href="https://intel.github.io/neural-compressor/latest/docs/source/api-doc/apis.html">APIs</a></td>
-      <td colspan="2" align="center"><a href="./docs/source/bench.md">GUI</a></td>
-    </tr>
-    <tr>
-      <td colspan="2" align="center"><a href="examples/README.md#notebook-examples">Notebook</a></td>
       <td colspan="2" align="center"><a href="examples/README.md">Examples</a></td>
-      <td colspan="4" align="center"><a href="https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html">Intel oneAPI AI Analytics Toolkit</a></td>
+      <td colspan="2" align="center"><a href="https://intel.github.io/neural-compressor/latest/docs/source/api-doc/apis.html">APIs</a></td>
     </tr>
   </tbody>
   <thead>
     <tr>
       <th colspan="8">Python-based APIs</th>
     </tr>
   </thead>
   <tbody>
     <tr>
         <td colspan="2" align="center"><a href="./docs/source/quantization.md">Quantization</a></td>
         <td colspan="2" align="center"><a href="./docs/source/mixed_precision.md">Advanced Mixed Precision</a></td>
-        <td colspan="2" align="center"><a href="./docs/source/pruning.md">Pruning (Sparsity)</a></td> 
+        <td colspan="2" align="center"><a href="./docs/source/pruning.md">Pruning (Sparsity)</a></td>
         <td colspan="2" align="center"><a href="./docs/source/distillation.md">Distillation</a></td>
     </tr>
     <tr>
-        <td colspan="2" align="center"><a href="./docs/source/orchestration.md">Orchestration</a></td>        
+        <td colspan="2" align="center"><a href="./docs/source/orchestration.md">Orchestration</a></td>
         <td colspan="2" align="center"><a href="./docs/source/benchmark.md">Benchmarking</a></td>
         <td colspan="2" align="center"><a href="./docs/source/distributed.md">Distributed Compression</a></td>
         <td colspan="2" align="center"><a href="./docs/source/export.md">Model Export</a></td>
     </tr>
   </tbody>
   <thead>
     <tr>
@@ -125,45 +119,55 @@
   </thead>
   <tbody>
     <tr>
         <td colspan="2" align="center"><a href="./neural_coder/docs/PythonLauncher.md">Launcher</a></td>
         <td colspan="2" align="center"><a href="./neural_coder/extensions/neural_compressor_ext_lab/README.md">JupyterLab Extension</a></td>
         <td colspan="2" align="center"><a href="./neural_coder/extensions/neural_compressor_ext_vscode/README.md">Visual Studio Code Extension</a></td>
         <td colspan="2" align="center"><a href="./neural_coder/docs/SupportMatrix.md">Supported Matrix</a></td>
-    </tr>    
+    </tr>
   </tbody>
   <thead>
       <tr>
         <th colspan="8">Advanced Topics</th>
       </tr>
   </thead>
   <tbody>
       <tr>
           <td colspan="2" align="center"><a href="./docs/source/adaptor.md">Adaptor</a></td>
           <td colspan="2" align="center"><a href="./docs/source/tuning_strategies.md">Strategy</a></td>
           <td colspan="2" align="center"><a href="./docs/source/distillation_quantization.md">Distillation for Quantization</a></td>
           <td colspan="2" align="center"><a href="./docs/source/smooth_quant.md">SmoothQuant</td>
       </tr>
   </tbody>
+  <thead>
+      <tr>
+        <th colspan="8">Innovations for Productivity</th>
+      </tr>
+  </thead>
+  <tbody>
+      <tr>
+          <td colspan="4" align="center"><a href="./neural_insights/README.md">Neural Insights</a></td>
+          <td colspan="4" align="center"><a href="./neural_solution/README.md">Neural Solution</a></td>
+      </tr>
+  </tbody>
 </table>
 
 ## Selected Publications/Events
+* Blog on Medium: [Intel Optimization at Netflix](https://medium.com/@amerather_9719/intel-optimization-at-netflix-79ef0efb9d2) (May 2023)
 * Blog on Medium: [Effective Post-training Quantization for Large Language Models with Enhanced SmoothQuant Approach](https://medium.com/@NeuralCompressor/effective-post-training-quantization-for-large-language-models-with-enhanced-smoothquant-approach-93e9d104fb98) (Apr 2023)
 * Blog by Intel: [Intel Xeon Processors Are Still the Only CPU With MLPerf Results, Raising the Bar By 5x](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Xeon-Processors-Are-Still-the-Only-CPU-With-MLPerf-Results/post/1472750) (Apr 2023)
-* Post on Social Media: [Adopt with Tencent TACO: Heterogeneous optimization is also key to improving AI computing power](https://mp.weixin.qq.com/s/I-FQqOuW7HTnwXegLGNAtw) (Mar 2023)
-* Post on Social Media: [Training and Inference for Stable Diffusion | Intel Business](https://www.youtube.com/watch?v=emCgSTlJaAg) (Jan 2023)
 * NeurIPS'2022: [Fast Distilbert on CPUs](https://arxiv.org/abs/2211.07715) (Oct 2022)
 * NeurIPS'2022: [QuaLA-MiniLM: a Quantized Length Adaptive MiniLM](https://arxiv.org/abs/2210.17114) (Oct 2022)
 
 > View our [Full Publication List](./docs/source/publication_list.md).
 
 ## Additional Content
 
 * [Release Information](./docs/source/releases_info.md)
 * [Contribution Guidelines](./docs/source/CONTRIBUTING.md)
 * [Legal Information](./docs/source/legal_information.md)
 * [Security Policy](SECURITY.md)
 
 ## Research Collaborations
 
-Welcome to raise any interesting research ideas on model compression techniques and feel free to reach us (inc.maintainers@intel.com). Look forward to our collaborations on Intel Neural Compressor!
+Welcome to raise any interesting research ideas on model compression techniques and feel free to reach us ([inc.maintainers@intel.com](mailto:inc.maintainers@intel.com)). Look forward to our collaborations on Intel Neural Compressor!
```

### Comparing `neural_compressor-2.1.1/README.md` & `neural_compressor-2.2/README.md`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 <div align="center">
-  
+
 Intel Neural Compressor
 ===========================
 <h3> An open-source Python library supporting popular model compression techniques on all mainstream deep learning frameworks (TensorFlow, PyTorch, ONNX Runtime, and MXNet)</h3>
 
 [![python](https://img.shields.io/badge/python-3.7%2B-blue)](https://github.com/intel/neural-compressor)
-[![version](https://img.shields.io/badge/release-2.1-green)](https://github.com/intel/neural-compressor/releases)
+[![version](https://img.shields.io/badge/release-2.2-green)](https://github.com/intel/neural-compressor/releases)
 [![license](https://img.shields.io/badge/license-Apache%202-blue)](https://github.com/intel/neural-compressor/blob/master/LICENSE)
 [![coverage](https://img.shields.io/badge/coverage-85%25-green)](https://github.com/intel/neural-compressor)
 [![Downloads](https://static.pepy.tech/personalized-badge/neural-compressor?period=total&units=international_system&left_color=grey&right_color=green&left_text=downloads)](https://pepy.tech/project/neural-compressor)
 
 [Architecture](./docs/source/design.md#architecture)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Workflow](./docs/source/design.md#workflow)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Results](./docs/source/validated_model_list.md)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Examples](./examples/README.md)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Documentations](https://intel.github.io/neural-compressor)
 </div>
 
@@ -31,19 +31,19 @@
 ### Install from pypi
 ```Shell
 pip install neural-compressor
 ```
 > More installation methods can be found at [Installation Guide](./docs/source/installation_guide.md). Please check out our [FAQ](./docs/source/faq.md) for more details.
 
 ## Getting Started
-### Quantization with Python API    
+### Quantization with Python API
 
 ```shell
 # Install Intel Neural Compressor and TensorFlow
-pip install neural-compressor 
+pip install neural-compressor
 pip install tensorflow
 # Prepare fp32 model
 wget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6/mobilenet_v1_1.0_224_frozen.pb
 ```
 ```python
 from neural_compressor.config import PostTrainingQuantConfig
 from neural_compressor.data import DataLoader
@@ -55,51 +55,45 @@
 from neural_compressor.quantization import fit
 q_model = fit(
     model="./mobilenet_v1_1.0_224_frozen.pb",
     conf=PostTrainingQuantConfig(),
     calib_dataloader=dataloader,
     eval_dataloader=dataloader)
 ```
-> More quick samples can be found in [Get Started Page](./docs/source/get_started.md).
 
 ## Documentation
 
 <table class="docutils">
   <thead>
   <tr>
     <th colspan="8">Overview</th>
   </tr>
   </thead>
   <tbody>
     <tr>
       <td colspan="2" align="center"><a href="./docs/source/design.md#architecture">Architecture</a></td>
       <td colspan="2" align="center"><a href="./docs/source/design.md#workflow">Workflow</a></td>
-      <td colspan="2" align="center"><a href="https://intel.github.io/neural-compressor/latest/docs/source/api-doc/apis.html">APIs</a></td>
-      <td colspan="2" align="center"><a href="./docs/source/bench.md">GUI</a></td>
-    </tr>
-    <tr>
-      <td colspan="2" align="center"><a href="examples/README.md#notebook-examples">Notebook</a></td>
       <td colspan="2" align="center"><a href="examples/README.md">Examples</a></td>
-      <td colspan="4" align="center"><a href="https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html">Intel oneAPI AI Analytics Toolkit</a></td>
+      <td colspan="2" align="center"><a href="https://intel.github.io/neural-compressor/latest/docs/source/api-doc/apis.html">APIs</a></td>
     </tr>
   </tbody>
   <thead>
     <tr>
       <th colspan="8">Python-based APIs</th>
     </tr>
   </thead>
   <tbody>
     <tr>
         <td colspan="2" align="center"><a href="./docs/source/quantization.md">Quantization</a></td>
         <td colspan="2" align="center"><a href="./docs/source/mixed_precision.md">Advanced Mixed Precision</a></td>
-        <td colspan="2" align="center"><a href="./docs/source/pruning.md">Pruning (Sparsity)</a></td> 
+        <td colspan="2" align="center"><a href="./docs/source/pruning.md">Pruning (Sparsity)</a></td>
         <td colspan="2" align="center"><a href="./docs/source/distillation.md">Distillation</a></td>
     </tr>
     <tr>
-        <td colspan="2" align="center"><a href="./docs/source/orchestration.md">Orchestration</a></td>        
+        <td colspan="2" align="center"><a href="./docs/source/orchestration.md">Orchestration</a></td>
         <td colspan="2" align="center"><a href="./docs/source/benchmark.md">Benchmarking</a></td>
         <td colspan="2" align="center"><a href="./docs/source/distributed.md">Distributed Compression</a></td>
         <td colspan="2" align="center"><a href="./docs/source/export.md">Model Export</a></td>
     </tr>
   </tbody>
   <thead>
     <tr>
@@ -108,45 +102,55 @@
   </thead>
   <tbody>
     <tr>
         <td colspan="2" align="center"><a href="./neural_coder/docs/PythonLauncher.md">Launcher</a></td>
         <td colspan="2" align="center"><a href="./neural_coder/extensions/neural_compressor_ext_lab/README.md">JupyterLab Extension</a></td>
         <td colspan="2" align="center"><a href="./neural_coder/extensions/neural_compressor_ext_vscode/README.md">Visual Studio Code Extension</a></td>
         <td colspan="2" align="center"><a href="./neural_coder/docs/SupportMatrix.md">Supported Matrix</a></td>
-    </tr>    
+    </tr>
   </tbody>
   <thead>
       <tr>
         <th colspan="8">Advanced Topics</th>
       </tr>
   </thead>
   <tbody>
       <tr>
           <td colspan="2" align="center"><a href="./docs/source/adaptor.md">Adaptor</a></td>
           <td colspan="2" align="center"><a href="./docs/source/tuning_strategies.md">Strategy</a></td>
           <td colspan="2" align="center"><a href="./docs/source/distillation_quantization.md">Distillation for Quantization</a></td>
           <td colspan="2" align="center"><a href="./docs/source/smooth_quant.md">SmoothQuant</td>
       </tr>
   </tbody>
+  <thead>
+      <tr>
+        <th colspan="8">Innovations for Productivity</th>
+      </tr>
+  </thead>
+  <tbody>
+      <tr>
+          <td colspan="4" align="center"><a href="./neural_insights/README.md">Neural Insights</a></td>
+          <td colspan="4" align="center"><a href="./neural_solution/README.md">Neural Solution</a></td>
+      </tr>
+  </tbody>
 </table>
 
 ## Selected Publications/Events
+* Blog on Medium: [Intel Optimization at Netflix](https://medium.com/@amerather_9719/intel-optimization-at-netflix-79ef0efb9d2) (May 2023)
 * Blog on Medium: [Effective Post-training Quantization for Large Language Models with Enhanced SmoothQuant Approach](https://medium.com/@NeuralCompressor/effective-post-training-quantization-for-large-language-models-with-enhanced-smoothquant-approach-93e9d104fb98) (Apr 2023)
 * Blog by Intel: [Intel Xeon Processors Are Still the Only CPU With MLPerf Results, Raising the Bar By 5x](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Xeon-Processors-Are-Still-the-Only-CPU-With-MLPerf-Results/post/1472750) (Apr 2023)
-* Post on Social Media: [Adopt with Tencent TACO: Heterogeneous optimization is also key to improving AI computing power](https://mp.weixin.qq.com/s/I-FQqOuW7HTnwXegLGNAtw) (Mar 2023)
-* Post on Social Media: [Training and Inference for Stable Diffusion | Intel Business](https://www.youtube.com/watch?v=emCgSTlJaAg) (Jan 2023)
 * NeurIPS'2022: [Fast Distilbert on CPUs](https://arxiv.org/abs/2211.07715) (Oct 2022)
 * NeurIPS'2022: [QuaLA-MiniLM: a Quantized Length Adaptive MiniLM](https://arxiv.org/abs/2210.17114) (Oct 2022)
 
 > View our [Full Publication List](./docs/source/publication_list.md).
 
 ## Additional Content
 
 * [Release Information](./docs/source/releases_info.md)
 * [Contribution Guidelines](./docs/source/CONTRIBUTING.md)
 * [Legal Information](./docs/source/legal_information.md)
 * [Security Policy](SECURITY.md)
 
 ## Research Collaborations
 
-Welcome to raise any interesting research ideas on model compression techniques and feel free to reach us (inc.maintainers@intel.com). Look forward to our collaborations on Intel Neural Compressor!
+Welcome to raise any interesting research ideas on model compression techniques and feel free to reach us ([inc.maintainers@intel.com](mailto:inc.maintainers@intel.com)). Look forward to our collaborations on Intel Neural Compressor!
```

### Comparing `neural_compressor-2.1.1/neural_coder/__init__.py` & `neural_compressor-2.2/neural_coder/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/__main__.py` & `neural_compressor-2.2/neural_coder/__main__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/__init__.py` & `neural_compressor-2.2/neural_coder/backends/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/intel_extension_for_transformers.yaml` & `neural_compressor-2.2/neural_coder/backends/intel_extension_for_transformers.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/keras_inc.yaml` & `neural_compressor-2.2/neural_coder/backends/onnx_inc_dynamic_quant.yaml`

 * *Files 20% similar despite different names*

```diff
@@ -10,18 +10,18 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 transformation:
   location:
-    - ["insert_below_dataloader_definition_line", "insert_below_model_definition_line"]
+    - insert_below_model_definition_line
   content:
     - |-
       [+] from neural_compressor.quantization import fit
       [+] from neural_compressor.config import PostTrainingQuantConfig
-      [+] from neural_compressor.experimental import common
-      [+] config = PostTrainingQuantConfig(backend='itex')
-      [+] quantized_model = fit(MODEL_NAME, conf=config, calib_dataloader=DATALOADER_NAME, eval_func=eval_func)
+      [+] config = PostTrainingQuantConfig(approach='dynamic', quant_level=1)
+      [+] MODEL_NAME = fit(MODEL_NAME, conf=config, calib_dataloader=DATALOADER_NAME, eval_func=EVAL_FUNCTION_NAME)
+      [+] MODEL_NAME.save("./quantized_model.onnx")
   order:
     - below:
       above:
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_bf16.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_bf16.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_bf16_channels_last.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_bf16_channels_last.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_bf16_ipex.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_bf16_ipex.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_bf16_ipex_channels_last.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_bf16_ipex_channels_last.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_fp32_channels_last.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_fp32_channels_last.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_fp32_ipex.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_fp32_ipex.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_fp32_ipex_channels_last.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_fp32_ipex_channels_last.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_gpu_to_cpu.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_gpu_to_cpu.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_int8.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_int8.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_jit_bf16.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_jit_bf16.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_jit_bf16_channels_last.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_jit_bf16_channels_last.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_jit_bf16_ipex.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_jit_bf16_ipex.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_jit_bf16_ipex_channels_last.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_jit_bf16_ipex_channels_last.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_jit_fp32.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_jit_fp32.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_jit_fp32_channels_last.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_jit_fp32_channels_last.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_jit_fp32_ipex.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_jit_fp32_ipex.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_jit_fp32_ipex_channels_last.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_jit_fp32_ipex_channels_last.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_onnxruntime_fp32.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_onnxruntime_fp32.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_onnxruntime_int8_qlinear.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_onnxruntime_int8_qlinear.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_openvino_fp32.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_openvino_fp32.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/nano_openvino_int8.yaml` & `neural_compressor-2.2/neural_coder/backends/nano_openvino_int8.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/onnx_inc_static_quant_qdq.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_inc_huggingface_optimum_dynamic.yaml`

 * *Files 25% similar despite different names*

```diff
@@ -10,22 +10,28 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 transformation:
   location:
-    - insert_below_model_definition_line
+    - ["insert_below_dataloader_definition_line", "insert_below_model_definition_line"]
   content:
     - |-
-      [+] from neural_compressor.experimental import Quantization, common
-      [+] from neural_compressor import options, conf
-      [+] conf.model.framework = 'onnxrt_qdqops'
-      [+] conf.quantization.approach = 'post_training_static_quant'
-      [+] quantizer = Quantization(conf)
-      [+] quantizer.model = common.Model(MODEL_NAME)
-      [+] quantizer.calib_dataloader = DATALOADER_NAME
-      [+] quantizer.eval_func = EVAL_FUNCTION_NAME
-      [+] MODEL_NAME = quantizer()
+      [+] def eval_func(model):
+      [+]     EVAL_FUNC_LINES
+      [+] from neural_compressor.config import PostTrainingQuantConfig
+      [+] from optimum.intel.neural_compressor import INCQuantizer
+      [+] quantization_config = PostTrainingQuantConfig(approach="dynamic", quant_level=1)
+      [+] quantizer = INCQuantizer.from_pretrained(MODEL_NAME)
+      [+] quantizer.quantize(quantization_config=quantization_config, save_directory="quantized_model", save_onnx_model=False)
+      [+] MODEL_NAME = quantizer._quantized_model
+      [+] MODEL_NAME.save("./quantized_model")
+      [+] MODEL_NAME.eval()
   order:
     - below:
       above:
+        - pytorch_jit_script
+        - pytorch_jit_script_ofi
+        - pytorch_jit_trace
+        - pytorch_jit_trace_ofi
+        - pytorch_channels_last
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_aliblade.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_aliblade.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_benchmark.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_benchmark.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_channels_last.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_channels_last.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_bf16.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_inc_dynamic_quant_fp8.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -10,24 +10,29 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 transformation:
   location:
-    - insert_below_model_definition_line
+    - ["insert_below_dataloader_definition_line", "insert_below_model_definition_line"]
   content:
     - |-
-      [+] import torch
-      [+] torch.backends.quantized.engine = 'onednn'
-      [+] from neural_compressor.experimental import MixedPrecision
-      [+] converter = MixedPrecision()
-      [+] converter.precisions = 'bf16'
-      [+] converter.model = MODEL_NAME
-      [+] MODEL_NAME = converter()
+      [+] def eval_func(model):
+      [+]     EVAL_FUNC_LINES
+      [+] try:
+      [+]     torch.backends.quantized.engine = 'onednn'
+      [+] except:
+      [+]     from torch.backends.quantized import engine; engine = 'onednn'
+      [+] from neural_compressor.config import PostTrainingQuantConfig
+      [+] from neural_compressor.quantization import fit
+      [+] conf = PostTrainingQuantConfig(approach="dynamic", precision = FP8_DATA_FORMAT, quant_level=1)
+      [+] MODEL_NAME = fit(model=MODEL_NAME, conf=conf, calib_dataloader=DATALOADER_NAME, eval_func=eval_func)
+      [+] MODEL_NAME.save("./quantized_model")
+      [+] MODEL_NAME.eval()
       [+] try:
       [+]     with torch.no_grad():
       [+]         MODEL_NAME = torch.jit.script(MODEL_NAME)
       [+]     MODEL_NAME = torch.jit.freeze(MODEL_NAME)
       [+] except:
       [+]     pass
   order:
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_dynamic_quant.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_jit_script.yaml`

 * *Files 19% similar despite different names*

```diff
@@ -13,38 +13,24 @@
 # limitations under the License.
 
 transformation:
   location:
     - insert_below_model_definition_line
   content:
     - |-
-      [+] def eval_func(model):
-      [+]     EVAL_FUNC_LINES
-      [+] try:
-      [+]     torch.backends.quantized.engine = 'onednn'
-      [+] except:
-      [+]     from torch.backends.quantized import engine; engine = 'onednn'
-      [+] from neural_compressor.conf.config import QuantConf
-      [+] from neural_compressor.experimental import Quantization, common
-      [+] quant_config = QuantConf()
-      [+] quant_config.usr_cfg.quantization.approach = "post_training_dynamic_quant"
-      [+] quant_config.usr_cfg.model.framework = "pytorch"
-      [+] quantizer = Quantization(quant_config)
-      [+] quantizer.model = common.Model(MODEL_NAME)
-      [+] quantizer.eval_func = eval_func
-      [+] MODEL_NAME = quantizer()
-      [+] MODEL_NAME = MODEL_NAME.model
-      [+] MODEL_NAME.eval()
-      [+] try:
+      [+] if "jit" not in str(type(MODEL_NAME)):
+      [+]     import torch
       [+]     with torch.no_grad():
+      [+]         MODEL_NAME.eval()
       [+]         MODEL_NAME = torch.jit.script(MODEL_NAME)
-      [+]     MODEL_NAME = torch.jit.freeze(MODEL_NAME)
-      [+] except:
-      [+]     pass
+      [+]         MODEL_NAME = torch.jit.freeze(MODEL_NAME)
   order:
     - below:
-      above:
-        - pytorch_jit_script
-        - pytorch_jit_script_ofi
-        - pytorch_jit_trace
-        - pytorch_jit_trace_ofi
+        - pytorch_inc_static_quant_fx
+        - pytorch_inc_static_quant_ipex
+        - pytorch_inc_dynamic_quant
+        - pytorch_ipex_fp32
+        - pytorch_ipex_bf16
+        - pytorch_ipex_int8_static_quant
+        - pytorch_ipex_int8_dynamic_quant
         - pytorch_channels_last
+      above:
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_huggingface_optimum_dynamic.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_inc_huggingface_optimum_static.yaml`

 * *Files 7% similar despite different names*

```diff
@@ -17,18 +17,19 @@
     - ["insert_below_dataloader_definition_line", "insert_below_model_definition_line"]
   content:
     - |-
       [+] def eval_func(model):
       [+]     EVAL_FUNC_LINES
       [+] from neural_compressor.config import PostTrainingQuantConfig
       [+] from optimum.intel.neural_compressor import INCQuantizer
-      [+] quantization_config = PostTrainingQuantConfig(approach="dynamic")
+      [+] quantization_config = PostTrainingQuantConfig(approach="static", quant_level=1)
       [+] quantizer = INCQuantizer.from_pretrained(MODEL_NAME)
-      [+] quantizer.quantize(quantization_config=quantization_config, save_directory="quantized_model", save_onnx_model=False)
+      [+] quantizer.quantize(quantization_config=quantization_config, calibration_dataset=eval_dataset, save_directory="quantized_model", save_onnx_model=False)
       [+] MODEL_NAME = quantizer._quantized_model
+      [+] MODEL_NAME.save("./quantized_model")
       [+] MODEL_NAME.eval()
   order:
     - below:
       above:
         - pytorch_jit_script
         - pytorch_jit_script_ofi
         - pytorch_jit_trace
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_huggingface_optimum_static.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_ipex_int8_static_quant.yaml`

 * *Files 26% similar despite different names*

```diff
@@ -10,27 +10,30 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 transformation:
   location:
-    - ["insert_below_dataloader_definition_line", "insert_below_model_definition_line"]
+    - ["insert_below_model_definition_line", "insert_below_input_definition_line"]
   content:
     - |-
-      [+] def eval_func(model):
-      [+]     EVAL_FUNC_LINES
-      [+] from neural_compressor.config import PostTrainingQuantConfig
-      [+] from optimum.intel.neural_compressor import INCQuantizer
-      [+] quantization_config = PostTrainingQuantConfig(approach="static")
-      [+] quantizer = INCQuantizer.from_pretrained(MODEL_NAME)
-      [+] quantizer.quantize(quantization_config=quantization_config, calibration_dataset=eval_dataset, save_directory="quantized_model", save_onnx_model=False)
-      [+] MODEL_NAME = quantizer._quantized_model
-      [+] MODEL_NAME.eval()
+      [+] if "quantize" not in str(type(MODEL_NAME)) and "jit" not in str(type(MODEL_NAME)):
+      [+]     import torch
+      [+]     import intel_extension_for_pytorch as ipex
+      [+]     qconfig = ipex.quantization.default_static_qconfig
+      [+]     MODEL_NAME = ipex.quantization.prepare(MODEL_NAME, qconfig, example_inputs=INPUT_NAME, inplace=False)
+      [+]     with torch.no_grad():
+      [+]         for i in range(10):
+      [+]             INFERENCE_LINE
+      [+]     MODEL_NAME = ipex.quantization.convert(MODEL_NAME)
+      [+]     with torch.no_grad():
+      [+]         INFERENCE_LINE
+      [+]     MODEL_NAME.eval()
   order:
     - below:
+        - pytorch_channels_last
       above:
         - pytorch_jit_script
         - pytorch_jit_script_ofi
         - pytorch_jit_trace
         - pytorch_jit_trace_ofi
-        - pytorch_channels_last
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_static_quant_fx.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_ipex_int8_dynamic_quant.yaml`

 * *Files 24% similar despite different names*

```diff
@@ -10,42 +10,30 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 transformation:
   location:
-    - ["insert_below_dataloader_definition_line", "insert_below_model_definition_line"]
+    - ["insert_below_model_definition_line", "insert_below_input_definition_line"]
   content:
     - |-
-      [+] def eval_func(model):
-      [+]     EVAL_FUNC_LINES
-      [+] try:
-      [+]     torch.backends.quantized.engine = 'onednn'
-      [+] except:
-      [+]     from torch.backends.quantized import engine; engine = 'onednn'
-      [+] from neural_compressor.conf.config import QuantConf
-      [+] from neural_compressor.experimental import Quantization, common
-      [+] quant_config = QuantConf()
-      [+] quant_config.usr_cfg.model.framework = "pytorch_fx"
-      [+] quantizer = Quantization(quant_config)
-      [+] quantizer.model = common.Model(MODEL_NAME)
-      [+] quantizer.calib_dataloader = DATALOADER_NAME
-      [+] quantizer.eval_func = eval_func
-      [+] MODEL_NAME = quantizer()
-      [+] MODEL_NAME = MODEL_NAME.model
-      [+] MODEL_NAME.eval()
-      [+] try:
+      [+] if "quantize" not in str(type(MODEL_NAME)) and "jit" not in str(type(MODEL_NAME)):
+      [+]     import torch
+      [+]     import intel_extension_for_pytorch as ipex
+      [+]     qconfig = ipex.quantization.default_dynamic_qconfig
+      [+]     MODEL_NAME = ipex.quantization.prepare(MODEL_NAME, qconfig, example_inputs=INPUT_NAME, inplace=False)
       [+]     with torch.no_grad():
-      [+]         MODEL_NAME = torch.jit.script(MODEL_NAME)
-      [+]     MODEL_NAME = torch.jit.freeze(MODEL_NAME)
-      [+] except:
-      [+]     pass
-      
+      [+]         for i in range(10):
+      [+]             INFERENCE_LINE
+      [+]     MODEL_NAME = ipex.quantization.convert(MODEL_NAME)
+      [+]     with torch.no_grad():
+      [+]         INFERENCE_LINE
+      [+]     MODEL_NAME.eval()
   order:
     - below:
+        - pytorch_channels_last
       above:
         - pytorch_jit_script
         - pytorch_jit_script_ofi
         - pytorch_jit_trace
         - pytorch_jit_trace_ofi
-        - pytorch_channels_last
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_inc_static_quant_ipex.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_inc_dynamic_quant.yaml`

 * *Files 19% similar despite different names*

```diff
@@ -10,29 +10,35 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 transformation:
   location:
-    - ["insert_below_dataloader_definition_line", "insert_below_model_definition_line"]
+    - insert_below_model_definition_line
   content:
     - |-
-      [+] from neural_compressor.conf.config import QuantConf
-      [+] from neural_compressor.experimental import Quantization, common
-      [+] from neural_compressor import options
-      [+] options.pytorch.quantization.use_bf16 = False
-      [+] quant_config = QuantConf()
-      [+] quant_config.usr_cfg.model.framework = "pytorch_ipex"
-      [+] quantizer = Quantization(quant_config)
-      [+] quantizer.model = common.Model(MODEL_NAME)
-      [+] quantizer.calib_dataloader = DATALOADER_NAME
-      [+] MODEL_NAME = quantizer()
-      [+] MODEL_NAME = MODEL_NAME.model
+      [+] def eval_func(model):
+      [+]     EVAL_FUNC_LINES
+      [+] try:
+      [+]     torch.backends.quantized.engine = 'onednn'
+      [+] except:
+      [+]     from torch.backends.quantized import engine; engine = 'onednn'
+      [+] from neural_compressor.config import PostTrainingQuantConfig
+      [+] from neural_compressor.quantization import fit
+      [+] conf = PostTrainingQuantConfig(approach="dynamic", quant_level=1)
+      [+] MODEL_NAME = fit(model=MODEL_NAME, conf=conf, eval_func=eval_func)
+      [+] MODEL_NAME.save("./quantized_model")
       [+] MODEL_NAME.eval()
+      [+] try:
+      [+]     with torch.no_grad():
+      [+]         MODEL_NAME = torch.jit.script(MODEL_NAME)
+      [+]     MODEL_NAME = torch.jit.freeze(MODEL_NAME)
+      [+] except:
+      [+]     pass
   order:
     - below:
       above:
         - pytorch_jit_script
         - pytorch_jit_script_ofi
         - pytorch_jit_trace
         - pytorch_jit_trace_ofi
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_ipex_bf16.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_ipex_bf16.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_ipex_fp32.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_ipex_fp32.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_ipex_int8_dynamic_quant.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_jit_trace.yaml`

 * *Files 15% similar despite different names*

```diff
@@ -13,27 +13,24 @@
 # limitations under the License.
 
 transformation:
   location:
     - ["insert_below_model_definition_line", "insert_below_input_definition_line"]
   content:
     - |-
-      [+] if "quantize" not in str(type(MODEL_NAME)) and "jit" not in str(type(MODEL_NAME)):
+      [+] if "jit" not in str(type(MODEL_NAME)):
       [+]     import torch
-      [+]     import intel_extension_for_pytorch as ipex
-      [+]     qconfig = ipex.quantization.default_dynamic_qconfig
-      [+]     MODEL_NAME = ipex.quantization.prepare(MODEL_NAME, qconfig, example_inputs=INPUT_NAME, inplace=False)
       [+]     with torch.no_grad():
-      [+]         for i in range(10):
-      [+]             INFERENCE_LINE
-      [+]     MODEL_NAME = ipex.quantization.convert(MODEL_NAME)
-      [+]     with torch.no_grad():
-      [+]         INFERENCE_LINE
-      [+]     MODEL_NAME.eval()
+      [+]         MODEL_NAME.eval()
+      [+]         MODEL_NAME = torch.jit.trace(MODEL_NAME, INPUT_NAME, strict=False, check_trace=False)
+      [+]         MODEL_NAME = torch.jit.freeze(MODEL_NAME)
   order:
     - below:
+        - pytorch_inc_static_quant_fx
+        - pytorch_inc_static_quant_ipex
+        - pytorch_inc_dynamic_quant
+        - pytorch_ipex_fp32
+        - pytorch_ipex_bf16
+        - pytorch_ipex_int8_static_quant
+        - pytorch_ipex_int8_dynamic_quant
         - pytorch_channels_last
       above:
-        - pytorch_jit_script
-        - pytorch_jit_script_ofi
-        - pytorch_jit_trace
-        - pytorch_jit_trace_ofi
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_ipex_int8_static_quant.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_jit_trace_ofi.yaml`

 * *Files 19% similar despite different names*

```diff
@@ -13,27 +13,23 @@
 # limitations under the License.
 
 transformation:
   location:
     - ["insert_below_model_definition_line", "insert_below_input_definition_line"]
   content:
     - |-
-      [+] if "quantize" not in str(type(MODEL_NAME)) and "jit" not in str(type(MODEL_NAME)):
+      [+] if "jit" not in str(type(MODEL_NAME)):
       [+]     import torch
-      [+]     import intel_extension_for_pytorch as ipex
-      [+]     qconfig = ipex.quantization.default_static_qconfig
-      [+]     MODEL_NAME = ipex.quantization.prepare(MODEL_NAME, qconfig, example_inputs=INPUT_NAME, inplace=False)
       [+]     with torch.no_grad():
-      [+]         for i in range(10):
-      [+]             INFERENCE_LINE
-      [+]     MODEL_NAME = ipex.quantization.convert(MODEL_NAME)
-      [+]     with torch.no_grad():
-      [+]         INFERENCE_LINE
-      [+]     MODEL_NAME.eval()
+      [+]         MODEL_NAME.eval()
+      [+]         MODEL_NAME = torch.jit.optimize_for_inference(torch.jit.trace(MODEL_NAME, INPUT_NAME, strict=False, check_trace=False))
   order:
     - below:
+        - pytorch_inc_static_quant_fx
+        - pytorch_inc_static_quant_ipex
+        - pytorch_inc_dynamic_quant
+        - pytorch_ipex_fp32
+        - pytorch_ipex_bf16
+        - pytorch_ipex_int8_static_quant
+        - pytorch_ipex_int8_dynamic_quant
         - pytorch_channels_last
       above:
-        - pytorch_jit_script
-        - pytorch_jit_script_ofi
-        - pytorch_jit_trace
-        - pytorch_jit_trace_ofi
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_jit_script.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_jit_script_ofi.yaml`

 * *Files 23% similar despite different names*

```diff
@@ -17,16 +17,15 @@
     - insert_below_model_definition_line
   content:
     - |-
       [+] if "jit" not in str(type(MODEL_NAME)):
       [+]     import torch
       [+]     with torch.no_grad():
       [+]         MODEL_NAME.eval()
-      [+]         MODEL_NAME = torch.jit.script(MODEL_NAME)
-      [+]         MODEL_NAME = torch.jit.freeze(MODEL_NAME)
+      [+]         MODEL_NAME = torch.jit.optimize_for_inference(torch.jit.script(MODEL_NAME))
   order:
     - below:
         - pytorch_inc_static_quant_fx
         - pytorch_inc_static_quant_ipex
         - pytorch_inc_dynamic_quant
         - pytorch_ipex_fp32
         - pytorch_ipex_bf16
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_jit_script_ofi.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_inc_bf16.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -13,23 +13,27 @@
 # limitations under the License.
 
 transformation:
   location:
     - insert_below_model_definition_line
   content:
     - |-
-      [+] if "jit" not in str(type(MODEL_NAME)):
-      [+]     import torch
+      [+] import torch
+      [+] torch.backends.quantized.engine = 'onednn'
+      [+] from neural_compressor.config import MixedPrecisionConfig
+      [+] from neural_compressor import mix_precision
+      [+] config = MixedPrecisionConfig()
+      [+] MODEL_NAME = mix_precision.fit(model=MODEL_NAME, config=config)
+      [+] try:
       [+]     with torch.no_grad():
-      [+]         MODEL_NAME.eval()
-      [+]         MODEL_NAME = torch.jit.optimize_for_inference(torch.jit.script(MODEL_NAME))
+      [+]         MODEL_NAME = torch.jit.script(MODEL_NAME)
+      [+]     MODEL_NAME = torch.jit.freeze(MODEL_NAME)
+      [+] except:
+      [+]     pass
   order:
     - below:
-        - pytorch_inc_static_quant_fx
-        - pytorch_inc_static_quant_ipex
-        - pytorch_inc_dynamic_quant
-        - pytorch_ipex_fp32
-        - pytorch_ipex_bf16
-        - pytorch_ipex_int8_static_quant
-        - pytorch_ipex_int8_dynamic_quant
+      above:
+        - pytorch_jit_script
+        - pytorch_jit_script_ofi
+        - pytorch_jit_trace
+        - pytorch_jit_trace_ofi
         - pytorch_channels_last
-      above:
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_jit_trace.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_inc_static_quant_fx.yaml`

 * *Files 21% similar despite different names*

```diff
@@ -10,27 +10,37 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 transformation:
   location:
-    - ["insert_below_model_definition_line", "insert_below_input_definition_line"]
+    - ["insert_below_dataloader_definition_line", "insert_below_model_definition_line"]
   content:
     - |-
-      [+] if "jit" not in str(type(MODEL_NAME)):
-      [+]     import torch
+      [+] def eval_func(model):
+      [+]     EVAL_FUNC_LINES
+      [+] try:
+      [+]     torch.backends.quantized.engine = 'onednn'
+      [+] except:
+      [+]     from torch.backends.quantized import engine; engine = 'onednn'
+      [+] from neural_compressor.config import PostTrainingQuantConfig
+      [+] from neural_compressor.quantization import fit
+      [+] conf = PostTrainingQuantConfig(quant_level=1)
+      [+] MODEL_NAME = fit(model=MODEL_NAME, conf=conf, calib_dataloader=DATALOADER_NAME, eval_func=eval_func)
+      [+] MODEL_NAME.save("./quantized_model")
+      [+] MODEL_NAME.eval()
+      [+] try:
       [+]     with torch.no_grad():
-      [+]         MODEL_NAME.eval()
-      [+]         MODEL_NAME = torch.jit.trace(MODEL_NAME, INPUT_NAME, strict=False, check_trace=False)
-      [+]         MODEL_NAME = torch.jit.freeze(MODEL_NAME)
+      [+]         MODEL_NAME = torch.jit.script(MODEL_NAME)
+      [+]     MODEL_NAME = torch.jit.freeze(MODEL_NAME)
+      [+] except:
+      [+]     pass
+      
   order:
     - below:
-        - pytorch_inc_static_quant_fx
-        - pytorch_inc_static_quant_ipex
-        - pytorch_inc_dynamic_quant
-        - pytorch_ipex_fp32
-        - pytorch_ipex_bf16
-        - pytorch_ipex_int8_static_quant
-        - pytorch_ipex_int8_dynamic_quant
-        - pytorch_channels_last
       above:
+        - pytorch_jit_script
+        - pytorch_jit_script_ofi
+        - pytorch_jit_trace
+        - pytorch_jit_trace_ofi
+        - pytorch_channels_last
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_jit_trace_ofi.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_inc_static_quant_ipex.yaml`

 * *Files 19% similar despite different names*

```diff
@@ -10,26 +10,24 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 transformation:
   location:
-    - ["insert_below_model_definition_line", "insert_below_input_definition_line"]
+    - ["insert_below_dataloader_definition_line", "insert_below_model_definition_line"]
   content:
     - |-
-      [+] if "jit" not in str(type(MODEL_NAME)):
-      [+]     import torch
-      [+]     with torch.no_grad():
-      [+]         MODEL_NAME.eval()
-      [+]         MODEL_NAME = torch.jit.optimize_for_inference(torch.jit.trace(MODEL_NAME, INPUT_NAME, strict=False, check_trace=False))
+      [+] from neural_compressor.config import PostTrainingQuantConfig
+      [+] from neural_compressor.quantization import fit
+      [+] conf = PostTrainingQuantConfig(backend='ipex', quant_level=1)
+      [+] MODEL_NAME = fit(model=MODEL_NAME, conf=conf, calib_dataloader=DATALOADER_NAME)
+      [+] MODEL_NAME.save("./quantized_model")
+      [+] MODEL_NAME.eval()
   order:
     - below:
-        - pytorch_inc_static_quant_fx
-        - pytorch_inc_static_quant_ipex
-        - pytorch_inc_dynamic_quant
-        - pytorch_ipex_fp32
-        - pytorch_ipex_bf16
-        - pytorch_ipex_int8_static_quant
-        - pytorch_ipex_int8_dynamic_quant
-        - pytorch_channels_last
       above:
+        - pytorch_jit_script
+        - pytorch_jit_script_ofi
+        - pytorch_jit_trace
+        - pytorch_jit_trace_ofi
+        - pytorch_channels_last
```

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_mixed_precision_cpu.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_mixed_precision_cpu.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_mixed_precision_cuda.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_mixed_precision_cuda.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_mixed_precision_intel_gpu.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_mixed_precision_intel_gpu.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_torchdynamo_jit_script.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_torchdynamo_jit_script.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_torchdynamo_jit_script_ofi.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_torchdynamo_jit_script_ofi.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_torchdynamo_jit_trace.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_torchdynamo_jit_trace.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/pytorch_torchdynamo_jit_trace_ofi.yaml` & `neural_compressor-2.2/neural_coder/backends/pytorch_torchdynamo_jit_trace_ofi.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/backends/template.yaml` & `neural_compressor-2.2/neural_coder/backends/template.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/__init__.py` & `neural_compressor-2.2/neural_coder/coders/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/autoinc/__init__.py` & `neural_compressor-2.2/neural_coder/coders/autoinc/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/autoinc/autoinc_harness.py` & `neural_compressor-2.2/neural_coder/coders/autoinc/autoinc_harness.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/autoinc/calib_dataloader.py` & `neural_compressor-2.2/neural_coder/coders/autoinc/calib_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/autoinc/domain.py` & `neural_compressor-2.2/neural_coder/coders/autoinc/domain.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/autoinc/eval_func.py` & `neural_compressor-2.2/neural_coder/coders/autoinc/eval_func.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/pytorch/__init__.py` & `neural_compressor-2.2/neural_coder/coders/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/pytorch/batch_size.py` & `neural_compressor-2.2/neural_coder/coders/pytorch/batch_size.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/pytorch/change_trainer_to_nlptrainer.py` & `neural_compressor-2.2/neural_coder/coders/pytorch/change_trainer_to_nlptrainer.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/pytorch/cuda_to_cpu.py` & `neural_compressor-2.2/neural_coder/coders/pytorch/cuda_to_cpu.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/pytorch/dummy_dataloader.py` & `neural_compressor-2.2/neural_coder/coders/pytorch/dummy_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/pytorch/harness.py` & `neural_compressor-2.2/neural_coder/coders/pytorch/harness.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/pytorch/lightning.py` & `neural_compressor-2.2/neural_coder/coders/pytorch/lightning.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/pytorch/reclaim_inference_transformers_trainer.py` & `neural_compressor-2.2/neural_coder/coders/pytorch/reclaim_inference_transformers_trainer.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/pytorch/reclaim_inputs.py` & `neural_compressor-2.2/neural_coder/coders/pytorch/reclaim_inputs.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/tensorflow/__init__.py` & `neural_compressor-2.2/neural_coder/coders/tensorflow/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/tensorflow/amp.py` & `neural_compressor-2.2/neural_coder/coders/tensorflow/amp.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/coders/tensorflow/inc.py` & `neural_compressor-2.2/neural_coder/coders/tensorflow/inc.py`

 * *Files 15% similar despite different names*

```diff
@@ -24,22 +24,20 @@
         # pdb.set_trace()
         lines = self.file.split('\n')
         for line in lines:
             if self.is_modify(line):
                 model_name = "model"
                 indent_level = get_line_indent_level(line)
                 self.result.append(line)
-                self.result.append(" " * indent_level + "from neural_compressor.conf.config import QuantConf")
-                self.result.append(" " * indent_level + "from neural_compressor.experimental import Quantization")
-                self.result.append(" " * indent_level + "from neural_compressor.experimental import common")
-                self.result.append(" " * indent_level + "quant_config = QuantConf()")
-                self.result.append(" " * indent_level + "quant_config.usr_cfg.model.framework = 'tensorflow'")
-                self.result.append(" " * indent_level + "quantizer = Quantization(quant_config)")
-                self.result.append(" " * indent_level + "quantizer.model = common.Model(" + model_name + ")")
-                self.result.append(" " * indent_level + model_name + " = quantizer.fit()")
+                self.result.append(" " * indent_level + "from neural_compressor.quantization import fit")
+                self.result.append(" " * indent_level + "from neural_compressor.config import PostTrainingQuantConfig")
+                self.result.append(" " * indent_level + "from neural_compressor import common")
+                self.result.append(" " * indent_level + "config = PostTrainingQuantConfig(quant_level=1)")
+                self.result.append(" " * indent_level + model_name + " = fit(" + model_name + ", conf=config)")
+                self.result.append(" " * indent_level + model_name + '.save("./quantized_model")')
             else:
                 self.result.append(line)
         for index, line in enumerate(self.result):
             if index != len(self.result)-1:
                 self.result[index] += '\n'
         return ''.join(self.result)
```

### Comparing `neural_compressor-2.1.1/neural_coder/coders/transform.py` & `neural_compressor-2.2/neural_coder/coders/transform.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/globals.py` & `neural_compressor-2.2/neural_coder/globals.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/graphers/__init__.py` & `neural_compressor-2.2/neural_coder/graphers/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/graphers/code_line.py` & `neural_compressor-2.2/neural_coder/graphers/code_line.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/graphers/function.py` & `neural_compressor-2.2/neural_coder/graphers/function.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/graphers/model.py` & `neural_compressor-2.2/neural_coder/graphers/model.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/graphers/preloads/__init__.py` & `neural_compressor-2.2/neural_coder/graphers/preloads/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/graphers/preloads/transformers.yaml` & `neural_compressor-2.2/neural_coder/graphers/preloads/transformers.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/interface.py` & `neural_compressor-2.2/neural_coder/interface.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/launcher.py` & `neural_compressor-2.2/neural_coder/launcher.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/utils/__init__.py` & `neural_compressor-2.2/neural_coder/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/utils/common.py` & `neural_compressor-2.2/neural_coder/utils/common.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/utils/cpu_info.py` & `neural_compressor-2.2/neural_coder/utils/cpu_info.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/utils/device.py` & `neural_compressor-2.2/neural_coder/utils/device.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/utils/handle_user_input.py` & `neural_compressor-2.2/neural_coder/utils/handle_user_input.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/utils/line_operation.py` & `neural_compressor-2.2/neural_coder/utils/line_operation.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/utils/numa_launcher.py` & `neural_compressor-2.2/neural_coder/utils/numa_launcher.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/utils/pdf_report.py` & `neural_compressor-2.2/neural_coder/utils/pdf_report.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_coder/version.py` & `neural_compressor-2.2/neural_coder/version.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/__init__.py` & `neural_compressor-2.2/neural_compressor/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -12,18 +12,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Intel Neural Compressor: An open-source Python library supporting popular model compression techniques."""
-from .benchmark import _Benchmark
 from .version import __version__
-from .contrib import *
 # we need to set a global 'NA' backend, or Model can't be used
-from .utils.utility import set_random_seed, set_tensorboard, set_workspace
-from .utils import options
-from .conf.config import conf
-from .conf.pythonic_config import config
 from .config import DistillationConfig, PostTrainingQuantConfig, \
                     WeightPruningConfig, QuantizationAwareTrainingConfig, \
                     MixedPrecisionConfig
+from .contrib import *
+from .model import *
+from .metric import *
+from .utils import options
+from .utils.utility import set_random_seed, set_tensorboard, set_workspace, set_resume_from
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/adaptor.py` & `neural_compressor-2.2/neural_compressor/adaptor/adaptor.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/keras.py` & `neural_compressor-2.2/neural_compressor/adaptor/keras.py`

 * *Files 18% similar despite different names*

```diff
@@ -35,21 +35,26 @@
   """Map all the quantized objects."""
   from neural_compressor.adaptor.keras_utils.quantizer import Quantize, DeQuantize
   from neural_compressor.adaptor.keras_utils.quantizer import FakeQuant
   from neural_compressor.adaptor.keras_utils.conv2d import QConv2D
   from neural_compressor.adaptor.keras_utils.depthwise_conv2d import QDepthwiseConv2D
   from neural_compressor.adaptor.keras_utils.separable_conv2d import QSeparableConv2D
   from neural_compressor.adaptor.keras_utils.dense import QDense
+  from neural_compressor.adaptor.keras_utils.pool2d import QMaxPool2D, QAvgPool2D
   custom_objects["Quantize"] = Quantize
   custom_objects["DeQuantize"] = DeQuantize
   custom_objects["FakeQuant"] = FakeQuant
   custom_objects["QConv2D"] = QConv2D
   custom_objects["QDepthwiseConv2D"] = QDepthwiseConv2D
   custom_objects["QSeparableConv2D"] = QSeparableConv2D
   custom_objects["QDense"] = QDense
+  custom_objects["QMaxPool2D"] = QMaxPool2D
+  custom_objects["QAvgPool2D"] = QAvgPool2D
+  custom_objects["QMaxPooling2D"] = QMaxPool2D
+  custom_objects["QAveragePooling2D"] = QAvgPool2D
   return custom_objects
 
 @adaptor_registry
 class KerasAdaptor(Adaptor):
     '''The keras class of framework adaptor layer.
 
     '''
@@ -58,19 +63,21 @@
         self.framework_specific_info = framework_specific_info
         self.approach = deep_get(self.framework_specific_info, 'approach', False)
         self.quantize_config = {'op_wise_config': {}}
         self.device = self.framework_specific_info['device']
         #self.work_dir = os.path.abspath(self.framework_specific_info['workspace_path'])
         self.recipes = deep_get(self.framework_specific_info, 'recipes', {})
         #os.makedirs(self.work_dir, exist_ok=True)
-        self.supported_op = ['Conv2D', 'Dense', 'SeparableConv2D', 'DepthwiseConv2D']
+        self.supported_op = ['Conv2D', 'Dense', 'SeparableConv2D', 'DepthwiseConv2D', 'AveragePooling2D', 
+        'MaxPooling2D', 'AvgPool2D', 'MaxPool2D']
 
         self.pre_optimized_object = None
         self.pre_optimized_model = None
         self.pre_optimizer_handle = None
+        self.bf16_ops = []
         self.fp32_ops = []
         self.query_handler = KerasQuery(local_config_file=os.path.join(
             os.path.dirname(__file__), 'keras.yaml'))
 
         self.fp32_results = []
         self.fp32_preds_as_label = False
         self.benchmark = (GLOBAL_STATE.STATE == MODE.BENCHMARK)
@@ -80,23 +87,31 @@
         self.conv_format = {}
 
     def tuning_cfg_to_fw(self, tuning_cfg):
         self.quantize_config['calib_iteration'] = tuning_cfg['calib_iteration']
         self.quantize_config['device'] = self.device
         self.quantize_config['advance'] = deep_get(tuning_cfg, 'advance')
         fp32_ops = []
+        bf16_ops = []
+        bf16_type = set(self.query_handler.get_op_types_by_precision(precision='bf16'))
         dispatched_op_names = [j[0] for j in tuning_cfg['op']]
         invalid_op_names = [i for i in self.quantize_config['op_wise_config']
                             if i not in dispatched_op_names]
 
         for op_name in invalid_op_names:
             self.quantize_config['op_wise_config'].pop(op_name)
 
         for each_op_info in tuning_cfg['op']:
             op_name = each_op_info[0]
+
+            if tuning_cfg['op'][each_op_info]['activation']['dtype'] == 'bf16':
+                if each_op_info[1] in bf16_type:
+                    bf16_ops.append(op_name)
+                continue
+
             if tuning_cfg['op'][each_op_info]['activation']['dtype'] == 'fp32':
                 if op_name in self.quantize_config['op_wise_config']:
                     self.quantize_config['op_wise_config'].pop(op_name)
                     fp32_ops.append(op_name)
                 continue
 
             is_perchannel = False
@@ -110,14 +125,16 @@
             is_asymmetric = False
             if 'activation' in tuning_cfg['op'][each_op_info]:
                 is_asymmetric = tuning_cfg['op'][each_op_info]['activation']['scheme'] == 'asym'
             self.quantize_config['op_wise_config'][op_name] = (is_perchannel,
                                                                algorithm,
                                                                is_asymmetric,
                                                                weight_bit)
+        self.bf16_ops = bf16_ops
+        self.bf16_ops.pop(-1)
         self.fp32_ops = fp32_ops
 
     def _pre_optimize(self, model):
         model = self._check_quantize_format(model)
         model = self._fuse_bn(model)
         return model
 
@@ -262,14 +279,18 @@
            Args:
                tune_cfg(dict): The chosen tuning configuration.
                model (object): The model to do quantization.
                dataloader(object): The dataloader used to load quantization dataset.
                q_func (optional): training function for quantization aware training mode.
         '''
         self.tuning_cfg_to_fw(tune_cfg)
+        # just convert the input model to mixed_bfloat16
+        if self.bf16_ops and not self.quantize_config['op_wise_config']:
+            converted_model = self.convert_bf16()
+            return converted_model
         logger.debug("Dump quantization configurations:")
         logger.debug(self.quantize_config)
         calib_sampling_size = tune_cfg.get('calib_sampling_size', 1)
         if isinstance(dataloader, BaseDataLoader):
             batch_size = dataloader.batch_size
             for i in range(batch_size):
                 if calib_sampling_size % (batch_size - i) == 0:
@@ -390,38 +411,61 @@
                 q_layers.append(dequantize_layer)
             elif layer['class_name'] in self.supported_op and \
                 layer['config']['name'] in self.quantize_config['op_wise_config']:
                 # index 0 is weight, index 1 is bias
                 q_layer_name = 'Q' + layer['class_name']
                 # this is for inbounds search
                 q_name  = layer['config']['name']
-                kernel = self.layer_weights[layer['config']['name']][0]
-                dim = list(range(0, kernel.ndim)) 
-                t_dim = [dim.pop(-1)]
-                t_dim.extend(dim)
-                channel_size = kernel.shape[-1]
-                kernel_channel = kernel.transpose(t_dim).reshape(channel_size, -1)
-                layer_config['min_value'] = json.dumps(\
-                        np.min(kernel_channel, axis=1).tolist())
-                layer_config['max_value'] = json.dumps(\
-                        np.max(kernel_channel, axis=1).tolist())
+                # for layers that have weights
+                if layer['config']['name'] in self.layer_weights:
+                    kernel = self.layer_weights[layer['config']['name']][0]
+                    dim = list(range(0, kernel.ndim)) 
+                    t_dim = [dim.pop(-1)]
+                    t_dim.extend(dim)
+                    channel_size = kernel.shape[-1]
+                    kernel_channel = kernel.transpose(t_dim).reshape(channel_size, -1)
+                    layer_config['min_value'] = json.dumps(\
+                            np.min(kernel_channel, axis=1).tolist())
+                    layer_config['max_value'] = json.dumps(\
+                            np.max(kernel_channel, axis=1).tolist())
+                else:
+                    # default value, but never expected to be used
+                    # cause no kernel weights for this layer
+                    layer_config['min_value'] = json.dumps([-10000])
+                    layer_config['max_value'] = json.dumps([10000])
                 layer_config['name'] = q_name
                 q_layer = {'class_name': q_layer_name,
                            'name': q_name,
                            'config': layer_config}
                 if 'inbound_nodes' in layer:
                     q_layer['inbound_nodes'] = inbound_reverse_map[layer['name']]
                 q_layers.append(q_layer)
             else:
                 q_layers.append(layer)
 
         json_model['config']['layers'] = q_layers
         quantized_model = self._restore_model_from_json(json_model)
         return quantized_model
 
+    def convert_bf16(self):
+        '''Execute the BF16 conversion.
+        '''
+        tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')
+        json_model = copy.deepcopy(json.loads(self.pre_optimized_object.to_json()))
+
+        for layer in json_model['config']['layers']:
+            if layer['config']['name'] in self.bf16_ops:
+                layer['config']['dtype'] = 'mixed_bfloat16'
+            
+        converted_model = self._restore_model_from_json(json_model)
+        tf.keras.mixed_precision.set_global_policy('float32')
+        
+        from neural_compressor.model.keras_model import KerasModel
+        converted_model = KerasModel(converted_model)
+        return converted_model
 
     #(TODO) choose the properly quantize mode
     def _check_quantize_mode(self, json_model):
         config = json_model["config"]
         layers = config["layers"]
         for idx,  layer in enumerate(layers):
             if 'ReLU' in layer['class_name']:
@@ -506,20 +550,23 @@
     def query_fw_capability(self, model):
         '''The function is used to return framework tuning capability.
 
            Args:
                model (object): The model to query quantization tuning capability.
         '''
         fp32_config = {'weight': {'dtype': 'fp32'}, 'activation': {'dtype': 'fp32'}}
+        bf16_config = {'weight': {'dtype': 'bf16'}, 'activation': {'dtype': 'bf16'}}
         int8_type = self.query_handler.get_op_types_by_precision(precision='int8')
         op_capability = self.query_handler.get_quantization_capability()
         conv_config = copy.deepcopy(op_capability['int8']['Conv2D'])
         conv_config = copy.deepcopy(op_capability['int8']['SeparableConv2D'])
         conv_config = copy.deepcopy(op_capability['int8']['DepthwiseConv2D'])
         dense_config = copy.deepcopy(op_capability['int8']['Dense'])
+        maxpool_config = copy.deepcopy(op_capability['int8']['MaxPooling2D'])
+        avgpool_config = copy.deepcopy(op_capability['int8']['AveragePooling2D'])
         other_config = copy.deepcopy(op_capability['int8']['default'])
 
         # # get fp32 layer weights
         keras_object = model._model_object
         self.conv_weights = {}
         self.bn_weights = {}
         self.layer_weights = {}
@@ -541,19 +588,23 @@
         self.fp32_layers = config["layers"]
 
         quantizable_op_details = OrderedDict()
         for details in self.fp32_layers:
             node_op = details['class_name']
             node_name = details['config']['name']
             if node_op == 'Conv2D': 
-                quantizable_op_details[(node_name, node_op)] = [conv_config, fp32_config]
+                quantizable_op_details[(node_name, node_op)] = [conv_config, bf16_config, fp32_config]
             elif node_op == 'Dense':
-                quantizable_op_details[(node_name, node_op)] = [dense_config, fp32_config]
+                quantizable_op_details[(node_name, node_op)] = [dense_config, bf16_config, fp32_config]
+            elif node_op in {'AveragePooling2D', 'AvgPool2D'}:
+                quantizable_op_details[(node_name, node_op)] = [avgpool_config, bf16_config, fp32_config]
+            elif node_op in {'MaxPooling2D', 'MaxPool2D'}:
+                quantizable_op_details[(node_name, node_op)] = [maxpool_config, bf16_config, fp32_config]
             else:
-                quantizable_op_details[(node_name, node_op)] = [fp32_config]
+                quantizable_op_details[(node_name, node_op)] = [bf16_config, fp32_config]
 
         capability = {
             'opwise': copy.deepcopy(quantizable_op_details),
             'optypewise': self.get_optype_wise_ability(quantizable_op_details),
         }
         logger.debug("Dump framework quantization capability:")
         logger.debug(capability)
@@ -693,14 +744,137 @@
            Args:
                model (neural_compressor.model): base model to be converted.
                source (string): The source model format.
                destination (string): The destination model format.
         '''
         pass
 
+    def _pre_hook_for_hvd(self, dataloader=None):
+        """Pre hook for Horovod."""
+        import horovod.tensorflow as hvd
+        self.hvd = hvd
+        self.hvd.init()
+
+    @dump_elapsed_time(customized_msg="Model training")
+    def train(self, model, dataloader, optimizer_tuple,
+              criterion_tuple, hooks, postprocess, **kwargs):
+        """Model training API.
+
+        Args:
+            model ([Graph, GraphDef or Path String]): The model could be the graph,
+                        graph_def object, the frozen pb or ckpt/savedmodel folder path.
+            dataloader (generator): generate the data and labels.
+            optimizer_tuple (tuple): optimizers for model training.
+            criterion_tuple (tuple): criterions for model training.
+            hooks (callback): on_epoch_begin hook on_epoch_end hook.
+            postprocess (object): process the result from the model.
+
+        Returns:
+            None.
+        """
+        # check model is savedmodel or not
+        import tensorflow as tf
+        from neural_compressor.model.tensorflow_model import get_model_type
+        tf.random.set_seed(1)
+        self.model_type = get_model_type(model._model)
+        optimizer = optimizer_tuple[0](**optimizer_tuple[1])
+        criterion = criterion_tuple[0](**criterion_tuple[1])
+        start_epochs = kwargs['kwargs'].get('start_epoch', None)
+        end_epochs = kwargs['kwargs'].get('end_epoch', None)
+        epochs = kwargs['kwargs'].get('epoch', None)
+        iters = kwargs['kwargs'].get('iteration', None)
+        callbacks = kwargs['kwargs'].get('callbacks', None)
+        execution_mode = kwargs['kwargs'].get('execution_mode', None)
+        distributed = getattr(dataloader, 'distributed', False)
+
+        if isinstance(model._model, tf.keras.Model):
+            input_model = model._model
+        else:
+            input_model = tf.keras.models.load_model(model._model)
+            # hooks = callbacks['tf_pruning'](model, input_model, hooks)
+        hooks['on_train_begin']()                 # on_train_begin hook
+        train_loss_results = []
+        if distributed:
+            try:
+                len_dataloader = len(dataloader)
+            except:
+                logger.info("The length of the distributed training dataloader is unknown."
+                            "When the iteration of training dataloader in each process is "
+                            "inconsistent, an error may occur.")
+            else:
+                list_len_dataloader = self.hvd.allgather_object(len_dataloader)
+                if self.hvd.rank() == 0:
+                    for i in range(len(list_len_dataloader)-1):
+                        if list_len_dataloader[i] != list_len_dataloader[i+1]:
+                            raise AttributeError("The traning dataloader's iteration is"
+                                                "different between processes, please reset dataloader's batch_size.")
+
+        def training_step(x, y, first_batch):
+            with tf.GradientTape() as tape:
+                tape.watch(input_model.trainable_variables)
+                y_ = input_model(x, training=True)
+                loss_value = criterion(y, y_)
+
+            tape = self.hvd.DistributedGradientTape(tape) if distributed else tape
+            # Get gradient
+            grads = tape.gradient(loss_value, input_model.trainable_variables) # pylint: disable=no-member
+            # Optimize the model
+            optimizer.apply_gradients(zip(grads, input_model.trainable_variables)) # pylint: disable=no-member
+            if distributed and first_batch:
+                self.hvd.broadcast_variables(input_model.variables, root_rank=0)
+                self.hvd.broadcast_variables(optimizer.variables(), root_rank=0)
+            return loss_value
+
+        training_step = training_step if execution_mode=='eager' else tf.function(training_step)
+        if start_epochs is not None and end_epochs is not None:
+            epochs = end_epochs - start_epochs
+        
+        for epoch in range(epochs):
+            cnt = 0
+            epoch_loss_avg = tf.keras.metrics.Mean()
+            # Training loop
+            for iter, data in enumerate(dataloader):
+                x, y = postprocess(data) if postprocess is not None else data
+                hooks['on_step_begin'](iter)      # on_step_begin hook
+                cnt += 1
+                loss_value = training_step(x, y, iter==0)
+                # Track progress
+                epoch_loss_avg.update_state(loss_value)  # Add current batch loss
+                hooks['on_before_optimizer_step']()  
+                hooks['on_after_optimizer_step']() 
+                if iters is not None and cnt >= iters:
+                    break
+            model._sess = None
+            # End epoch
+            train_loss_results.append(epoch_loss_avg.result())
+            if distributed:
+                logger.info("Epoch-{:03d} training on rank {!s} have been done." \
+                    .format(epoch+1, self.hvd.allgather_object(self.hvd.rank())))
+            logger.info("Epoch {:03d}: Loss: {:.3f}".format(epoch+1, epoch_loss_avg.result()))
+
+        hooks['on_train_end']()                  # on_train_end hook
+        model._sess = None
+
+        if distributed:
+            if self.hvd.rank() == 0:
+                # Update the input model with pruned weights manually due to keras API limitation.
+                if isinstance(model._model, tf.keras.Model):
+                    model._model = input_model
+                else:
+                    input_model.save(model._model)
+            rank_list = self.hvd.allgather_object(self.hvd.rank())
+            logger.info(f"rank 0 has saved the pruned model to '{model._model}',"
+                        f"all ranks {rank_list} ready.")
+        else:
+            if isinstance(model._model, tf.keras.Model):
+                model._model = input_model
+            else:
+                input_model.save(model._model)
+
+
 class KerasQuery(QueryBackendCapability):
     def __init__(self, local_config_file=None):
         super().__init__()
         self.version = tf.version.VERSION
         self.cfg = local_config_file
         self.cur_config = None
         self._one_shot_query()
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/keras_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/conv2d.py` & `neural_compressor-2.2/neural_compressor/adaptor/keras_utils/conv2d.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/dense.py` & `neural_compressor-2.2/neural_compressor/adaptor/keras_utils/dense.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/depthwise_conv2d.py` & `neural_compressor-2.2/neural_compressor/adaptor/keras_utils/depthwise_conv2d.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/quantizer.py` & `neural_compressor-2.2/neural_compressor/adaptor/keras_utils/quantizer.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/keras_utils/separable_conv2d.py` & `neural_compressor-2.2/neural_compressor/adaptor/keras_utils/separable_conv2d.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/mxnet.py` & `neural_compressor-2.2/neural_compressor/adaptor/mxnet.py`

 * *Files 1% similar despite different names*

```diff
@@ -21,15 +21,15 @@
 
 from neural_compressor.adaptor.adaptor import adaptor_registry, Adaptor
 from neural_compressor.adaptor.query import QueryBackendCapability
 from neural_compressor.utils.utility import (LazyImport, GLOBAL_STATE, MODE, CpuInfo,
                                              dump_elapsed_time, singleton)
 from neural_compressor.adaptor.mxnet_utils.util import *
 from collections import OrderedDict
-from ..experimental.data.dataloaders.base_dataloader import BaseDataLoader
+from neural_compressor.data.dataloaders.base_dataloader import BaseDataLoader
 from copy import deepcopy
 import math
 
 mx = LazyImport("mxnet")
 logger = logging.getLogger("neural_compressor")
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/mxnet.yaml` & `neural_compressor-2.2/neural_compressor/adaptor/mxnet.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/mxnet_utils/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/mxnet_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/mxnet_utils/util.py` & `neural_compressor-2.2/neural_compressor/adaptor/mxnet_utils/util.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/onnxrt.py` & `neural_compressor-2.2/neural_compressor/adaptor/onnxrt.py`

 * *Files 8% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 from importlib.util import find_spec
 from neural_compressor.adaptor.adaptor import adaptor_registry, Adaptor
 from neural_compressor.adaptor.query import QueryBackendCapability
 from neural_compressor.adaptor.ox_utils.util import PROVIDERS, ONNXRT_BACKENDS
 from neural_compressor.utils.utility import LazyImport, dump_elapsed_time, \
                                             GLOBAL_STATE, MODE
 from neural_compressor.utils.utility import Statistics
-from neural_compressor.experimental.data.dataloaders.base_dataloader import BaseDataLoader
+from neural_compressor.data.dataloaders.base_dataloader import BaseDataLoader
 from neural_compressor.conf.dotdict import deep_get
 from neural_compressor.utils.utility import CpuInfo
 import math
 import sys
 import re
 
 onnx = LazyImport("onnx")
@@ -113,15 +113,15 @@
                 local_config_file=os.path.join(os.path.dirname(__file__), config_file))
         else:
             self.query_handler = ONNXRTQuery(
                 dynamic=self.dynamic, 
                 static=self.static, 
                 format=self.format,
                 local_config_file=os.path.join(os.path.dirname(__file__), config_file))
- 
+
         self.work_space = framework_specific_info["workspace_path"]
         self.reduce_range = framework_specific_info["reduce_range"] if \
             "reduce_range" in framework_specific_info else not CpuInfo().vnni
         self.benchmark = (GLOBAL_STATE.STATE == MODE.BENCHMARK)
         os.makedirs(self.work_space, exist_ok=True)
         self.pre_optimized_model = None
         self.smooth_quant_model = None
@@ -129,15 +129,15 @@
 
         for precision in self.query_handler.get_precisions():
             if precision != 'fp32':
                 if self.device == 'cpu' and precision == 'fp16':
                     continue
                 self.quantizable_op_types += \
                     self.query_handler.get_op_types_by_precision(precision=precision)
- 
+
         if self.backend == 'TensorrtExecutionProvider':
             self.recipes['add_qdq_pair_to_weight'] = True
             self.recipes['dedicated_qdq_pair'] = True
             self.recipes['graph_optimization_level'] = 'DISABLE_ALL'
             self.recipes['optypes_to_exclude_output_quant'] = ['Conv', 'Gemm', 'Add', 'MatMul']
             self.static = True
             self.dynamic = False
@@ -148,95 +148,41 @@
         self.fp32_preds_as_label = False
         self.quantize_config = {} # adaptor should know current configs at any time
         self.quantize_params = {} # adaptor should know current params at any time
         self.min_max = None
 
         self.optype_statistics = None
 
-    def smooth_quant(self, model, dataloader, iterations, tune_cfg, alpha=0.5, folding=False,
-                                    percentile=99.999, op_types=['MatMul', 'Linear', 'Conv'], scales_per_op=True):
+    def smooth_quant(self, model, dataloader, iterations, tune_cfg, alpha=0.5, folding=True,
+            percentile=99.999, op_types=['MatMul', 'Gemm', 'Conv', 'FusedConv'], scales_per_op=True):
         """Get augmented model with smooth quant.
 
         Args:
-            model_wrapper: origin_model
-            dataloader: dataloader
-            iterations: iterations
-            tune_cfg: quantization config
-            alpha: smooth alpha in SmoothQuant, 1.0 will fallback to SPIQ
-            folding: whether insert mul(False) or just allow foldable layers(True) for SmoothQuant
-            percentile:Percentile of calibration to remove outliers
-            op_types: The op types whose input tensor will be dumped
-            scales_per_op: True, each op will have an individual scale, mainly for accuracy
-                           False, ops with the same input will share a scale, mainly for performance
+            model_wrapper (object): origin_model
+            dataloader (object): dataloader
+            iterations (int): iterations
+            tune_cfg (dict): quantization config
+            alpha (float or str): smooth alpha in SmoothQuant, 1.0 will fallback to SPIQ
+            folding (bool): whether fold those foldable Mul which are inserted for SmoothQuant
+            percentile (float): percentile of calibration to remove outliers
+            op_types (list): The op types whose input tensor will be dumped
+            scales_per_op (bool): True, each op will have an individual scale, mainly for accuracy
+                                  False, ops with the same input will share a scale, mainly for performance
 
         Returns:
             model: A modified onnx model
         """
         if self.smooth_quant_model is not None:
             return self.smooth_quant_model
-        from neural_compressor.adaptor.ox_utils.calibration import ONNXRTAugment
-        from onnx import numpy_helper
-        if isinstance(alpha, str):
-            logger.warning(f"onnx backend only support float alpha, reset alpha to 0.5 ")
-            alpha = 0.5
-        black_nodes = []
-        white_nodes = []
-        if tune_cfg is not None:
-            quantize_config = self._cfg_to_quantize_config(tune_cfg)
-            black_nodes = [node for node in quantize_config if quantize_config[node] == 'fp32']
-            white_nodes = [node for node in quantize_config if quantize_config[node] != 'fp32']
-        
-        augment = ONNXRTAugment(self.pre_optimized_model,
-                                dataloader, self.quantizable_op_types,
-                                black_nodes=black_nodes, white_nodes=white_nodes,
-                                iterations=list(range(0, iterations)),
-                                backend=self.backend, reduce_range=self.reduce_range)
-
-        max_vals_per_channel, shape_infos = augment.calib_smooth(percentile, op_types)
-
-        input_tensors_2_weights = {}
-        input_tensors_2_weights_nodes = {}
-        for name in max_vals_per_channel.keys():
-            curr_tensor_to_weight = []
-            curr_tensor_to_weight_nodes = []
-            nodes = self.pre_optimized_model.input_name_to_nodes[name]
-            for node in nodes:
-                if node.op_type not in op_types:
-                    continue
-                if len(node.input) >= 2:
-                    input = node.input[1]  ##TODO always dump the index 1 to get the weight
-                    if self.pre_optimized_model.get_initializer(input):
-                        weight = numpy_helper.to_array(self.pre_optimized_model.get_initializer(input))
-                        curr_tensor_to_weight.append(weight)
-                        curr_tensor_to_weight_nodes.append(node)
-            input_tensors_2_weights[name] = curr_tensor_to_weight
-            input_tensors_2_weights_nodes[name] = curr_tensor_to_weight_nodes
-
-        if scales_per_op:
-            from neural_compressor.adaptor.ox_utils.util import get_smooth_scales_per_op, \
-                insert_smooth_mul_op_per_op, adjust_weights_per_op
-            scales = get_smooth_scales_per_op(max_vals_per_channel, input_tensors_2_weights,
-                                                    input_tensors_2_weights_nodes, alpha)
-            new_added_mul_nodes, new_init_tensors, op_nodes = insert_smooth_mul_op_per_op(scales, shape_infos,
-                                                                                input_tensors_2_weights_nodes)
-            adjust_weights_per_op(self.pre_optimized_model, op_nodes, scales)
-        else:
-            from neural_compressor.adaptor.ox_utils.util import get_smooth_scales_per_input, \
-                insert_smooth_mul_op_per_input, adjust_weights_per_input
-            scales = get_smooth_scales_per_input(max_vals_per_channel, input_tensors_2_weights, alpha)
-            new_added_mul_nodes, new_init_tensors = insert_smooth_mul_op_per_input(scales, shape_infos,
-                                                                            input_tensors_2_weights_nodes)
-            adjust_weights_per_input(self.pre_optimized_model, input_tensors_2_weights_nodes, scales)
-
-        self.pre_optimized_model.add_nodes(new_added_mul_nodes)
-        self.pre_optimized_model.add_initializers(new_init_tensors)
-        self.pre_optimized_model.update()
-        self.pre_optimized_model.topological_sort()
-        self.pre_optimized_model.remove_unused_constant()
-        self.smooth_quant_model = self.pre_optimized_model
+
+        from .ox_utils.smooth_quant import ORTSmoothQuant
+        quantize_config = self._cfg_to_quantize_config(tune_cfg) if tune_cfg is not None else None
+        sq = ORTSmoothQuant(self.pre_optimized_model, dataloader, self.reduce_range, self.backend)
+        self.smooth_quant_model = sq.transform(
+            alpha, folding, percentile, op_types, scales_per_op, iterations, quantize_config)
         return self.smooth_quant_model
 
     @dump_elapsed_time("Pass quantize model")
     def quantize(self, tune_cfg, model, data_loader, q_func=None):
         """The function is used to do calibration and quanitization in post-training
            quantization.
 
@@ -322,38 +268,40 @@
                         format(calib_sampling_size, data_loader.batch_size,
                                data_loader.batch_size * iterations))
                 quantize_params = self._get_quantize_params(tmp_model, data_loader, \
                                           quantize_config, iterations)
         else:
             quantize_params = None
         self.quantize_params = quantize_params
+
         from neural_compressor.adaptor.ox_utils.quantizer import Quantizer
         from neural_compressor import options
+
         quantizer = Quantizer(tmp_model,
             quantize_config,
             format,
             self.static,
             quantize_params,
             self.quantizable_op_types,
             self.query_handler.get_fallback_list(),
             self.reduce_range,
             options.onnxrt.qdq_setting.AddQDQPairToWeight if \
-                not options.onnxrt.qdq_setting.AddQDQPairToWeight else \
+                'add_qdq_pair_to_weight' not in self.recipes else \
                 self.recipes.get('add_qdq_pair_to_weight', False),
             options.onnxrt.qdq_setting.OpTypesToExcludeOutputQuantizatioin if \
-                options.onnxrt.qdq_setting.OpTypesToExcludeOutputQuantizatioin is not None else \
+                'optypes_to_exclude_output_quant' not in self.recipes else \
                 self.recipes.get('optypes_to_exclude_output_quant', []),
             options.onnxrt.qdq_setting.DedicatedQDQPair if \
-                not options.onnxrt.qdq_setting.DedicatedQDQPair else \
-                self.recipes.get('dedicated_qdq_pair', False))
+                'dedicated_qdq_pair' not in self.recipes else \
+                self.recipes.get('dedicated_qdq_pair', False),
+            self.backend)
         quantizer.quantize_model()
         tmp_model.q_config = self._generate_qconfig(model.model, tune_cfg, quantize_params)
         tmp_model.model = quantizer.model.model
         self.quantize_config = quantize_config # update so other methods can know current configs
-
         self._dump_model_op_stats(tmp_model)
         tmp_model.topological_sort()
         return tmp_model
 
     def _generate_qconfig(self, model, tune_cfg, quantize_params):
         tune_cfg = copy.deepcopy(tune_cfg)
         for node in model.graph.node:
@@ -516,15 +464,15 @@
         black_nodes = [node for node in quantize_config if quantize_config[node]=='fp32']
         white_nodes = [node for node in quantize_config if quantize_config[node]!='fp32']
         augment = ONNXRTAugment(model, \
                   data_loader, self.quantizable_op_types, \
                   black_nodes=black_nodes, white_nodes=white_nodes, \
                   iterations=list(range(0, quantize_config['calib_iteration'])),
                   backend=self.backend, reduce_range=self.reduce_range)
-        self.min_max = augment.dump_minmax()
+        self.min_max = augment.dump_minmax(quantize_config)
         quantize_params = augment.dump_calibration(quantize_config, min_max=self.min_max)
         return quantize_params
 
     def inspect_tensor(self, model, dataloader, op_list=[],
                        iteration_list=[],
                        inspect_type='activation',
                        save_to_disk=False,
@@ -541,15 +489,16 @@
         if len(op_list) > 0 and isinstance(op_list, KeysView):
             op_list = [item[0] for item in op_list]
         augment = ONNXRTAugment(model, dataloader, [], \
                   iterations=iteration_list,
                   white_nodes=op_list,
                   backend=self.backend)
         tensors = augment.dump_tensor(activation=(inspect_type!='weight'),
-                                      weight=(inspect_type!='activation'))
+                                      weight=(inspect_type!='activation'),
+                                      format=self.format)
         if save_to_disk:
             if not save_path:
                 save_path = self.work_space
             dump_data_to_local(tensors, save_path, 'inspect_result.pkl')
         return tensors
 
     def set_tensor(self, model, tensor_dict):
@@ -637,65 +586,27 @@
                 is_nlp = True
                 break
         
         # 2. according to input
         # typically, NLP models have multiple inputs, 
         # and the dimension of each input is usually 2 (batch_size, max_seq_len)
         if not model.is_large_model:
-            sess = ort.InferenceSession(model.model.SerializeToString())
+            sess = ort.InferenceSession(model.model.SerializeToString(), providers=[self.backend])
         elif model.model_path is not None: # pragma: no cover
-            sess = ort.InferenceSession(model.model_path)
+            sess = ort.InferenceSession(model.model_path, providers=[self.backend])
         else: # pragma: no cover
             assert False, "Please use model path instead of onnx model object to quantize."
         input_shape_lens = [len(input.shape) for input in  sess.get_inputs()]
         if len(input_shape_lens) > 1 and all(shape_len == 2 for shape_len in input_shape_lens):
             is_nlp = True
 
         # 3. according to attention structure
-        for node in model.model.graph.node:
-            if node.op_type == 'Add':
-                start_node = node
-                qkv_nodes_list = [
-                    # match base attention structure
-                    model.match_parent_path(
-                        start_node,
-                        ["Add", "MatMul", "Reshape", "Transpose", "MatMul"],
-                        [0, None, 0, 0, 0],),
-                    model.match_parent_path(
-                        start_node,
-                        ["Add", "MatMul", "Reshape", "Transpose", "MatMul"],
-                        [1, None, 0, 0, 0]),
-
-                    # match gpt attention no past structure
-                    model.match_parent_path(
-                        start_node,
-                        ["Reshape", "Gemm", "Reshape", "Reshape", "Transpose", "MatMul"],
-                        [ None, 0, 0, 0, 0, 0],
-                        output_name_to_node=model.output_name_to_node,
-                        return_indice=[])
-                    ]
-                if not any(qkv_nodes_list):
-                    continue
-                qkv_nodes = [qkv for qkv in qkv_nodes_list if qkv is not None][-1]
-                other_inputs = []
-                for input in start_node.input:
-                    if input not in model.output_name_to_node:
-                        continue
-                    if input == qkv_nodes[0].output[0]:
-                        continue
-                    other_inputs.append(input)
-                if len(other_inputs) != 1:
-                    continue
-                root_input = other_inputs[0]
-                input_name_to_nodes = model.input_name_to_nodes
-                children = input_name_to_nodes[root_input]
-                children_types = [child.op_type for child in children]
-                if children_types.count("MatMul") == 3:
-                    is_nlp = True
-                    break
+        qkv = model.find_qkv_in_attention()
+        if len(qkv) != 0:
+            is_nlp = True
 
         # 4. according to LSTM/Attention optype
         op_types = [node.op_type for node in model.model.graph.node]
         if "LSTM" in op_types or 'Attention' in op_types:
             is_nlp = True
 
         logger.warning("The model is automatically detected as {} model. "
@@ -729,22 +640,23 @@
                 "to overwrite it".format(level))
         sess_options.graph_optimization_level = optimization_levels[level]
         sess_options.optimized_model_filepath = os.path.join(self.work_space, \
             "Optimized_model.onnx")
         if sys.version_info < (3,10) and find_spec('onnxruntime_extensions'): # pragma: no cover
             from onnxruntime_extensions import get_library_path
             sess_options.register_custom_ops_library(get_library_path())
+        backend = self.backend if self.backend != 'TensorrtExecutionProvider' else 'CUDAExecutionProvider'
         if not model.is_large_model:
             ort.InferenceSession(model.model.SerializeToString(),
                                  sess_options,
-                                 providers=[self.backend])
+                                 providers=[backend])
         elif model.model_path is not None: # pragma: no cover
             ort.InferenceSession(model.model_path,
                                  sess_options,
-                                 providers=[self.backend])
+                                 providers=[backend])
         else: # pragma: no cover 
             logger.warning('Please use model path instead of onnx model object to quantize')
 
         tmp_model = onnx.load(sess_options.optimized_model_filepath, load_external_data=False)
         if model.is_large_model: # pragma: no cover
             from onnx.external_data_helper import load_external_data_for_model
             load_external_data_for_model(tmp_model, os.path.split(model.model_path)[0])
@@ -992,42 +904,100 @@
 
         if self.format == "qdq":
             self._optypewise_filter_for_qdq(optype_wise)
 
         first_quantizable_node = []
         last_quantizable_node = []
         all_conv_matmul = []
+        attention_matmul = []
         for _, node in enumerate(self.pre_optimized_model.nodes()):
-            if node.op_type in ['Conv', 'MatMul']:
+            if node.op_type in ['Conv', 'MatMul', 'Attention']:
                 # get first Conv or MatMul node
                 if len(first_quantizable_node) == 0:
                     first_quantizable_node.append(node)
-                
+
                 # get last Conv or MatMul node
                 if len(last_quantizable_node) != 0:
                     last_quantizable_node.pop()
                 last_quantizable_node.append(node)
 
                 all_conv_matmul.append(node)
-
+                if node.op_type != 'Conv':
+                    attention_matmul.append(node)
+        
         if len(first_quantizable_node) != 0:
             recipes_ops['first_conv_or_matmul_quantization'] = [(first_quantizable_node[0].name, 
                                                                 first_quantizable_node[0].op_type)]
         if len(last_quantizable_node) != 0:
             recipes_ops['last_conv_or_matmul_quantization'] = [(last_quantizable_node[0].name, 
                                                                 last_quantizable_node[0].op_type)]
+        
+        
+        ffn_matmul = []
+        attention_matmul_optype = [node.op_type for node in attention_matmul]
+        # find matmul ops in feed forward network (FFN) structure whitch mainly in transfomers based NLP models
+        if len(attention_matmul) > 0 and 'Attention' in attention_matmul_optype:
+            # model is optimized and Attention is fused,
+            # index of Attention is used as split to find FFN MatMul
+            first_attention_index = attention_matmul_optype.index('Attention')
+            attention_matmul_optype = attention_matmul_optype[first_attention_index:]
+            attention_matmul = attention_matmul[first_attention_index:]
+            attention_index = list(np.where(np.array(attention_matmul_optype) == 'Attention')[0])
+            block_len = attention_index[1] - attention_index[0] if len(attention_index) > 2 else 4
+            for idx in range(len(attention_index)):
+                if idx != len(attention_index) - 1:
+                    index = attention_index[idx + 1]
+                    if index - 2 >= 0 and index - 1 >= 0:
+                        ffn_matmul.append([attention_matmul[index - 2], 
+                                           attention_matmul[index - 1]])
+                else:
+                    index = attention_index[idx]
+                    if index + block_len - 2 < len(attention_matmul) and \
+                        index + block_len - 1 < len(attention_matmul):
+                        ffn_matmul.append([attention_matmul[index + block_len - 2], 
+                                        attention_matmul[index + block_len - 1]])
+        else:
+            # model is not optimized or Attention isn't fused, 
+            # query MatMul, key MatMul and value MatMul are used as split to find FFN MatMul
+            qkv = self.pre_optimized_model.find_qkv_in_attention(find_all=True)
+            if len(qkv) != 0:
+                attention_starts = [nodes[0] for nodes in qkv]
+                attention_index = [np.where(np.array([n.name for n in attention_matmul]) \
+                                            == attention_start)[0].tolist()[0] \
+                                                for attention_start in attention_starts]
+                block_len = attention_index[1] - attention_index[0] if len(attention_index) > 2 else 4
+                for idx in range(len(attention_index)):
+                    if idx != len(attention_index) - 1:
+                        index = attention_index[idx + 1]
+                        if index - 2 >= 0 and index - 1 >= 0:
+                            ffn_matmul.append([attention_matmul[index - 2],
+                                            attention_matmul[index - 1]])
+                    else:
+                        index = attention_index[idx]
+                        if index + block_len - 2 < len(attention_matmul) and \
+                            index + block_len - 1 < len(attention_matmul):
+                            ffn_matmul.append([attention_matmul[index + block_len - 2],
+                                            attention_matmul[index + block_len - 1]])
+
+        block_wise = []
+        for block in reversed(ffn_matmul):
+            node_info = []
+            for node in block:
+                node_info.append((node.name, node.op_type))
+            if len(node_info) != 0:
+                block_wise.append(node_info)
 
         for _, node in enumerate(self.pre_optimized_model.nodes()):
             # for TRT EP, only insert Q/DQ to inputs of Add nodes followed by ReduceMean
             if node.op_type == 'Add' and self.backend == 'TensorrtExecutionProvider':
                 children = self.pre_optimized_model.get_children(node)
                 if 'ReduceMean' not in [i.op_type for i in children]:
                     op_wise.update({(node.name, node.op_type): 
                         [{'weight': {'dtype': 'fp32'}, 'activation': {'dtype': 'fp32'}}]})
-                continue
+                    continue
 
             if node.op_type in optype_wise:
                 if (exclude_first_quantizable_op and node in first_quantizable_node) \
                      or (exclude_last_quantizable_op and node in last_quantizable_node):
                     tmp_cfg = copy.deepcopy(optype_wise[node.op_type])
                     tmp_cfg = list(filter(lambda x:'quant_mode' not in x['activation'], tmp_cfg))
                     op_wise.update({(node.name, node.op_type): tmp_cfg})
@@ -1055,15 +1025,14 @@
                     backbone_nodes = self.pre_optimized_model.get_nodes_chain(backbone_queue_extra, 
                                                     first_quantizable_node, backbone_nodes)
             backbone_nodes += [i.name for i in first_quantizable_node]
             
             for _, node in enumerate(self.pre_optimized_model.nodes()):
                 if node.name not in backbone_nodes and node.op_type in optype_wise:
                     recipes_ops['pre_post_process_quantization'].append((node.name, node.op_type))
-
             if exclude_pre_post_process:
                 for _, node in enumerate(self.pre_optimized_model.nodes()):
                     if node.op_type in optype_wise:
                         # nodes not in backbone are not quantized
                         if node.name not in backbone_nodes:
                             tmp_cfg = copy.deepcopy(optype_wise[node.op_type])
                             tmp_cfg = list(filter(lambda x:'quant_mode' not in x['activation'], tmp_cfg))
@@ -1071,16 +1040,16 @@
                             continue
                         if (node.name, node.op_type) in op_wise:
                             op_wise.update(
                                 {(node.name, node.op_type): copy.deepcopy(op_wise[(node.name, node.op_type)])})
                         else: # pragma: no cover
                             op_wise.update(
                                 {(node.name, node.op_type): copy.deepcopy(optype_wise[node.op_type])})
-        
-        return {'optypewise': optype_wise, 'opwise': op_wise, 'recipes_ops': recipes_ops}
+
+        return {'optypewise': optype_wise, 'opwise': op_wise, 'recipes_ops': recipes_ops, 'block_wise': block_wise}
 
     def _optypewise_filter_for_qdq(self, optype_wise):
         """Filter optypes that don't support per_channel in QDQ format.
 
         Args:
             optype_wise (dict): optype and quantization config
         Returns:
@@ -1220,15 +1189,14 @@
                     else:
                         for i in range(len_inputs):
                             # in case dataloader contains non-array input
                             if not isinstance(inputs[i], np.ndarray):
                                 ort_inputs.update({inputs_names[i]: np.array(inputs[i])})
                             else:
                                 ort_inputs.update({inputs_names[i]: inputs[i]})
-
                 if measurer is not None:
                     measurer.start()
                     predictions = session.run(None, ort_inputs)
                     measurer.end()
                 else:
                     predictions = session.run(None, ort_inputs)
 
@@ -1294,27 +1262,152 @@
         if self.min_max:
             for node_name in inspect_node_list:
                 node = find_by_name(node_name, fp32_model.nodes())
                 filtered_params[node_name] = {
                     'min': np.array(self.min_max[node.output[0]][0], dtype=np.float32),
                     'max': np.array(self.min_max[node.output[0]][1], dtype=np.float32)}
         if save_path:
-            dump_data_to_local(filtered_params, save_path, 'dequan_min_max.pkl')
+            dump_data_to_local(filtered_params, save_path, 'activation_min_max.pkl')
             dump_data_to_local(tune_cfg, save_path, 'cfg.pkl')
         return inspect_node_list, tune_cfg
 
     def save(self, model, path):
         """ save model
 
         Args:
             model (ModelProto): model to save
             path (str): save path
         """
         model.save(os.path.join(path, "best_model.onnx"))
 
+    def get_output_op_names(self, qmodel):
+        """Get the ouput ops' names."""
+        outputs = qmodel.output()
+        output_op_names = []
+        for output in outputs:
+            output_op_names.append(qmodel.output_name_to_node[output].name)
+        logger.debug(f"output op names: {output_op_names}")
+        return output_op_names
+
+    def calculate_op_sensitivity(self, model, dataloader, tune_cfg, output_op_names, 
+                                 confidence_batches, fallback=True, requantize_cfgs=None):
+        """Compute the op sensitivity.
+        
+        The sensitivity metric is the mse between the output of the last quantized op of 
+        the quantized model and the output of its corresponding op in the fp32 model.
+        
+          1. Backup the tune cfg
+          2. Fallback each int8 op and compute its mse if use fallback (with 'fallback == True'),
+            or re-quantize each fp32 op(fallen back in the previous stage) and compute its MSE if not.
+          3. Sorted op name list according to its MSE
+        
+        Args:
+          fp32_model: The fp32 model.
+          dataloader: the dataloader with full dataset.
+          tune_cfg: tuning config
+          fallback: denote fallback stage or re-quantize stage
+          requantize_cfgs: the dict of tuning configs for all re-quantizable ops
+
+        Returns:
+          A list of op names, sorted by its MSE sensitivity.
+        """
+        from copy import deepcopy
+
+        fp32_op_cfg = {'activation': {'dtype': 'fp32', 'quant_mode': 'fp32'},
+                       'weight': {'dtype': 'fp32'}}
+
+        if fallback:
+            ops_list = [op for op, config in tune_cfg['op'].items()
+                       if config['activation']['quant_mode'] in ('static', 'dynamic')]
+            replace_cfgs = {op : fp32_op_cfg for op in tune_cfg['op']}
+        else:
+            ops_list = [op for op, config in tune_cfg['op'].items() 
+                       if config['activation']['quant_mode'] == 'fp32' and op in requantize_cfgs]
+            replace_cfgs = requantize_cfgs
+
+        # Step2. compute mse
+        mse_result = self._get_mse_order(
+            model, deepcopy(tune_cfg), replace_cfgs, ops_list, dataloader, 
+            output_op_names, confidence_batches)
+
+        # Step3. sort
+        mse_order = [op for op, _ in sorted(mse_result.items(), key=lambda i: i[1])]
+        logger.debug("Dump MSE order:")
+        for op in mse_order:
+            logger.debug(f"{op}: {mse_result[op]}")
+        return mse_order
+
+    def _get_mse_order(self, fp32_model, tune_cfg, replace_cfgs, ops_lst, dataloader, 
+                       output_op_names, confidence_batches):
+        """Compute MSE."""
+        op_cfg = tune_cfg['op']
+        mse_result = {}
+        
+        fp32_output = self._inference_model_on_batches(
+            fp32_model, tune_cfg, dataloader, output_op_names, confidence_batches)
+
+        for op in ops_lst:
+            # backup and set replace tuning config
+            backup_cfg = op_cfg[op] 
+            op_cfg[op] = replace_cfgs[op]
+
+            # quantize and inference the model
+            q_model = self.quantize(tune_cfg, fp32_model, dataloader)
+            q_output = self._inference_model_on_batches(
+                q_model, tune_cfg, dataloader, output_op_names, confidence_batches)
+
+            mse_result[op] = self._calculate_mse(fp32_output, q_output)
+
+            # recover tune_cfg
+            op_cfg[op] = backup_cfg
+
+        return mse_result
+
+    def _calculate_mse(self, fp32_output, q_output):
+        """MSE calculation."""
+        result = []
+        for i, j in zip(fp32_output, q_output):
+            result.append(np.square(i - j).mean())
+        return np.array(result).mean()
+
+    def _inference_model_on_batches(self, model, tune_cfg, dataloader,
+                                    output_op_name, iterations):
+        """Inference model on batches."""
+        ort_inputs = {}
+        predictions = []
+
+        session = ort.InferenceSession(self.work_space + 'eval.onnx',
+                                       providers=[self.backend]) if model.is_large_model else \
+                  ort.InferenceSession(model.model.SerializeToString(),
+                                       providers=[self.backend])
+        inputs_names = [i.name for i in session.get_inputs()]
+        len_inputs = len(session.get_inputs())
+        for idx, (inputs, _) in enumerate(dataloader):
+            if idx + 1 > iterations:
+                break
+            if len_inputs == 1:
+                ort_inputs.update(
+                    inputs if isinstance(inputs, dict) else {inputs_names[0]: inputs}
+                )
+            else:
+                assert len_inputs == len(inputs), \
+                    'number of input tensors must align with graph inputs'
+
+                if isinstance(inputs, dict):  # pragma: no cover
+                    ort_inputs.update(inputs)
+                else:
+                    for i in range(len_inputs):
+                        # in case dataloader contains non-array input
+                        if not isinstance(inputs[i], np.ndarray):
+                            ort_inputs.update({inputs_names[i]: np.array(inputs[i])})
+                        else:
+                            ort_inputs.update({inputs_names[i]: inputs[i]})
+
+            predictions.extend(session.run(None, ort_inputs))
+        return predictions
 
 @adaptor_registry
 class ONNXRT_QLinearOpsAdaptor(ONNXRUNTIMEAdaptor):
     """The ONNXRT adaptor layer, do onnx-rt quantization, calibration, inspect layer tensors.
 
     Args:
         framework_specific_info (dict): framework specific configuration for quantization.
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/onnxrt.yaml` & `neural_compressor-2.2/neural_compressor/adaptor/onnxrt_cuda.yaml`

 * *Files 3% similar despite different names*

```diff
@@ -26,15 +26,15 @@
                     'granularity': ['per_channel', 'per_tensor'],
                     'algorithm': ['minmax']
                     },
         'activation': &uint8_asym_pertensor_minmax {
                     'dtype': ['uint8'],
                     'scheme': ['asym'],
                     'granularity': ['per_tensor'],
-                    'algorithm': ['minmax']
+                    'algorithm': ['minmax', 'kl', 'percentile']
                     },
         'mode': ['QDQ', 'QLinear']
       },
       'FusedConv': {
         'weight':   *int8_sym_perchanneltensor_minmax, #'QDQ': *int8_sym_pertensor_minmax
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
@@ -93,14 +93,24 @@
       },
       'Gather': *default_dynamic,
       'Attention': *default_dynamic,
       'EmbedLayerNormalization': *default_dynamic,
       'LSTM': *default_dynamic,
     }
   }
+  fp16: &common_fp16 ['Concat', 'Gather', 'Reshape', 'Squeeze', 'Transpose', 'Unsqueeze',
+    'EmbedLayerNormalization', 'Attention', 'Split', 'Sigmoid', 'Relu', 'Mul', 'Pad', 'MaxPool',
+    'MatMul', 'LeakyRelu',  'GlobalAveragePool', 'Gemm', 'Conv', 'AveragePool', 'Add', 'Clip',
+    'BatchNormalization', 'Softmax', 'Sum', 'Abs', 'BiasGelu', 'Exp', 'FastGelu',
+    'Gelu', 'Log', 'Round', 'Sigmoid', 'Sqrt', 'Tanh', 'Sub', 'Mul', 'Div', 'Pow',
+    'ReduceMean', 'Equal', 'FusedMatMul', 'Greater', 'GreaterOrEqual', 'Less', 'LessOrEqual',
+    'ReduceL1', 'ReduceL2', 'ReduceLogSum', 'ReduceLogSumExp', 'ReduceMax', 'ReduceProd',
+    'ReduceSum', 'ReduceSumSquare', 'LayerNormalization', 'Concat']
+  bf16: &common_bf16 ['Concat', 'Gather', 'Reshape', 'Squeeze', 'Transpose', 'Unsqueeze',
+    'Split', 'Sigmoid', 'Relu', 'Mul', 'MatMul', 'Gemm', 'Add']
   recipes: &default_optimization
     graph_optimization:   # from onnxruntime graph_optimization_level
       level: ['DISABLE_ALL', 'ENABLE_BASIC', 'ENABLE_EXTENDED', 'ENABLE_ALL']
 
 -
   version:
     name: '1.7.0'
@@ -133,14 +143,16 @@
       'GlobalAveragePool': *default_static_qlinear_qdq,
       'Pad': *default_static_qlinear_qdq,
       'Split': *default_static_qlinear_qdq,
       'Add': *default_static_qlinear,
     },
     'dynamic': *ref_1_6_dynamic 
   }
+  fp16: *common_fp16
+  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: '1.8.0'
   int8: {
@@ -200,14 +212,16 @@
       },
       'Gather': *default_dynamic,
       'Attention': *default_dynamic,
       'EmbedLayerNormalization': *default_dynamic,
       'LSTM': *default_dynamic,
     }
   }
+  fp16: *common_fp16
+  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: '1.9.0'
   int8: {
@@ -274,14 +288,16 @@
         'activation': *uint8_asym_pertensor_minmax
       },
       'Gather': *default_dynamic,
       'Attention': *default_dynamic,
       'LSTM': *default_dynamic,
     }
   }
+  fp16: *common_fp16
+  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: '1.10.0'
   int8: {
@@ -328,14 +344,16 @@
       'AveragePool': *default_static_qlinear_qdq,
       'Unsqueeze': *default_static_qlinear_qdq,
       'Transpose': *default_static_qlinear_qdq,
       'Resize': *default_static_qlinear_qdq,
     },
     'dynamic': *ref_1_9_dynamic
   }
+  fp16: *common_fp16
+  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: '1.11.0'
   int8: &ref_1_11 {
@@ -389,23 +407,29 @@
       'Transpose': *default_static_qlinear_qdq,
       'ArgMax': *default_static_qlinear,
       'Resize': *default_static_qlinear_qdq,
 
     },
     'dynamic': *ref_1_9_dynamic
   }
+  fp16: *common_fp16
+  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: '1.12.0'
   int8: *ref_1_11
+  fp16: *common_fp16
+  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: 'default'
   int8: *ref_1_6
+  fp16: *common_fp16
+  bf16: *common_bf16
   recipes:
-    <<: *default_optimization
+    <<: *default_optimization
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/onnxrt_cuda.yaml` & `neural_compressor-2.2/neural_compressor/adaptor/onnxrt.yaml`

 * *Files 12% similar despite different names*

```diff
@@ -22,64 +22,69 @@
       'Conv': {
         'weight':   &int8_sym_perchanneltensor_minmax {
                     'dtype': ['int8'],
                     'scheme': ['sym'],
                     'granularity': ['per_channel', 'per_tensor'],
                     'algorithm': ['minmax']
                     },
-        'activation': &uint8_asym_pertensor_minmax {
+        'activation': &uint8_asym_pertensor {
                     'dtype': ['uint8'],
                     'scheme': ['asym'],
                     'granularity': ['per_tensor'],
-                    'algorithm': ['minmax']
+                    'algorithm': ['minmax', 'kl', 'percentile']
                     },
         'mode': ['QDQ', 'QLinear']
       },
       'FusedConv': {
         'weight':   *int8_sym_perchanneltensor_minmax, #'QDQ': *int8_sym_pertensor_minmax
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': &uint8_asym_pertensor_minmax {
+                    'dtype': ['uint8'],
+                    'scheme': ['asym'],
+                    'granularity': ['per_tensor'],
+                    'algorithm': ['minmax']
+                    },
         'mode': ['QDQ', 'QLinear']
       },
       'Gather': {
         'weight':   &uint8_asym_perchanneltensor_minmax {
                     'dtype': ['uint8'],
                     'scheme': ['asym'],
                     'granularity': ['per_channel', 'per_tensor'],
                     'algorithm': ['minmax']
                     }, 
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
-      'MatMul': {
+      'MatMul': &default_static_qlinear_qdq {
         'weight':   &int8_sym_pertensor_minmax {
                     'dtype': ['int8'],
                     'scheme': ['sym'],
                     'granularity': ['per_tensor'],
                     'algorithm': ['minmax']
                     }, 
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': *uint8_asym_pertensor,
         'mode': ['QDQ', 'QLinear']
       },
-      'Attention': &default_static_qlinear_qdq {
+      'Attention': &default_static_qlinear_qdq_minmax {
         'weight':   *int8_sym_pertensor_minmax,
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'Mul': &default_static_qlinear {
         'weight':   *int8_sym_pertensor_minmax,
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QLinear']
       },
-      'Relu': *default_static_qlinear_qdq, 
-      'Clip': *default_static_qlinear_qdq,
-      'LeakyRelu': *default_static_qlinear_qdq,
-      'Sigmoid': *default_static_qlinear_qdq,
-      'MaxPool': *default_static_qlinear_qdq,
-      'EmbedLayerNormalization': *default_static_qlinear_qdq,
-      'GlobalAveragePool': *default_static_qlinear_qdq,
+      'Relu': *default_static_qlinear_qdq_minmax, 
+      'Clip': *default_static_qlinear_qdq_minmax,
+      'LeakyRelu': *default_static_qlinear_qdq_minmax,
+      'Sigmoid': *default_static_qlinear_qdq_minmax,
+      'MaxPool': *default_static_qlinear_qdq_minmax,
+      'EmbedLayerNormalization': *default_static_qlinear_qdq_minmax,
+      'GlobalAveragePool': *default_static_qlinear_qdq_minmax,
       'Add': *default_static_qlinear,
     },
     'dynamic': &ref_1_6_dynamic {
       'Conv': {
         'weight':   *uint8_asym_perchanneltensor_minmax,
         'activation': *uint8_asym_pertensor_minmax
       },
@@ -93,19 +98,14 @@
       },
       'Gather': *default_dynamic,
       'Attention': *default_dynamic,
       'EmbedLayerNormalization': *default_dynamic,
       'LSTM': *default_dynamic,
     }
   }
-  fp16: &common_fp16 ['Concat', 'Gather', 'Reshape', 'Squeeze', 'Transpose', 'Unsqueeze',
-    'EmbedLayerNormalization', 'Attention', 'Split', 'Sigmoid', 'Relu', 'Mul', 'Pad', 'MaxPool',
-    'MatMul', 'LeakyRelu',  'GlobalAveragePool', 'Gemm', 'Conv', 'AveragePool', 'Add', 'Clip']
-  bf16: &common_bf16 ['Concat', 'Gather', 'Reshape', 'Squeeze', 'Transpose', 'Unsqueeze',
-    'Split', 'Sigmoid', 'Relu', 'Mul', 'MatMul', 'Gemm', 'Add']
   recipes: &default_optimization
     graph_optimization:   # from onnxruntime graph_optimization_level
       level: ['DISABLE_ALL', 'ENABLE_BASIC', 'ENABLE_EXTENDED', 'ENABLE_ALL']
 
 -
   version:
     name: '1.7.0'
@@ -114,40 +114,38 @@
       'FusedConv': {
         'weight': *int8_sym_perchanneltensor_minmax, #'QDQ': *int8_sym_pertensor_minmax
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'Conv': {
         'weight':   *int8_sym_perchanneltensor_minmax,
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': *uint8_asym_pertensor,
         'mode': ['QDQ', 'QLinear']
       },
       'Gather': {
         'weight':   *uint8_asym_perchanneltensor_minmax,
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'MatMul': *default_static_qlinear_qdq,
-      'Attention': *default_static_qlinear_qdq,
+      'Attention': *default_static_qlinear_qdq_minmax,
       'Mul': *default_static_qlinear,
-      'Relu': *default_static_qlinear_qdq,
-      'Clip': *default_static_qlinear_qdq,
-      'LeakyRelu': *default_static_qlinear_qdq,
-      'Sigmoid': *default_static_qlinear_qdq,
-      'MaxPool': *default_static_qlinear_qdq,
-      'EmbedLayerNormalization': *default_static_qlinear_qdq,
-      'GlobalAveragePool': *default_static_qlinear_qdq,
-      'Pad': *default_static_qlinear_qdq,
-      'Split': *default_static_qlinear_qdq,
+      'Relu': *default_static_qlinear_qdq_minmax,
+      'Clip': *default_static_qlinear_qdq_minmax,
+      'LeakyRelu': *default_static_qlinear_qdq_minmax,
+      'Sigmoid': *default_static_qlinear_qdq_minmax,
+      'MaxPool': *default_static_qlinear_qdq_minmax,
+      'EmbedLayerNormalization': *default_static_qlinear_qdq_minmax,
+      'GlobalAveragePool': *default_static_qlinear_qdq_minmax,
+      'Pad': *default_static_qlinear_qdq_minmax,
+      'Split': *default_static_qlinear_qdq_minmax,
       'Add': *default_static_qlinear,
     },
     'dynamic': *ref_1_6_dynamic 
   }
-  fp16: *common_fp16
-  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: '1.8.0'
   int8: {
@@ -155,46 +153,46 @@
       'FusedConv': {
         'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'Conv': {
         'weight':   *int8_sym_perchanneltensor_minmax,
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': *uint8_asym_pertensor,
         'mode': ['QDQ', 'QLinear']
       },
       'Gather': {
         'weight':   *uint8_asym_perchanneltensor_minmax,
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'MatMul': {
         'weight':  *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': *uint8_asym_pertensor,
         'mode': ['QDQ', 'QLinear']
       },
-      'Attention': *default_static_qlinear_qdq,
+      'Attention': *default_static_qlinear_qdq_minmax,
       'Mul': *default_static_qlinear,
-      'Relu': *default_static_qlinear_qdq,
-      'Clip': *default_static_qlinear_qdq,
-      'LeakyRelu': *default_static_qlinear_qdq,
-      'Sigmoid': *default_static_qlinear_qdq,
-      'MaxPool': *default_static_qlinear_qdq,
-      'EmbedLayerNormalization': *default_static_qlinear_qdq,
-      'GlobalAveragePool': *default_static_qlinear_qdq,
-      'Pad': *default_static_qlinear_qdq,
-      'Split': *default_static_qlinear_qdq,
+      'Relu': *default_static_qlinear_qdq_minmax,
+      'Clip': *default_static_qlinear_qdq_minmax,
+      'LeakyRelu': *default_static_qlinear_qdq_minmax,
+      'Sigmoid': *default_static_qlinear_qdq_minmax,
+      'MaxPool': *default_static_qlinear_qdq_minmax,
+      'EmbedLayerNormalization': *default_static_qlinear_qdq_minmax,
+      'GlobalAveragePool': *default_static_qlinear_qdq_minmax,
+      'Pad': *default_static_qlinear_qdq_minmax,
+      'Split': *default_static_qlinear_qdq_minmax,
       'Add': *default_static_qlinear,
-      'Squeeze': *default_static_qlinear_qdq,
-      'Reshape': *default_static_qlinear_qdq,
-      'Concat': *default_static_qlinear_qdq,
-      'AveragePool': *default_static_qlinear_qdq,
-      'Unsqueeze': *default_static_qlinear_qdq,
-      'Transpose': *default_static_qlinear_qdq,
-      'Resize': *default_static_qlinear_qdq,
+      'Squeeze': *default_static_qlinear_qdq_minmax,
+      'Reshape': *default_static_qlinear_qdq_minmax,
+      'Concat': *default_static_qlinear_qdq_minmax,
+      'AveragePool': *default_static_qlinear_qdq_minmax,
+      'Unsqueeze': *default_static_qlinear_qdq_minmax,
+      'Transpose': *default_static_qlinear_qdq_minmax,
+      'Resize': *default_static_qlinear_qdq_minmax,
     },
     'dynamic': {
       'Conv': {
         'weight': *uint8_asym_perchanneltensor_minmax,
         'activation': *uint8_asym_pertensor_minmax
       },
       'FusedConv': {
@@ -207,16 +205,14 @@
       },
       'Gather': *default_dynamic,
       'Attention': *default_dynamic,
       'EmbedLayerNormalization': *default_dynamic,
       'LSTM': *default_dynamic,
     }
   }
-  fp16: *common_fp16
-  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: '1.9.0'
   int8: {
@@ -224,50 +220,50 @@
       'FusedConv': {
         'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'Conv': {
         'weight':   *int8_sym_perchanneltensor_minmax,
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': *uint8_asym_pertensor,
         'mode': ['QDQ', 'QLinear']
       },
       'Gather': {
         'weight':   *uint8_asym_perchanneltensor_minmax,
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'MatMul': {
         'weight':   *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': *uint8_asym_pertensor,
         'mode': ['QDQ', 'QLinear']
       },
       'EmbedLayerNormalization': {
         'weight': *uint8_asym_pertensor_minmax, # QDQ: *int8_sym_pertensor_minmax
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
-      'Attention': *default_static_qlinear_qdq,
+      'Attention': *default_static_qlinear_qdq_minmax,
       'Mul': *default_static_qlinear,
-      'Relu': *default_static_qlinear_qdq,
-      'Clip': *default_static_qlinear_qdq,
-      'LeakyRelu': *default_static_qlinear_qdq,
-      'Sigmoid': *default_static_qlinear_qdq,
-      'MaxPool': *default_static_qlinear_qdq,
-      'GlobalAveragePool': *default_static_qlinear_qdq,
-      'Pad': *default_static_qlinear_qdq,
-      'Split': *default_static_qlinear_qdq,
+      'Relu': *default_static_qlinear_qdq_minmax,
+      'Clip': *default_static_qlinear_qdq_minmax,
+      'LeakyRelu': *default_static_qlinear_qdq_minmax,
+      'Sigmoid': *default_static_qlinear_qdq_minmax,
+      'MaxPool': *default_static_qlinear_qdq_minmax,
+      'GlobalAveragePool': *default_static_qlinear_qdq_minmax,
+      'Pad': *default_static_qlinear_qdq_minmax,
+      'Split': *default_static_qlinear_qdq_minmax,
       'Add': *default_static_qlinear,
-      'Squeeze': *default_static_qlinear_qdq,
-      'Reshape': *default_static_qlinear_qdq,
-      'Concat': *default_static_qlinear_qdq,
-      'AveragePool': *default_static_qlinear_qdq,
-      'Unsqueeze': *default_static_qlinear_qdq,
-      'Transpose': *default_static_qlinear_qdq,
-      'Resize': *default_static_qlinear_qdq,
+      'Squeeze': *default_static_qlinear_qdq_minmax,
+      'Reshape': *default_static_qlinear_qdq_minmax,
+      'Concat': *default_static_qlinear_qdq_minmax,
+      'AveragePool': *default_static_qlinear_qdq_minmax,
+      'Unsqueeze': *default_static_qlinear_qdq_minmax,
+      'Transpose': *default_static_qlinear_qdq_minmax,
+      'Resize': *default_static_qlinear_qdq_minmax,
     },
     'dynamic': &ref_1_9_dynamic {
       'Conv': {
         'weight': *uint8_asym_pertensor_minmax,
         'activation': *uint8_asym_pertensor_minmax
       },
       'FusedConv': {
@@ -283,16 +279,14 @@
         'activation': *uint8_asym_pertensor_minmax
       },
       'Gather': *default_dynamic,
       'Attention': *default_dynamic,
       'LSTM': *default_dynamic,
     }
   }
-  fp16: *common_fp16
-  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: '1.10.0'
   int8: {
@@ -300,55 +294,53 @@
       'FusedConv': {
         'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'Conv': {
         'weight':   *int8_sym_perchanneltensor_minmax,
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': *uint8_asym_pertensor,
         'mode': ['QDQ', 'QLinear']
       },
       'Gather': {
         'weight':   *uint8_asym_perchanneltensor_minmax,
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'MatMul': {
         'weight':   *int8_sym_perchanneltensor_minmax,
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': *uint8_asym_pertensor,
         'mode': ['QDQ', 'QLinear']
       },
       'EmbedLayerNormalization': {
         'weight': *uint8_asym_pertensor_minmax, # QDQ: *int8_sym_pertensor_minmax
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
-      'Attention': *default_static_qlinear_qdq,
+      'Attention': *default_static_qlinear_qdq_minmax,
       'Mul': *default_static_qlinear,
-      'Relu': *default_static_qlinear_qdq,
-      'Clip': *default_static_qlinear_qdq,
-      'LeakyRelu': *default_static_qlinear_qdq,
-      'Sigmoid': *default_static_qlinear_qdq,
-      'MaxPool': *default_static_qlinear_qdq,
-      'GlobalAveragePool': *default_static_qlinear_qdq,
-      'Pad': *default_static_qlinear_qdq,
-      'Split': *default_static_qlinear_qdq,
+      'Relu': *default_static_qlinear_qdq_minmax,
+      'Clip': *default_static_qlinear_qdq_minmax,
+      'LeakyRelu': *default_static_qlinear_qdq_minmax,
+      'Sigmoid': *default_static_qlinear_qdq_minmax,
+      'MaxPool': *default_static_qlinear_qdq_minmax,
+      'GlobalAveragePool': *default_static_qlinear_qdq_minmax,
+      'Pad': *default_static_qlinear_qdq_minmax,
+      'Split': *default_static_qlinear_qdq_minmax,
       'Add': *default_static_qlinear,
-      'Squeeze': *default_static_qlinear_qdq,
-      'Reshape': *default_static_qlinear_qdq,
-      'Concat': *default_static_qlinear_qdq,
-      'AveragePool': *default_static_qlinear_qdq,
-      'Unsqueeze': *default_static_qlinear_qdq,
-      'Transpose': *default_static_qlinear_qdq,
-      'Resize': *default_static_qlinear_qdq,
+      'Squeeze': *default_static_qlinear_qdq_minmax,
+      'Reshape': *default_static_qlinear_qdq_minmax,
+      'Concat': *default_static_qlinear_qdq_minmax,
+      'AveragePool': *default_static_qlinear_qdq_minmax,
+      'Unsqueeze': *default_static_qlinear_qdq_minmax,
+      'Transpose': *default_static_qlinear_qdq_minmax,
+      'Resize': *default_static_qlinear_qdq_minmax,
     },
     'dynamic': *ref_1_9_dynamic
   }
-  fp16: *common_fp16
-  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: '1.11.0'
   int8: &ref_1_11 {
@@ -356,75 +348,69 @@
       'FusedConv': {
         'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'Conv': {
         'weight':   *int8_sym_perchanneltensor_minmax,
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': *uint8_asym_pertensor,
         'mode': ['QDQ', 'QLinear']
       },
       'Gather': {
         'weight':   *uint8_asym_perchanneltensor_minmax,
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'MatMul': {
         'weight':   *int8_sym_perchanneltensor_minmax,
-        'activation': *uint8_asym_pertensor_minmax,
+        'activation': *uint8_asym_pertensor,
         'mode': ['QDQ', 'QLinear']
       },
       'Gemm': {
         'weight':   *int8_sym_perchanneltensor_minmax,
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
       'EmbedLayerNormalization': {
         'weight': *uint8_asym_pertensor_minmax, # QDQ: *int8_sym_pertensor_minmax
         'activation': *uint8_asym_pertensor_minmax,
         'mode': ['QDQ', 'QLinear']
       },
-      'Attention': *default_static_qlinear_qdq,
+      'Attention': *default_static_qlinear_qdq_minmax,
       'Mul': *default_static_qlinear,
-      'Relu': *default_static_qlinear_qdq,
-      'Clip': *default_static_qlinear_qdq,
-      'LeakyRelu': *default_static_qlinear_qdq,
-      'Sigmoid': *default_static_qlinear_qdq,
-      'MaxPool': *default_static_qlinear_qdq,
-      'GlobalAveragePool': *default_static_qlinear_qdq,
-      'Pad': *default_static_qlinear_qdq,
-      'Split': *default_static_qlinear_qdq,
+      'Relu': *default_static_qlinear_qdq_minmax,
+      'Clip': *default_static_qlinear_qdq_minmax,
+      'LeakyRelu': *default_static_qlinear_qdq_minmax,
+      'Sigmoid': *default_static_qlinear_qdq_minmax,
+      'MaxPool': *default_static_qlinear_qdq_minmax,
+      'GlobalAveragePool': *default_static_qlinear_qdq_minmax,
+      'Pad': *default_static_qlinear_qdq_minmax,
+      'Split': *default_static_qlinear_qdq_minmax,
       'Add': *default_static_qlinear,
-      'Squeeze': *default_static_qlinear_qdq,
-      'Reshape': *default_static_qlinear_qdq,
-      'Concat': *default_static_qlinear_qdq,
-      'AveragePool': *default_static_qlinear_qdq,
-      'Unsqueeze': *default_static_qlinear_qdq,
-      'Transpose': *default_static_qlinear_qdq,
+      'Squeeze': *default_static_qlinear_qdq_minmax,
+      'Reshape': *default_static_qlinear_qdq_minmax,
+      'Concat': *default_static_qlinear_qdq_minmax,
+      'AveragePool': *default_static_qlinear_qdq_minmax,
+      'Unsqueeze': *default_static_qlinear_qdq_minmax,
+      'Transpose': *default_static_qlinear_qdq_minmax,
       'ArgMax': *default_static_qlinear,
-      'Resize': *default_static_qlinear_qdq,
+      'Resize': *default_static_qlinear_qdq_minmax,
 
     },
     'dynamic': *ref_1_9_dynamic
   }
-  fp16: *common_fp16
-  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: '1.12.0'
   int8: *ref_1_11
-  fp16: *common_fp16
-  bf16: *common_bf16
   recipes:
     <<: *default_optimization
 
 -
   version:
     name: 'default'
   int8: *ref_1_6
-  fp16: *common_fp16
-  bf16: *common_bf16
   recipes:
-    <<: *default_optimization
+    <<: *default_optimization
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/onnxrt_trt.yaml` & `neural_compressor-2.2/neural_compressor/adaptor/onnxrt_trt.yaml`

 * *Files 1% similar despite different names*

```diff
@@ -26,30 +26,30 @@
                     'granularity': ['per_tensor', 'per_channel'],
                     'algorithm': ['minmax']
                   },
         'activation': {
                     'dtype': ['int8'],
                     'scheme': ['sym'],
                     'granularity': ['per_tensor'],
-                    'algorithm': ['minmax']
+                    'algorithm': ['minmax', 'kl', 'percentile']
                   },
         'mode': ['QDQ']
       },
       'MatMul': &cap_s8_sym_pertensor_default {
         'weight': {
                     'dtype': ['int8'],
                     'scheme': ['sym'],
                     'granularity': ['per_tensor'],
                     'algorithm': ['minmax']
                   },
         'activation': {
                     'dtype': ['int8'],
                     'scheme': ['sym'],
                     'granularity': ['per_tensor'],
-                    'algorithm': ['minmax']
+                    'algorithm': ['minmax', 'kl', 'percentile']
                   },
         'mode': ['QDQ']
       },
       'Attention': *cap_s8_sym_pertensor_default,
       'LeakyRelu': *cap_s8_sym_pertensor_default,
       'Gather': *cap_s8_sym_default,
       'Sigmoid': *cap_s8_sym_pertensor_default,
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/calibration.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/calibration.py`

 * *Files 6% similar despite different names*

```diff
@@ -21,25 +21,27 @@
 # license information.
 # --------------------------------------------------------------------------
 """Calibration for onnx models."""
 
 import copy
 import logging
 import sys
-
+import os
 import numpy as np
 import onnx
 import onnxruntime
 import onnx.numpy_helper as numpy_helper
 from onnx import helper, TensorProto, shape_inference
 from packaging.version import Version
 from importlib.util import find_spec
 from neural_compressor.model.onnx_model import ONNXModel
 from neural_compressor.adaptor.ox_utils.util import make_dquant_node, is_B_transposed, \
     _get_qrange_for_qType, calculate_scale_zp
+from neural_compressor.adaptor.ox_utils.calibrator import CALIBRATOR
+from neural_compressor.adaptor.ox_utils.util import find_by_name
 
 logger = logging.getLogger("neural_compressor")
 ONNX18_VERSION = Version("1.8.0")
 ORT112_VERSION = Version("1.12.0")
 
 
 class ONNXRTAugment:
@@ -47,15 +49,15 @@
 
     def __init__(self, model_wrapper,
                  dataloader,
                  dump_op_types,
                  black_nodes=[],
                  white_nodes=[],
                  iterations=[],
-                 backend=['CPUExecutionProvider'],
+                 backend='CPUExecutionProvider',
                  reduce_range=False):
         """Initialization.
 
         Args:
             model_wrapper (Model): model to be augmented
             dataloader (object): user implemented object to read in and preprocess calibration dataset
             dump_op_types (list): operator types to be calibrated and quantized
@@ -143,28 +145,26 @@
                     for input in node.input:
                         if self.already_quantized and \
                                 input.replace('_dequantized', '_quantized') in initializers:
                             tensors_to_dump.add(input)
                         elif not self.already_quantized and input in initializers:
                             tensors_to_dump.add(input)
                 elif activation_only:
-                    tensors_to_dump.update(node.output)
+                    tensors_to_dump.update([node.input[0]])
 
         model_inputs = [i.name for i in model.graph.input]
         for tensor in tensors_to_dump:
             if tensor not in node_outputs and tensor not in initializers and \
                     tensor not in model_inputs:
                 continue
             if self.augment_nodes:
                 for augment_node_type in self.augment_nodes:
                     if augment_node_type in ['DequantizeLinear']:
                         # insert DequantizeLinear node as output
-                        if tensor.endswith('_scale') or tensor.endswith('_zero_point') or \
-                                tensor.endswith('_QuantizeLinear') or \
-                                tensor.endswith('_QuantizeInput_quantized'):
+                        if tensor.endswith('_scale') or tensor.endswith('_zero_point'):
                             continue
 
                         if not self.dynamically_quantized:
                             tensor = tensor.replace('_QuantizeInput', '_quantized') if \
                                 tensor.endswith('_QuantizeInput') else tensor
                         else:
                             tensor = tensor.replace('_output_quantized', '') if \
@@ -198,43 +198,60 @@
 
         self.augmented_model = model
         if self.model_wrapper.is_large_model:  # pragma: no cover
             onnx.save_model(model,
                             self.model_wrapper.model_path + '_augment.onnx',
                             save_as_external_data=True,
                             all_tensors_to_one_file=True,
-                            location="weights.pb",
                             convert_attribute=False)
 
-    def get_intermediate_outputs(self, calib_mode=None):
+    def get_intermediate_outputs(self, q_config=None):
         """Gather intermediate model outputs after running inference."""
         # conduct inference session and get intermediate outputs
         so = onnxruntime.SessionOptions()
         if sys.version_info < (3, 10) and find_spec('onnxruntime_extensions'):  # pragma: no cover
             from onnxruntime_extensions import get_library_path
             so.register_custom_ops_library(get_library_path())
 
+        backend = self.backend if self.backend != 'TensorrtExecutionProvider' else 'CUDAExecutionProvider'
         session = onnxruntime.InferenceSession(
                     self.augmented_model.SerializeToString(),
                     so,
-                    provider=self.backend) if not self.model_wrapper.is_large_model else \
+                    providers=[backend]) if not self.model_wrapper.is_large_model else \
                   onnxruntime.InferenceSession(
                     self.model_wrapper.model_path  + '_augment.onnx',
                     so,
-                    provider=self.backend)
+                    providers=[backend])
 
-        intermediate_outputs = []
+        
         len_inputs = len(session.get_inputs())
         inputs_names = [session.get_inputs()[i].name for i in range(len_inputs)]
-        output_dicts = {}
-
+        
         node_output_names = [output.name if output.name not in self.dequantized_output \
                                  else self.dequantized_output[output.name] \
                              for output in session.get_outputs()]
+        
+        augment_model_wrapper = ONNXModel(self.augmented_model) \
+            if not self.model_wrapper.is_large_model else \
+            ONNXModel(self.model_wrapper.model_path  + '_augment.onnx')
+        input_name_to_nodes = augment_model_wrapper.input_name_to_nodes
+        output_name_to_node = augment_model_wrapper.output_name_to_node
+        name_to_node = {}
+        for data_name in node_output_names:
+            node = None
+            if data_name in output_name_to_node:
+                node = output_name_to_node[data_name]
+            elif data_name in input_name_to_nodes:
+                node = input_name_to_nodes[data_name][0]
+            assert node, '{} is neither an input nor an output of nodes in augmented model.'.format(data_name)
+            name_to_node[data_name] = node.name
 
+        output_dicts = {}
+        intermediate_tensor = {}
+        name_to_calibrator = {}
         for idx, (inputs, labels) in enumerate(self.dataloader):
             ort_inputs = {}
             if len_inputs == 1:
                 ort_inputs.update(
                     inputs if isinstance(inputs, dict) else {inputs_names[0]: inputs}
                 )
             else:
@@ -244,34 +261,64 @@
                     ort_inputs.update(inputs)
                 else:
                     for i in range(len_inputs):
                         if not isinstance(inputs[i], np.ndarray):  # pragma: no cover
                             ort_inputs.update({inputs_names[i]: np.array(inputs[i])})
                         else:
                             ort_inputs.update({inputs_names[i]: inputs[i]})
+
+            def _collect_data():
+                for output_idx, output in enumerate(session.run(None, ort_inputs)):
+                    if q_config is not None and output.size != 0:
+                        node_name = name_to_node[node_output_names[output_idx]]
+                        if node_output_names[output_idx] not in name_to_calibrator:
+                            calib_method = q_config[node_name]['activation']['algorithm'] \
+                                if q_config and node_name in q_config \
+                                and 'activation' in q_config[node_name] else 'minmax'
+                            assert calib_method in CALIBRATOR, \
+                                'Calibration method {} is not registerd.'.format(calib_method)
+                            calibrator = CALIBRATOR[calib_method]()
+                        else:
+                            calibrator = name_to_calibrator[node_output_names[output_idx]]
+                        
+                        # currently, the calibration range for each iteration is collected if 
+                        # the calibration method is minmax, otherwise the tensor data is collected.
+                        # TODO: for kl and percentile method, need to support range collection 
+                        # per iteration in the future.
+                        if calibrator.method_name == 'minmax':
+                            calibrator.collect(output)
+                            output_dicts[node_output_names[output_idx]] = [list(calibrator.calib_range)]
+                            name_to_calibrator[node_output_names[output_idx]] = calibrator
+                        else:
+                            intermediate_tensor.setdefault(
+                                (node_output_names[output_idx], node_name), []).append(output)
+                    elif q_config is None:
+                        output_dicts.setdefault(node_output_names[output_idx], \
+                            []).append(output)
+                            
             if self.iterations != []:
                 if idx > max(self.iterations):
                     break
                 if idx in self.iterations:
-                    for output_idx, output in enumerate(session.run(None, ort_inputs)):
-                        if calib_mode == 'naive' and output.size != 0:
-                            output_dicts.setdefault(node_output_names[output_idx], \
-                                                    []).append([output.min(), output.max()])
-                        elif calib_mode == None:
-                            output_dicts.setdefault(node_output_names[output_idx], \
-                                                    []).append(output)
+                    _collect_data()
             else:
-                for output_idx, output in enumerate(session.run(None, ort_inputs)):
-                    if calib_mode == 'naive' and output.size != 0:
-                        output_dicts.setdefault(node_output_names[output_idx], \
-                                                []).append([output.min(), output.max()])
-                    elif calib_mode == None:
-                        output_dicts.setdefault(node_output_names[output_idx], \
-                                                []).append(output)
+                _collect_data()
 
+        # for kl and percentile method, collect calibration range after all tensors are collected.
+        merged_dict = intermediate_tensor
+        for (output_name, node_name), datas in merged_dict.items():
+            if any([data is None for data in datas]):
+                continue
+            calib_method = q_config[node_name]['activation']['algorithm'] \
+                if q_config and node_name in q_config and 'activation' in q_config[node_name] else 'minmax'
+            calibrator = CALIBRATOR[calib_method]()
+            calibrator.collect(datas)
+            output_dicts.setdefault(output_name, []).append(list(calibrator.calib_range))
+            calibrator.clear()
+            del calibrator
         return list(output_dicts.keys()), output_dicts
 
     def _dequantize(self, tensor, scale_tensor, zo_tensor):
         """Helper function to dequantize tensor."""
         int_tensor = self.model_wrapper.get_initializer(tensor)
         if int_tensor:  # weight tensor
             return self._dequantize_weight(tensor, scale_tensor, zo_tensor)
@@ -345,59 +392,50 @@
             inputs=[tensor_name + '_DequantizeLinear'],
             outputs=[tensor_name + '_output'],
             perm=(1, 0, 2, 3) if dim == 4 else (1, 0),
             name=tensor_name + '_post_transpose')
         added_nodes = [pre_transpose_node, dequantize_node, post_transpose_node]
         return added_nodes, tensor_name + '_output'
 
-    def _map_calibration(self, node_output_names, output_dicts, calib_mode='naive'):
+    def _map_calibration(self, node_output_names, output_dicts):
         """Map tensor names and min/max values."""
         merged_dict = {}
         for name, minmaxs in output_dicts.items():
             for minmax in minmaxs:
+                if len(minmax) < 2:
+                    continue
                 merged_dict.setdefault(name + '_Min', []).append(minmax[0])
                 merged_dict.setdefault(name + '_Max', []).append(minmax[1])
 
         # Characterizing distribution of a node's values across test data sets
         clean_merged_dict = dict((i, merged_dict[i]) for i in merged_dict)
-        if calib_mode == 'naive':
-            pairs = [
-                tuple([
-                    float(min(clean_merged_dict[name + '_Min'])),
-                    float(max(clean_merged_dict[name + '_Max']))
-                ]) for name in node_output_names
-            ]
-        else:
-            raise ValueError('Unknown value for calib_mode. \
-                             Currently only naive mode is supported.')
+        pairs = [
+            tuple([
+                float(min(clean_merged_dict[name + '_Min'])),
+                float(max(clean_merged_dict[name + '_Max']))
+            ]) for name in node_output_names
+        ]
 
         final_dict = dict(zip(node_output_names, pairs))
-
         return final_dict
 
-    def dump_minmax(self, calib_mode='naive'):
+    def dump_minmax(self, q_config):
         """Get min/max values of tensors."""
         self.augment_graph()
-        node_output_names, output_dicts = self.get_intermediate_outputs(calib_mode)
-        return self._map_calibration(node_output_names, output_dicts,
-                                     calib_mode=calib_mode)
+        node_output_names, output_dicts = self.get_intermediate_outputs(q_config)
+        return self._map_calibration(node_output_names, output_dicts)
 
-    def dump_calibration(self, q_config, calib_mode='naive', min_max=None):
+    def dump_calibration(self, q_config, min_max=None):
         """Gather calibration params for quantization.
 
         Args:
             q_config (dict): op-wise quantization config
-            calib_mode (str, optional): type 'naive' gives (Min, Max) pairs
-                                        for each intermediate model output across
-                                        test data sets, where the first element is
-                                        a minimum of all values and the second element
-                                        is a maximum of all values. Defaults to 'naive'.
             min_max (dict, optional): min/max values of tensors
         """
-        return self.calculate_quantization_params(q_config, self.dump_minmax(calib_mode)) if min_max is None \
+        return self.calculate_quantization_params(q_config, self.dump_minmax(q_config)) if min_max is None \
             else self.calculate_quantization_params(q_config, min_max)
 
     def calculate_quantization_params(self, q_config, quantization_thresholds):
         """Given quantization thresholds, calculate the quantization params.
 
         Args:
             q_config (dict): op-wise quantization config
@@ -439,22 +477,24 @@
             node_params = self.calculate_scale_zeropoint(parent, child, node_thresholds[0],
                                                          node_thresholds[1], scheme, qType,
                                                          _get_qrange_for_qType(qType, self.reduce_range))
             quantization_params[tensor_name] = node_params
 
         return quantization_params
 
-    def dump_tensor(self, activation=True, weight=False):
+    def dump_tensor(self, activation=True, weight=False, format=None):
         """Dump activation or weight or both from the model."""
+        is_qdq = False
         if "QuantizeLinear" in [node.op_type for node in self.model.graph.node] or \
                 "DynamicQuantizeLinear" in [node.op_type for node in self.model.graph.node]:
             self.augment_nodes = ["DequantizeLinear"]
             self.already_quantized = True
             self.dynamically_quantized = \
                 "DynamicQuantizeLinear" in [node.op_type for node in self.model.graph.node]
+            is_qdq = format == 'qdq'
         self.augment_graph(activation_only=not weight, weight_only=not activation)
         _, output_dicts = self.get_intermediate_outputs()
         iters = len(list(output_dicts.values())[-1])
         map_node_activation = [{} for _ in range(iters)]
         map_node_weight = {}
         self.white_nodes = [node.replace('_quant', '') for node in self.white_nodes]
         augmengted_wrapper = ONNXModel(self.augmented_model)
@@ -463,36 +503,45 @@
         model_output_names = [t.name for t in self.model.graph.output]
         model_input_names = [t.name for t in self.model.graph.input]
         model_initializer_names = [t.name for t in self.model.graph.initializer]
         for tensor_name, tensors in output_dicts.items():
             if tensor_name.replace('_dequantized', '_quantized') in model_initializer_names:
                 nodes = [node for node in map_input[tensor_name] \
                          if node.name.replace('_quant', '') in self.white_nodes]
-            elif tensor_name.replace('_quantized', '') in model_input_names:
-                continue
-            else:
+            elif tensor_name in model_output_names:
                 nodes = [map_output[tensor_name]]
+            else:
+                nodes = map_input[tensor_name]
             for node in nodes:
                 node_name = node.name.replace('_quant', '')
                 if tensor_name in model_output_names and node_name not in self.white_nodes:
                     continue
-                while node_name not in self.white_nodes and self.already_quantized:
-                    node = augmengted_wrapper.get_parents(node, output_name_to_node=map_output)[0]
-                    node_name = node.name.replace('_quant', '')
                 if node_name not in self.white_nodes:
                     continue
                 if node_name not in map_node_weight:
                     map_node_weight[node_name] = {}
-                if tensor_name not in model_initializer_names:
+                if ((is_qdq and tensor_name.replace('_dequantized', '_quantized') not in model_initializer_names) or \
+                    (not is_qdq and tensor_name not in model_initializer_names)) and \
+                    tensor_name in node.input[:2]:
                     for i in range(iters):
-                        map_node_activation[i][node_name] = \
-                            {tensor_name.replace('_quantized', ''): tensors[i]}
-                else:
-                    map_node_weight[node_name].update({tensor_name.replace('_quantized', ''): \
-                                                           tensors[0]})
+                        if node.op_type in ['Attention', 'QAttention'] and tensor_name not in node.input[:2]:
+                            continue
+                        if is_qdq:
+                            map_node_activation[i][node_name] = \
+                                {tensor_name.replace('_dequantized', '').replace('_' + node_name, ''): tensors[i]}
+                        else:
+                            map_node_activation[i][node_name] = \
+                                {tensor_name.replace('_quantized', ''): tensors[i]}
+                elif not (node.op_type in ['QGemm'] and tensor_name not in node.input[:6]) and \
+                    not (node.op_type in ['QLinearConv'] and tensor_name not in node.input[:8]) and \
+                    not (node.op_type in ['Conv', 'Gemm', 'FusedConv'] and tensor_name not in node.input[:2]):
+                    if is_qdq:
+                        map_node_weight[node_name].update({tensor_name.replace('_dequantized', ''): tensors[0]})
+                    else:
+                        map_node_weight[node_name].update({tensor_name.replace('_quantized', ''): tensors[0]})
         dumped_tensors_map = {}
         if weight:
             dumped_tensors_map.update({"weight": map_node_weight})
         if activation:
             dumped_tensors_map.update({"activation": map_node_activation})
         return dumped_tensors_map
 
@@ -504,20 +553,22 @@
         rmin = min(rmin, 0)
         rmax = max(rmax, 0)
         if next_node:
             if next_node.op_type == 'Relu':
                 if rmin < 0:
                     rmin = 0
             elif next_node.op_type == 'Clip' and len(next_node.input) == 3:
-                clip_min = numpy_helper.to_array(self.model_wrapper.get_initializer(next_node.input[1]))
-                clip_max = numpy_helper.to_array(self.model_wrapper.get_initializer(next_node.input[2]))
-                if rmin < clip_min:
-                    rmin = clip_min.tolist() if not isinstance(clip_min.tolist(), list)  else clip_min.tolist()[0]
-                if rmax > clip_max:
-                    rmax = clip_max.tolist() if not isinstance(clip_max.tolist(), list)  else clip_max.tolist()[0]
+                if self.model_wrapper.get_initializer(next_node.input[1]) is not None:
+                    clip_min = numpy_helper.to_array(self.model_wrapper.get_initializer(next_node.input[1]))
+                    if rmin < clip_min:
+                        rmin = clip_min.tolist() if not isinstance(clip_min.tolist(), list)  else clip_min.tolist()[0]
+                if self.model_wrapper.get_initializer(next_node.input[2]) is not None:
+                    clip_max = numpy_helper.to_array(self.model_wrapper.get_initializer(next_node.input[2]))
+                    if rmax > clip_max:
+                        rmax = clip_max.tolist() if not isinstance(clip_max.tolist(), list)  else clip_max.tolist()[0]
 
         if last_node:
             if last_node.op_type in ['Conv', 'FusedConv']:
                 attrs = [attr for attr in last_node.attribute]
                 attrs_names = [attr.name for attr in last_node.attribute]
                 if 'activation' in attrs_names:
                     if attrs[attrs_names.index('activation')].s == b'Relu':
@@ -561,43 +612,43 @@
                         group = attr.i
                         break
             # currently only normal conv and depthwise conv are supported
             if group > 1:  # group conv, need to check depthwise or not
                 weight_name = node.input[1]
                 weight_shape = numpy_helper.to_array(
                     model.graph.initializer[name_to_indices[weight_name]]).shape
-                input_channel = weight_shape.shape[1]
+                input_channel = weight_shape[1]
                 if input_channel != 1:  # TODO need to double check
                     return True
         return False
 
-    def _get_input_tensor_of_ops(self, op_types=['MatMul', 'Linear', 'Conv']):
+    def _get_input_tensor_of_ops(self, op_types=['MatMul', 'Gemm', 'Conv', 'FusedConv']):
         """Traverse the graph and get all the data tensors flowing into layers of {op_types}.
 
         Group conv is excluded.
         TODO the tensors could be set/filtered in configuration.
 
         Args:
             op_types: The op types whose input tensor will be dumped
 
         Returns:
-            A set of tensor names 
+            A dict of dumped tensor: node info
         """
-        tensors_to_dump = set()
+        tensors_to_node = {}
         model = self.model
         initializers = {i.name: i for i in model.graph.initializer}
 
         for node in model.graph.node:
             if len(op_types) == 0 or node.op_type in op_types:
-                if node.op_type == "Conv" and self._check_is_group_conv(node, model):
+                if node.op_type in ["Conv", "FusedConv"] and self._check_is_group_conv(node, model):
                     continue
                 # also need to check whether the layer has weight
                 if len(node.input) >= 2 and node.input[1] in initializers.keys():
-                    tensors_to_dump.add(node.input[0])
-        return tensors_to_dump
+                    tensors_to_node.setdefault(node.input[0], []).append([node.name, node.input, node.output])
+        return tensors_to_node
 
     def _get_max_per_channel(self, datas: list, percentile):
         """Get the max values per input channel.
 
         Args:
             datas: The tensors
             percentile: percentile of calibration to remove outliers
@@ -620,35 +671,47 @@
                 assert False, "not supported"
         permute_datas = np.stack(permute_datas, axis=0)
         permute_datas = permute_datas.reshape(-1, permute_datas.shape[-1])
         max_per_channels = np.percentile(permute_datas, percentile, axis=0)
         max_per_channels = max_per_channels.astype(np.single)
         return max_per_channels
 
-    def calib_smooth(self, percentile, op_types):
+    def calib_smooth(self, percentile, op_types, q_config):
         """Smooth model calibration.
 
         Mainly get the max info per channel of input tensors.
 
         Args:
             percentile:Percentile of calibration to remove outliers
             op_types: The op types whose input tensor will be dumped
 
         Returns:
             max_vals_per_channel: max values per channel of input tensors
             shape_infos: The shape information of input tensors
         """
         # add the input tensors of {op_types} to outputs of the model
-        tensors_to_dump = self._get_input_tensor_of_ops(op_types)
-        self.model_wrapper.add_tensors_to_outputs(tensors_to_dump)
+        tensors_to_node = self._get_input_tensor_of_ops(op_types)
+        self.model_wrapper.add_tensors_to_outputs(tensors_to_node.keys())
         self.augmented_model = self.model_wrapper.model
+        if self.model_wrapper.is_large_model:  # pragma: no cover
+            onnx.save_model(self.augmented_model,
+                            self.model_wrapper.model_path + '_augment.onnx',
+                            save_as_external_data=True,
+                            all_tensors_to_one_file=True,
+                            convert_attribute=False)
+            
         _, output_dicts = self.get_intermediate_outputs()
 
         # remove the input tensors of {op_types} to outputs of the model
-        self.model_wrapper.remove_tensors_from_outputs(tensors_to_dump)
+        self.model_wrapper.remove_tensors_from_outputs(tensors_to_node.keys())
         max_vals_per_channel = {}
         shape_infos = {}
-        for key in tensors_to_dump:
+        for key, val in tensors_to_node.items():
             max_val_per_channel = self._get_max_per_channel(output_dicts[key], percentile=percentile)
             max_vals_per_channel[key] = max_val_per_channel
             shape_infos[key] = output_dicts[key][0].shape
-        return max_vals_per_channel, shape_infos
+            for item in val:
+                shape_infos[item[1][1]] = numpy_helper.to_array(
+                        self.model_wrapper.get_initializer(item[1][1]),
+                        base_dir=os.path.dirname(self.model_wrapper.model_path) if \
+                                self.model_wrapper.model_path is not None else "").shape
+        return max_vals_per_channel, shape_infos, tensors_to_node
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/activation.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/activation.py`

 * *Files 7% similar despite different names*

```diff
@@ -134,8 +134,16 @@
         for attribute in node.attribute: # pragma: no cover
             kwargs.update(attribute_to_kwarg(attribute))
 
         activation_node = onnx.helper.make_node(
             node.op_type.split('QLinear')[-1], inputs,
             outputs, node.name + '_convert', **kwargs)
         add_nodes.append(activation_node)
-        return True, add_nodes, inits
+        return True, add_nodes, inits
+
+@op_registry(op_types="Softmax, BiasGelu, Elu, Exp, FastGelu, Gelu, Softplus, Tanh")
+class Float16ActivationOperator(Operator):
+    """Float16 Activation operator."""
+
+    def __init__(self, onnx_quantizer, onnx_node):
+        """Initialization."""
+        super(Float16ActivationOperator, self).__init__(onnx_quantizer, onnx_node)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/argmax.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/argmax.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/attention.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/attention.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/binary_op.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/binary_op.py`

 * *Files 4% similar despite different names*

```diff
@@ -30,14 +30,16 @@
 
     def quantize_check(self):
         """Check if quantizaion can be done."""
         node = self.node
         data_found, _, _, _, _ = self.quantizer._get_quantization_params(node.output[0])
         if not data_found:
             return False
+        if self.quantizer.backend == 'TensorrtExecutionProvider':
+            return True
         if not all([self.quantizer.is_valid_quantize_weight(i) for i in node.input]):
             return False
         return True
 
     def quantize(self):
         """Do quantizaion."""
         node = self.node
@@ -128,7 +130,15 @@
             kwargs.update(attribute_to_kwarg(attribute))
 
         binary_node = onnx.helper.make_node(
             node.op_type.split('QLinear')[-1], inputs,
             outputs, node.name + '_convert', **kwargs)
         add_nodes.append(binary_node)
         return True, add_nodes, inits
+
+@op_registry(op_types="Sum, Sub, Div, Pow, Equal, Greater, GreaterOrEqual, Less, LessOrEqual")
+class Float16BinaryOperator(Operator):
+    """Float16 Binary operator."""
+
+    def __init__(self, onnx_quantizer, onnx_node):
+        """Initialization."""
+        super(Float16BinaryOperator, self).__init__(onnx_quantizer, onnx_node)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/concat.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/concat.py`

 * *Files 2% similar despite different names*

```diff
@@ -97,17 +97,19 @@
 
             self.quantizer.new_nodes += [qlconcat_node]
             self.quantizer.remove_nodes.append(node)
     
     def cast(self): # pragma: no cover
         """Cast node."""
         node = self.node
-        if node.input[0] not in [i.tensor_name for i in self.quantizer.new_value_info.values()]:
+        cast_tensor = [i.tensor_name for i in self.quantizer.new_value_info.values()]
+        if not all([i in cast_tensor for i in node.input]):
             return
-        self.quantizer.dtype_cast(self.node, self.dtype)
+        self.quantizer.cast_inputs(self.node, self.dtype)
+        self.quantizer.cast_outputs(self.node, self.dtype)
 
 @qop_registry(op_types="QLinearConcat")
 class QConcatOperator(QOperator):
     """QConcat Operator."""
 
     def __init__(self, onnx_node, children, initializers):
         """Initialization."""
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/conv.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/conv.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/direct_q8.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/direct_q8.py`

 * *Files 2% similar despite different names*

```diff
@@ -75,15 +75,16 @@
             node.output[0] = node.output[0] + '_quantized' 
     
     def cast(self): # pragma: no cover
         """Cast node."""
         node = self.node
         if node.input[0] not in [i.tensor_name for i in self.quantizer.new_value_info.values()]:
             return
-        self.quantizer.dtype_cast(self.node, self.dtype)
+        self.quantizer.cast_inputs(self.node, self.dtype, [0])
+        self.quantizer.cast_outputs(self.node, self.dtype)
 
 @qop_registry(op_types="Reshape, Transpose, Squeeze, Unsqueeze")
 class QDirectOperator(QOperator):
     """QDirect Operator."""
 
     def __init__(self, onnx_node, children, initializers):
         """Initialization."""
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/embed_layernorm.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/embed_layernorm.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/gather.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/gather.py`

 * *Files 20% similar despite different names*

```diff
@@ -60,14 +60,15 @@
         """Convert to QOperator format."""
         node = self.node
         
         parents = self.quantizer.model.get_parents(node)
         children = self.quantizer.model.get_children(node)
 
         if any([i.op_type == 'DequantizeLinear' for i in parents]):
+            from onnx import numpy_helper
             inputs = []
             inputs.append(parents[0].input[0])
             inputs.append(node.input[1])
 
             gather_new_output = node.output[0] + "_quantized"
 
             kwargs = {}
@@ -76,30 +77,45 @@
 
             gather_node = onnx.helper.make_node("Gather",
                                                 inputs,
                                                 [gather_new_output],
                                                 node.name,
                                                 **kwargs)
             self.quantizer.new_nodes.append(gather_node)
-            if any([i.op_type  != 'QuantizeLinear' for i in children]): # pragma: no cover
+            if any([i.op_type != 'QuantizeLinear' for i in children]): # pragma: no cover
                 dq_inputs = []
                 dq_inputs.append(gather_new_output)
                 dq_inputs.extend(parents[0].input[1:])
                 dq_node = onnx.helper.make_node("DequantizeLinear",
                                                 dq_inputs,
                                                 [node.output[0]],
                                                 node.name + '_DequantizeLinear')
                 self.quantizer.new_nodes.append(dq_node)
                 
+            out_scale = 1.
+            out_zp = 0
             for child in children:
                 if child.op_type == 'QuantizeLinear':
+                    out_scale = numpy_helper.to_array(self.quantizer.model.get_initializer(child.input[1]))
+                    out_zp = numpy_helper.to_array(self.quantizer.model.get_initializer(child.input[2]))
                     self.quantizer.remove_nodes.append(child)
                     for n in self.quantizer.model.get_children(child):
                         self.quantizer.model.replace_node_input(n, 
                                         child.output[0], gather_new_output)
+            
+            # int8 weight will be recalculated for the first time
+            if any([child.op_type == 'QuantizeLinear' for child in children]) and \
+                    self.quantizer.model.get_initializer(parents[0].input[0]) is not None and \
+                        parents[0].input[0] not in self.quantizer.recalculate_quantized_value:
+                int8_tensor = numpy_helper.to_array(self.quantizer.model.get_initializer(parents[0].input[0]))
+                in_scale = numpy_helper.to_array(self.quantizer.model.get_initializer(parents[0].input[1]))
+                in_zp = numpy_helper.to_array(self.quantizer.model.get_initializer(parents[0].input[2]))
+                new_int8_tensor = (((int8_tensor.astype('float32') - in_zp) * in_scale) / out_scale).round() + out_zp
+                self.quantizer.model.set_initializer(parents[0].input[0], new_int8_tensor.astype(int8_tensor.dtype))
+                self.quantizer.recalculate_quantized_value.append(parents[0].input[0])
             self.quantizer.remove_nodes.extend([node, parents[0]])
             
 @qop_registry(op_types="Gather")
 class QGatherOperator(QOperator):
     """QGather Operator."""
 
     def __init__(self, onnx_node, children, initializers):
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/gavgpool.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/gavgpool.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/gemm.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/gemm.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/lstm.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/lstm.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/matmul.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/matmul.py`

 * *Files 7% similar despite different names*

```diff
@@ -25,33 +25,45 @@
 class MatMulOperator(Operator):
     """MatMul Operator."""
 
     def __init__(self, onnx_quantizer, onnx_node):
         """Initialization."""
         super(MatMulOperator, self).__init__(onnx_quantizer, onnx_node)
 
+    def quantize_check(self):
+        """Check if quantizaion can be done."""
+        node = self.node
+        if not all([self.quantizer.model.get_initializer(i) is None for i in node.input]):
+            return True
+        elif all([i not in self.quantizer.quantized_value_map for i in node.input]):
+            return False
+        else:
+            return True
+
     def quantize(self):
         """Do quantizaion."""
         node = self.node
         self.quantizer.quantize_inputs(node, [0])
         if self.per_channel and find_by_name(node.input[1], self.quantizer.model.initializer()):
             self.quantizer.quantize_weights_per_channel(node, [1],
                                     self.weight_dtype, self.weight_scheme, 1)
         else:
             self.quantizer.quantize_inputs(node, [1])
 
-        if not self.disable_qdq_for_node_output or self.quantizer.mode != 'qdq':
+        if not self.disable_qdq_for_node_output:
             self.quantizer.quantize_outputs(node)
         node.name = node.name + "_quant"
 
     def convert_check(self, convert_format):
         """Check if conversion can be done."""
         node = self.node
         assert convert_format in ['dynamic', 'static'], \
             "convert format for {} should be in ['dynamic', 'static']".format(node.op_type)
+        if not node.name.endswith('_quant'):
+            return False
         return True
 
     def convert(self, convert_format):
         """Convert to QOperator format."""
         node = self.node
 
         if convert_format == 'dynamic':
@@ -108,30 +120,37 @@
                 self.quantizer.remove_nodes.append(parents[1])
             self.quantizer.remove_nodes.append(node)
         elif convert_format == 'static':
             parents = self.quantizer.model.get_parents(node)
             if len(self.quantizer.model.get_children(node)) == 0 or \
                 not node.name.endswith('_quant'): # pragma: no cover
                 return
-            child = self.quantizer.model.get_children(node)[0]
-
-            qlinear_matmul_output = child.output[0]
 
             qlinear_matmul_inputs = []
-            for parent in parents:
-                qlinear_matmul_inputs.extend(parent.input)
-            qlinear_matmul_inputs.extend(child.input[1:])
-
-            qlinear_matmul_node = onnx.helper.make_node("QLinearMatMul", 
-                                                        qlinear_matmul_inputs, 
-                                                        [qlinear_matmul_output],
-                                                        node.name)
+            if self.disable_qdq_for_node_output:
+                for i in range(len(parents[0].input)):
+                    qlinear_matmul_inputs.extend([parent.input[i] for parent in parents])
+                qlinear_matmul_node = onnx.helper.make_node("MatMulIntegerToFloat",
+                                                            qlinear_matmul_inputs,
+                                                            node.output,
+                                                            node.name,
+                                                            domain='com.microsoft')
+            else:
+                child = self.quantizer.model.get_children(node)[0]
+                qlinear_matmul_output = child.output[0]
+                for parent in parents:
+                    qlinear_matmul_inputs.extend(parent.input)
+                qlinear_matmul_inputs.extend(child.input[1:])
+                qlinear_matmul_node = onnx.helper.make_node("QLinearMatMul",
+                                                            qlinear_matmul_inputs,
+                                                            [qlinear_matmul_output],
+                                                            node.name)
+                self.quantizer.remove_nodes.append(child)
             self.quantizer.new_nodes.append(qlinear_matmul_node)
             self.quantizer.remove_nodes.extend(parents)
-            self.quantizer.remove_nodes.append(child)
             self.quantizer.remove_nodes.append(node)
             
 @qop_registry(op_types="QLinearMatMul")
 class QMatMulOperator(QOperator):
     """QLinearMatMul Operator."""
 
     def __init__(self, onnx_node, children, initializers):
@@ -172,7 +191,15 @@
             kwargs.update(attribute_to_kwarg(attribute))
 
         matmul_node = onnx.helper.make_node(
             'MatMul', inputs,
             outputs, node.name + '_convert', **kwargs)
         add_nodes.append(matmul_node)
         return True, add_nodes, inits
+
+@op_registry(op_types="FusedMatMul")
+class FusedMatMulOperator(Operator):
+    """FusedMatMul Operator."""
+
+    def __init__(self, onnx_quantizer, onnx_node):
+        """Initialization."""
+        super(FusedMatMulOperator, self).__init__(onnx_quantizer, onnx_node)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/maxpool.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/maxpool.py`

 * *Files 3% similar despite different names*

```diff
@@ -62,15 +62,15 @@
         node = self.node
         parent = self.quantizer.model.get_parents(node)[0]
         children = self.quantizer.model.get_children(node)
         if parent.op_type != 'DequantizeLinear' or \
             all([i.op_type != 'QuantizeLinear' for i in children]): # pragma: no cover
             return
         node.input[0] = parent.input[0]
-        node.output[0] = node.output[0] + '_quantized'
+        node.output[0] = node.output[0].replace('_QuantizeInput', '_quantized')
         for child in children:
             if child.op_type == 'QuantizeLinear':
                 self.quantizer.remove_nodes.append(child)
                 for n in self.quantizer.model.get_children(child):
                     self.quantizer.model.replace_node_input(n,
                                     child.output[0], node.output[0])
 
@@ -78,8 +78,8 @@
 
 @qop_registry(op_types="MaxPool")
 class QMaxPoolOperator(QOperator):
     """QMaxPool Operator."""
 
     def __init__(self, onnx_node, children, initializers):
         """Initialization."""
-        super().__init__(onnx_node, children, initializers)
+        super().__init__(onnx_node, children, initializers)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/ops.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/ops.py`

 * *Files 2% similar despite different names*

```diff
@@ -103,15 +103,16 @@
 
     def convert(self, convert_format):
         """Convert to QOperator format."""
         return
 
     def cast(self): # pragma: no cover
         """Cast node."""
-        self.quantizer.dtype_cast(self.node, self.dtype)
+        self.quantizer.cast_inputs(self.node, self.dtype)
+        self.quantizer.cast_outputs(self.node, self.dtype)
 
 class QOperator(object):
     """Base QOperator."""
 
     def __init__(self, onnx_node, children, initializers):
         """Initialization."""
         self.node = onnx_node
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/pad.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/pad.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/pooling.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/pooling.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/resize.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/resize.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/operators/split.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/operators/split.py`

 * *Files 1% similar despite different names*

```diff
@@ -83,15 +83,16 @@
         self.quantizer.remove_nodes.extend([parent, node])
 
     def cast(self): # pragma: no cover
         """Cast node."""
         node = self.node
         if node.input[0] not in [i.tensor_name for i in self.quantizer.new_value_info.values()]:
             return
-        self.quantizer.dtype_cast(self.node, self.dtype)
+        self.quantizer.cast_inputs(self.node, self.dtype)
+        self.quantizer.cast_outputs(self.node, self.dtype)
 
 @qop_registry(op_types="Split")
 class QSplitOperator(QOperator):
     """QSplit Operator."""
 
     def __init__(self, onnx_node, children, initializers):
         """Initialization."""
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/quantizer.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/quantizer.py`

 * *Files 2% similar despite different names*

```diff
@@ -45,34 +45,36 @@
 
 class Quantizer:
     """Quantizer class."""
 
     def __init__(self, model, q_config, mode, static, quantization_params,
                  op_types_to_quantize, fallback_list=['fp32'], reduce_range=None,
                  add_qdq_pair_to_weight=False, optypes_to_exclude_output_quant=[],
-                 dedicated_qdq_pair=False):
+                 dedicated_qdq_pair=False, backend='CPUExecutionProvider'):
         """Initialization.
 
         Args:
             model (ModelProto or ONNXModel): onnx model or onnx model wrapper by neural compressor
             q_config (dict): op-wise quantization config.
             mode (QuantizationMode): quantizaion mode
             static (bool): static or not
             quantization_params (dict): scale and zero point of tensors
             op_types_to_quantize (list): optypes to quantize
             fallback_list (list, optional): fallback data type. Defaults to ['fp32'].
             reduce_range (bool, optional): use 7 bit or not. Defaults to None.
             add_qdq_pair_to_weight (bool, optional): add QDQ pair to weight or not. Defaults to False.
             optypes_to_exclude_output_quant (list, optional): optypes to exclude output quantization. Defaults to [].
             dedicated_qdq_pair (bool, optional): dedicate QDQ pair or not. Defaults to False.
+            backend (str, optional): backend of onnxrt adaptor. Defaults to CPUExecutionProvider
         """
         self.model = ONNXModel(model) if not isinstance(model, ONNXModel) else model
         model = onnx.shape_inference.infer_shapes(self.model.model) if \
             not self.model.is_large_model else self.model.model
         self.config = q_config
+        self.backend = backend
         self.reduce_range = reduce_range
         self.mode = mode # QuantizationMode.Value
         self.static = static  # use static quantization for inputs.
         self.fuse_dynamic_quant = False
         self.quantization_params = quantization_params
         self.op_types_to_quantize = op_types_to_quantize
         self.fallback_list = fallback_list
@@ -84,14 +86,17 @@
         self.value_infos.update({it.name: it for it in model.graph.input})
         self.replace_input = []
         self.remove_nodes = []
         # List of quantized weights
         self.quantized_value_map = {}
         self.new_value_info = {}
 
+        # List of recalculated quantize weight for Gather op.
+        self.recalculate_quantized_value = []
+
         # QuantizeRange tensor name and zero tensor name for scale and zero point calculation.
         # Used when static is False
         self.fixed_qrange_uint8_name = "fixed_quantization_range_uint8"
         self.fixed_qrange_int8_name = "fixed_quantization_range_int8"
         # For uint8 data-type, to compute zero point, we subtract rmin from 0 (represented by fixed_zero_name tensor)
         self.fixed_zero_name = "fixed_zero"
         # For int8 data-type, zero point is always zero (respresented by fixed_zero_point_name tensor)
@@ -366,17 +371,19 @@
                     visited_op = []
                     dfs(visited_op, n, match_pair)
         self.model.remove_nodes(self.remove_nodes)
         for node, old_input_name, new_input_name in self.replace_input:
             self.model.replace_node_input(node, old_input_name, new_input_name)
         self.model.update()
 
-    def dtype_cast(self, node, cfg, keep_io_types=True): # pragma: no cover
-        """Cast node dtype."""
+    def cast_inputs(self, node, cfg, indices=None):
+        """Cast node input dtype."""
         for idx, tensor_name in enumerate(node.input):
+            if indices and idx not in indices:
+                continue
             initializer = find_by_name(tensor_name, self.model.initializer())
             if initializer is not None:
                 if initializer.data_type != onnx_proto.TensorProto.FLOAT: 
                     continue
                 do_cast = cast_tensor(initializer, cfg)
                 if do_cast:
                     self.new_value_info[tensor_name] = ValueInfo(tensor_name,
@@ -388,18 +395,20 @@
                     continue 
                 name = node.name + '_input_cast' + str(idx)
                 self.new_nodes.append(onnx.helper.make_node(
                         'Cast', [tensor_name], [name], to=dtype_mapping[cfg], name=name))
                 node.input[idx] = name
                 self.new_value_info[name] = ValueInfo(tensor_name,
                                                              TensorProto.FLOAT, dtype_mapping[cfg])
-        if all([i not in self.new_value_info for i in node.input]):
-            return
 
+    def cast_outputs(self, node, cfg, indices=None):
+        """Cast node output dtype."""
         for idx, tensor_name in enumerate(node.output):
+            if indices and idx not in indices:
+                continue
             if tensor_name in self.value_infos and \
                 self.value_infos[tensor_name].type.HasField('tensor_type') and \
                 self.value_infos[tensor_name].type.tensor_type.elem_type != TensorProto.FLOAT:
                 continue 
             node.output[idx] = tensor_name + "_to_cast_" + str(idx)
             name = node.name + '_output_cast' + str(idx)
             self.new_nodes.append(onnx.helper.make_node(
@@ -641,30 +650,33 @@
     def quantize_bias(self, bias_name, input_name, weight_name, beta=1.0):
         """Quantized the bias.
         
         Zero Point == 0 and Scale == Input_Scale * Weight_Scale
         """
         # get scale for weight
         weight_scale_initializer = find_by_name(weight_name + '_scale', self.model.initializer())
-        weight_scale = self.tensor_proto_to_array(weight_scale_initializer)
+        weight_scale = self.tensor_proto_to_array(weight_scale_initializer, os.path.dirname(self.model.model_path)) if \
+            self.model.model_path is not None else self.tensor_proto_to_array(weight_scale_initializer)
 
         # get bias
         bias_initializer = find_by_name(bias_name, self.model.initializer())
-        bias_data = self.tensor_proto_to_array(bias_initializer)
+        bias_data = self.tensor_proto_to_array(bias_initializer, os.path.dirname(self.model.model_path)) if \
+            self.model.model_path is not None else self.tensor_proto_to_array(bias_initializer)
         quantized_bias_name = bias_name + "_quantized"
 
         if input_name in self.quantized_value_map:
             input_scale_name = self.quantized_value_map[input_name].scale_name
         elif input_name in self.quantization_params:
             _, input_scale_name, _, _, _ = self._get_quantization_params(input_name)
         else:
             raise ValueError("Expected {} to be in quantized value map \
                               for static quantization".format(input_name))
         inputscale_initializer = find_by_name(input_scale_name, self.model.initializer())
-        input_scale = self.tensor_proto_to_array(inputscale_initializer)
+        input_scale = self.tensor_proto_to_array(inputscale_initializer, os.path.dirname(self.model.model_path)) if \
+            self.model.model_path is not None else self.tensor_proto_to_array(inputscale_initializer)
 
         # calcuate scale for bias
 
         bias_scale = input_scale * weight_scale * beta
 
         # quantize bias
         quantized_data = (np.asarray(bias_data) / bias_scale).round().astype(np.int32)
@@ -778,15 +790,16 @@
     def quantize_weight_per_channel(self, weight_name, weight_qType, scheme, channel_axis):
         """Quantize weight per-channel."""
         initializer = find_by_name(weight_name, self.model.initializer())
         if initializer is None:
             raise ValueError("{} is not an initializer", weight_name)
 
         if initializer.name not in self.quantized_value_map:
-            weights = self.tensor_proto_to_array(initializer)
+            weights = self.tensor_proto_to_array(initializer, os.path.dirname(self.model.model_path)) if \
+                self.model.model_path is not None else self.tensor_proto_to_array(initializer)
             rmin, rmax, zero_point, scale, quantized_weights = quantize_data_per_channel(
                 weights, channel_axis, _get_qrange_for_qType(weight_qType,self.reduce_range), weight_qType, scheme)
 
             weight = QuantizedInitializer(initializer.name, initializer, rmin, rmax,
                                           zero_point, scale,
                                           weights,
                                           quantized_weights.flatten().tolist(), 
@@ -837,18 +850,18 @@
                                                     zero_scale_shape, weight.scales)
         zero_initializer = onnx.helper.make_tensor(zero_point_name, zero_point_type, 
                                                     zero_scale_shape, weight.zero_points)
 
         self.model.initializer().extend([scale_initializer, zero_initializer])
 
     @staticmethod
-    def tensor_proto_to_array(initializer):
+    def tensor_proto_to_array(initializer, base_dir=""):
         """Convert TensorProto to array."""
         if initializer.data_type == onnx_proto.TensorProto.FLOAT:
-            weights = onnx.numpy_helper.to_array(initializer)
+            weights = onnx.numpy_helper.to_array(initializer, base_dir)
         else:
             raise ValueError('Only float type quantization is supported. \
                 Weights {} is {}.'.format(initializer.name, 
                     dtype_to_name(dtype_mapping, initializer.data_type)))
         return weights
 
     def _get_quantization_params(self, param_name):
@@ -887,15 +900,16 @@
 
         return True, scale_name, zero_point_name, scale_shape, zero_point_shape
 
     def _get_quantized_weight(self, initializer, qType, scheme):
         """Get quantized weight."""
         if initializer.name in self.quantized_value_map:
             return self.quantized_value_map[initializer.name]
-        weights_data = self.tensor_proto_to_array(initializer)
+        weights_data = self.tensor_proto_to_array(initializer, os.path.dirname(self.model.model_path)) if \
+            self.model.model_path is not None else self.tensor_proto_to_array(initializer)
         rmin, rmax, zero_point, scale, quantized_weights_data = quantize_data(
             weights_data.flatten().tolist(), _get_qrange_for_qType(qType, \
             self.reduce_range), qType, scheme)
         weight = QuantizedInitializer(initializer.name,
                                       initializer, [rmin], [rmax], [zero_point], [scale],
                                       weights_data,
                                       quantized_weights_data,
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/ox_utils/util.py` & `neural_compressor-2.2/neural_compressor/adaptor/ox_utils/util.py`

 * *Files 26% similar despite different names*

```diff
@@ -503,247 +503,14 @@
         if item.name == name:
             items.append(item)
     if len(items) > 0:
         return items[0]
     else:
         return None
 
-def get_smooth_scales_per_op(max_vals_per_channel, input_tensors_2_weights,
-                              input_tensors_2_weights_nodes, alpha):
-    """Get the smooth scales for weights.
-
-    The ops with the same input will share one mul layer.
-    TODO support individual scales for each layer.
-
-    Args:
-        max_vals_per_channel: Max values per channel after calibration
-        input_tensors_2_weights: A dict saved input tensor name and its corresponding weights
-        input_tensors_2_weights_nodes:A dict saved input tensor name and its corresponding weight nodes
-        alpha: smooth alpha in paper
-
-    Returns:
-        the smooth scales for weights, currently one input tensor only have one scale
-    """
-    scales = {}
-    for key in input_tensors_2_weights_nodes.keys():
-        nodes = input_tensors_2_weights_nodes[key]
-        for index, node in enumerate(nodes):
-            name = node.name
-            weight = input_tensors_2_weights[key][index]
-            if len(weight.shape) == 4:  # conv
-                if weight.shape[1] == 1:  # depthwise conv
-                    pass
-                else:
-                    weight = np.moveaxis(weight, 0, 1)
-            weight = weight.reshape(weight.shape[0], -1)
-            weight_max_per_channel = np.amax(weight, axis=-1)
-            input_power = np.power(max_vals_per_channel[key], alpha)
-            weight_power = np.power(weight_max_per_channel, 1 - alpha)
-            scale = np.clip(input_power / weight_power, a_min=1e-5, a_max=None)
-            scales[name] = scale
-    return scales
-
-def get_smooth_scales_per_input(max_vals_per_channel, input_tensors_2_weights, alpha):
-    """Get the smooth scales for weights.
-
-    The ops with the same input will share one mul layer.
-    TODO support individual scales for each layer.
-
-    Args:
-        max_vals_per_channel: Max values per channel after calibration
-        input_tensors_2_weights: A dict saved input tensor name and its corresponding weights
-        alpha: smooth alpha in paper
-
-    Returns:
-        the smooth scales for weights, currently one input tensor only have one scale
-    """
-    scales = {}
-    for key in input_tensors_2_weights.keys():
-        weights = input_tensors_2_weights[key]
-        weights_in_channel_max = []
-        for weight in weights:  # mamul ic*oc, conv oc*ic*k*k
-            if len(weight.shape) == 4:  # conv
-                if weight.shape[1] == 1:  # depthwise conv
-                    pass
-                else:
-                    weight = np.moveaxis(weight, 0, 1)
-            weight = weight.reshape(weight.shape[0], -1)
-            cur_max = np.amax(weight, axis=-1)
-            weights_in_channel_max.append(cur_max)
-        weigths_stack = np.stack(weights_in_channel_max, axis=-1)
-        weigths_stack = np.abs(weigths_stack.reshape(weigths_stack.shape[0], -1))
-        weights_max = np.amax(weigths_stack, axis=-1)
-        input_power = np.power(max_vals_per_channel[key], alpha)
-        weight_power = np.power(weights_max, 1 - alpha)
-        scale = np.clip(input_power / weight_power, a_min=1e-5, a_max=None)
-        scales[key] = scale
-    return scales
-
-def insert_smooth_mul_op_per_input(scales, shape_infos, input_tensors_2_weights_nodes):
-    """Insert the mul layer after inupt.
-
-    The ops with the same input will share one mul layer.
-
-    Args:
-        scales: The smooth scales
-        shape_infos: the input tensor shape information
-        input_tensors_2_weights_nodes:  A dict
-
-    Returns:
-        new_added_mul_nodes: added Mul layers
-        new_init_tensors: added scales tensor
-    """
-    new_added_mul_nodes = []
-    new_init_tensors = []  # scales_tensor
-    for key in scales.keys():
-        scale_factor = 1.0 / scales[key]
-        shape_info = shape_infos[key]
-        if len(shape_info) == 3 or len(shape_info) == 2:  # the last dim is input channel
-            pass
-        elif len(shape_info) == 4:
-            scale_factor = np.reshape(scale_factor, (1, -1, 1, 1))
-        else:
-            assert False, "not support"
-        name = key + "_" + "smooth_scale"
-        scale_tensor = helper.make_tensor(
-            name=name,
-            data_type=onnx_proto.TensorProto.FLOAT,
-            dims=scale_factor.shape,
-            vals=scale_factor.flatten().tolist())
-        new_init_tensors.append(scale_tensor)
-        mul_output_name = key + "_smooth_output"
-        mul_node = helper.make_node(
-            "Mul",
-            inputs=[key, key + "_" + "smooth_scale"],
-            outputs=[mul_output_name],
-            name=key + "_smooth_mul"
-        )
-        new_added_mul_nodes.append(mul_node)
-        for node in input_tensors_2_weights_nodes[key]:
-            for index, input in enumerate(node.input):
-                if input == key:
-                    node.input[index] = mul_output_name
-    return new_added_mul_nodes, new_init_tensors
-
-def adjust_weights_per_op(model, nodes, scales):
-    """Adjust the weights per input scale.
-
-    Each op has one individual Mul layer.
-
-    Args:
-        model: The onnx model
-        nodes: The nodes whose weights needs to be adjustd
-        scales: The input scales
-    """
-    name_to_indices = {}
-    for index, i in enumerate(model.model.graph.initializer):
-        name_to_indices[i.name] = index
-    for key in nodes.keys():
-        node = nodes[key]
-        input = node.input[1]
-        if input in name_to_indices.keys():
-            weight = numpy_helper.to_array(model.model.graph.initializer[name_to_indices[input]])
-            if len(weight.shape) == 2:
-                scale = np.expand_dims(scales[key],
-                                       axis=-1)  # TODO, to support conv
-                new_weight = weight * scale
-            elif len(weight.shape) == 4:  # TODO need to check conv
-                scale = np.reshape(scales[key], (1, -1, 1, 1))
-                new_weight = weight * scale
-            else:
-                assert False, "not support"
-            new_tensor = numpy_helper.from_array(new_weight, input)
-            model.model.graph.initializer[name_to_indices[input]].CopyFrom(new_tensor)
-
-def adjust_weights_per_input(model, nodes, scales):
-    """Adjust the weights per input scale.
-
-    The ops with the same input will share one mul layer
-
-    Args:
-        model: The onnx model
-        nodes: The nodes whose weights needs to be adjustd
-        scales: The input scales
-    """
-    name_to_indices = {}
-    for index, i in enumerate(model.model.graph.initializer):
-        name_to_indices[i.name] = index
-    for key in nodes.keys():
-        curr_nodes = nodes[key]
-        for node in curr_nodes:
-            input = node.input[1]  # TODO
-            if input in name_to_indices.keys():
-                weight = numpy_helper.to_array(model.model.graph.initializer[name_to_indices[input]])
-                if len(weight.shape) == 2:
-                    scale = np.expand_dims(scales[key],
-                                           axis=-1)  # TODO, to support conv
-                    new_weight = weight * scale
-                elif len(weight.shape) == 4:  # TODO need to check conv
-                    scale = np.reshape(scales[key], (1, -1, 1, 1))
-                    new_weight = weight * scale
-                else:
-                    assert False, "not support"
-                new_tensor = numpy_helper.from_array(new_weight, input)
-                model.model.graph.initializer[name_to_indices[input]].CopyFrom(new_tensor)
-
-def insert_smooth_mul_op_per_op(scales, shape_infos, input_tensors_2_weights_nodes):
-    """Insert the mul layer before op.
-
-    Each op has one individual Mul layer.
-
-    Args:
-        scales: The smooth scales
-        shape_infos: the input tensor shape information
-        input_tensors_2_weights_nodes:  A dict
-
-    Returns:
-        new_added_mul_nodes: added Mul layers
-        new_init_tensors: added scales tensor
-        name_2_nodes: a dict, key is the node name, value is the node
-    """
-    name_2_nodes = {}
-    for key in input_tensors_2_weights_nodes.keys():
-        nodes = input_tensors_2_weights_nodes[key]
-        for node in nodes:
-            name_2_nodes[node.name] = node
-    new_added_mul_nodes = []
-    new_init_tensors = []  # scales_tensor
-    for input_key in input_tensors_2_weights_nodes.keys():
-        shape_info = shape_infos[input_key]
-        nodes = input_tensors_2_weights_nodes[input_key]
-        for node in nodes:
-            key = node.name
-            scale_factor = 1.0 / scales[key]
-            if len(shape_info) == 3 or len(shape_info) == 2:  # the last dim is input channel
-                pass
-            elif len(shape_info) == 4:
-                scale_factor = np.reshape(scale_factor, (1, -1, 1, 1))
-            else:
-                assert False, "not support"
-            name = key + "_" + "smooth_scale"
-            scale_tensor = helper.make_tensor(
-                name=name,
-                data_type=onnx_proto.TensorProto.FLOAT,
-                dims=scale_factor.shape,
-                vals=scale_factor.flatten().tolist())
-            new_init_tensors.append(scale_tensor)
-            mul_output_name = key + "_smooth_output"
-            mul_node = helper.make_node(
-                "Mul",
-                inputs=[input_key, name],
-                outputs=[mul_output_name],
-                name=key + "_smooth_mul"
-            )
-            new_added_mul_nodes.append(mul_node)
-            node = name_2_nodes[key]
-            for index, input in enumerate(node.input):
-                if input == input_key:
-                    node.input[index] = mul_output_name
-    return new_added_mul_nodes, new_init_tensors, name_2_nodes
-
 def trt_env_setup(model):
     """Set environment variable for Tensorrt Execution Provider."""
     is_int8 = False
     for node in model.graph.node:
         if node.op_type in ["QuantizeLinear", "DequantizeLinear"]:
             is_int8 = True
             break
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/pytorch.py` & `neural_compressor-2.2/neural_compressor/adaptor/pytorch.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,26 +15,26 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import copy
 import gc
 import math
 import os
+import re
 from collections import OrderedDict, UserDict, namedtuple
 from packaging.version import Version
 import yaml
 from functools import partial
 from neural_compressor.utils.utility import dump_elapsed_time
 from .adaptor import adaptor_registry, Adaptor
 from ..utils.utility import LazyImport, CpuInfo, GLOBAL_STATE, MODE
 from ..utils.utility import Statistics
 from ..utils import logger
 from .query import QueryBackendCapability
 from ..data.dataloaders.base_dataloader import BaseDataLoader
-from .torch_utils.smooth_quant import TorchSmoothQuant
 torch = LazyImport("torch")
 json = LazyImport("json")
 hvd = LazyImport("horovod.torch")
 torch_utils = LazyImport("neural_compressor.adaptor.torch_utils")
 ipex = LazyImport("intel_extension_for_pytorch")
 
 REDUCE_RANGE = False if CpuInfo().vnni else True
@@ -72,18 +72,18 @@
 
 
 def pytorch_forward_wrapper(model, input, device='cpu', conf=None, running_mode='inference'):
     version = get_torch_version()
     if isinstance(input, dict) or isinstance(input, UserDict):
         if device == 'cpu':
             output = model(**input)
-        elif device == 'ipex':  # pragma: no cover
+        elif device == 'ipex':
             # have to split the case to avoid exposing ipex.DEVICE outside
             # which require intel extension installed
-            if version.release < Version("1.12.0").release:
+            if version.release < Version("1.12.0").release:  # pragma: no cover
                 if running_mode == "calibration":
                     with ipex.quantization.calibrate(conf, default_recipe=True):   # pylint: disable=E1101
                         output = model(**input)
                 else:
                     output = model(**input)
             else:
                 output = model(**input)
@@ -91,16 +91,16 @@
             for inp in input.keys():
                 input[inp] = input[inp].to("dpcpp" if device=="gpu" else device) \
                     if isinstance(input[inp], torch.Tensor) else input[inp]
             output = model(**input)
     elif isinstance(input, list) or isinstance(input, tuple):
         if device == 'cpu':
             output = model(*input)
-        elif device == 'ipex':  # pragma: no cover
-            if version.release < Version("1.12.0").release:
+        elif device == 'ipex':
+            if version.release < Version("1.12.0").release:  # pragma: no cover
                 if running_mode == "calibration":
                     with ipex.quantization.calibrate(conf, default_recipe=True):   # pylint: disable=E1101
                         output = model(*input)
                 else:
                     output = model(*input)
             else:
                 output = model(*input)
@@ -109,63 +109,66 @@
             input = [inp.to(tmp_device) \
                     if isinstance(inp, torch.Tensor) else inp
                     for inp in input] # pylint: disable=E1133
             output = model(*input)
     else:
         if device == 'cpu' or not isinstance(input, torch.Tensor):
             output = model(input)
-        elif device == 'ipex':  # pragma: no cover
-            if version.release < Version("1.12.0").release:
+        elif device == 'ipex':
+            if version.release < Version("1.12.0").release:  # pragma: no cover
                 if running_mode == "calibration":
                     with ipex.quantization.calibrate(conf, default_recipe=True):    # pylint: disable=E1101
                         output = model(input)
                 else:
                     output = model(input)
             else:
                 output = model(input)
         else:  # pragma: no cover
             input = input.to("dpcpp" if device == "gpu" else device)  # pylint: disable=no-member
             output = model(input)
     return output
 
 
-def get_example_inputs(model, dataloader):  # pragma: no cover
+def get_example_inputs(model, dataloader):
     version = get_torch_version()
     # Suggest set dataloader like calib_dataloader
     if dataloader is None:
         return None
     try:
         for idx, (input, label) in enumerate(dataloader):
             output = pytorch_forward_wrapper(model,
                                              input)
-            if isinstance(input, dict) or isinstance(input, UserDict):
+            if isinstance(input, (dict, UserDict)): # pragma: no cover
                 assert version.release >= Version("1.12.0").release, \
                 "INC support IPEX version >= 1.12.0"
                 if "label" in input.keys():
                     input.pop("label")
-                named_input = namedtuple("input", input.keys())
-                input = named_input._make(input.values())
-                return input
-            if isinstance(input, list) or isinstance(input, tuple):
+                if version.release <= Version("2.0.1").release:
+                    return tuple(input.values())
+                else:
+                    return dict(input)
+
+            if isinstance(input, (list, tuple)):
                 return tuple(input)
             if isinstance(input, torch.Tensor):
                 return input
             break
-    except Exception as e:
+    except Exception as e: # pragma: no cover
         for idx, input in enumerate(dataloader):
             output = pytorch_forward_wrapper(model,
                                      input)
-            if isinstance(input, dict) or isinstance(input, UserDict):
+            if isinstance(input, (dict, UserDict)): # pragma: no cover
                 assert version.release >= Version("1.12.0").release, \
                 "INC support IPEX version >= 1.12.0"
                 if "label" in input.keys():
                     input.pop("label")
-                named_input = namedtuple("input", input.keys())
-                input = named_input._make(input.values())
-                return input
+                if version.release <= Version("2.0.1").release:
+                    return tuple(input.values())
+                else:
+                    return dict(input)
             if isinstance(input, list) or isinstance(input, tuple):
                 return tuple(input)
             if isinstance(input, torch.Tensor):
                 return input
             break
     if idx == 0:
         assert False, "Please checkout the example_inputs format."
@@ -455,15 +458,15 @@
             logger.info((f"Currently, PyTorch does not natively support {dtype},"+ \
                 f"it will simulate its numerics instead."))
             unsigned, num_bits = _get_signed_and_bits(dtype)
             torch_dtype = torch.quint8 if unsigned else torch.qint8
             quant_min, quant_max = calculate_quant_min_max(unsigned, num_bits)
             logger.info((f"For {dtype}, replace it with {torch_dtype} and " + \
                 f"set quant_min: {quant_min}, quant_max: {quant_max}"))
-        else:
+        else: # pragma: no cover
             assert False, "Unsupport dtype with {}".format(dtype)
 
     if algorithm == 'placeholder' or torch_dtype == torch.float:  # pragma: no cover
         return torch.quantization.PlaceholderObserver \
             if get_torch_version().release < Version("1.8.0").release \
                 else torch.quantization.PlaceholderObserver.with_args(dtype=torch_dtype,
                                                                       compute_dtype=compute_dtype)
@@ -515,15 +518,15 @@
         dtype (sting): Quantized data type
 
     Return:
         fake quantization (object)
     """
     version = get_torch_version()
     if scheme == 'asym_float' \
-                 and version.release >= Version("1.7.0").release:
+                 and version.release >= Version("1.7.0").release: # pragma: no cover
         return torch.quantization.default_float_qparams_observer
     if algorithm == 'placeholder' or dtype == 'fp32':  # pragma: no cover
         return _observer(algorithm, scheme, granularity, dtype, compute_dtype=compute_dtype)
     fake_quant = torch.quantization.FakeQuantize \
                  if version.release < Version("1.10.0").release else \
                      torch.quantization.FusedMovingAvgObsFakeQuantize
     if algorithm == 'minmax':
@@ -785,33 +788,39 @@
         self.query_handler = None
         self.approach = ''
         self.pre_optimized_model = None
         self.sub_module_list = None
         self.default_qconfig = framework_specific_info.get('default_qconfig', None)
         self.performance_only = framework_specific_info.get("performance_only", False)
         self.example_inputs = framework_specific_info.get("example_inputs", None)
+        if isinstance(self.example_inputs, (list, tuple)):
+            self.example_inputs = tuple(self.example_inputs)
+        elif isinstance(self.example_inputs, (dict, UserDict)):
+            self.example_inputs = dict(self.example_inputs)
         if 'recipes' in framework_specific_info:
             self.recipes = framework_specific_info['recipes']
+        else:
+            self.recipes = None
 
         if 'approach' in framework_specific_info:  # pragma: no cover
             self.approach = framework_specific_info['approach']
             if framework_specific_info['approach'] in ["post_training_static_quant",
                 "post_training_auto_quant"]:
-                if self.version.release < Version("1.7.0").release:
+                if self.version.release < Version("1.7.0").release: # pragma: no cover
                     self.q_mapping = tq.default_mappings.DEFAULT_MODULE_MAPPING
-                elif self.version.release < Version("1.8.0").release:
+                elif self.version.release < Version("1.8.0").release: # pragma: no cover
                     self.q_mapping = \
                         tq.quantization_mappings.get_static_quant_module_mappings()
                 else:
                     self.q_mapping = \
                         tq.quantization_mappings.get_default_static_quant_module_mappings()
             elif framework_specific_info['approach'] == "quant_aware_training":
-                if self.version.release < Version("1.7.0").release:
+                if self.version.release < Version("1.7.0").release: # pragma: no cover
                     self.q_mapping = tq.default_mappings.DEFAULT_QAT_MODULE_MAPPING
-                elif self.version.release < Version("1.8.0").release:
+                elif self.version.release < Version("1.8.0").release: # pragma: no cover
                     self.q_mapping = \
                         tq.quantization_mappings.get_qat_module_mappings()
                 else:
                     self.q_mapping = \
                         tq.quantization_mappings.get_default_qat_module_mappings()
             elif framework_specific_info['approach'] == "post_training_dynamic_quant":
                 if self.version.release < Version("1.7.0").release:
@@ -820,16 +829,23 @@
                 elif self.version.release < Version("1.8.0").release:
                     self.q_mapping = \
                         tq.quantization_mappings.get_dynamic_quant_module_mappings()
                 else:
                     self.q_mapping = \
                         tq.quantization_mappings.get_default_dynamic_quant_module_mappings()
             else:
-                assert False, "Unsupport approach: {}".format(self.approach)
+                if not self.benchmark:
+                    assert False, "Unsupport approach: {}".format(self.approach)
 
+        # TODO: will be removed once 'op_type_dict' and 'op_name_dicts' 
+        # for quant_aware_training can be handled in strategy
+        if self.approach == 'quant_aware_training':
+            self.qat_optype_wise = framework_specific_info.get('qat_optype_wise', None)
+            self.qat_op_wise = framework_specific_info.get('qat_op_wise', None)
+        
         self.fp32_results = []
         self.fp32_preds_as_label = False
 
     def calib_func(self, model, dataloader, tmp_iterations, conf=None):
         try:
             for idx, (input, label) in enumerate(dataloader):
                 output = pytorch_forward_wrapper(model,
@@ -919,15 +935,15 @@
                             metric.hvd = hvd
 
                 if self.fp32_preds_as_label:
                     self.fp32_results.append(output) if self.is_baseline else \
                         results.append(output)
                 if idx + 1 == iteration:
                     break
-        except Exception as e:
+        except Exception as e: # pragma: no cover
             logger.warning("The dataloader didn't include label, will try input without label!")
             for idx, input in enumerate(dataloader):
                 if (isinstance(input, dict) or isinstance(input, UserDict)):
                     if not self.benchmark:
                         assert "label" in input, \
                             "The dataloader must include label to measure the metric!"
                         label = input["label"].to("cpu")
@@ -1039,26 +1055,26 @@
 
         Returns:
             q_capability (dictionary): tuning capability for each op from model.
         """
         tmp_model = model
         tmp_model.eval()
         quantizable_ops = []
+        self.block_wise =[]
         self._get_quantizable_ops_recursively(tmp_model, '', quantizable_ops)
-        # capability = self.query_handler.get_quantization_capability()['dynamic'] \
-        #     if self.approach == "post_training_dynamic_quant" else \
-        #     self.query_handler.get_quantization_capability()['quant_aware'] \
-        #     if self.approach == "quant_aware_training" else \
-        #     self.query_handler.get_quantization_capability()['static']
-        
         q_capability = {}
+        q_capability['block_wise'] = None
         q_capability['optypewise'] = OrderedDict()
         q_capability['opwise'] = OrderedDict()
+        # add block ops
+        if self.block_wise:
+            logger.debug(f"*** Found {len(self.block_wise)} blocks: {self.block_wise}")
+        q_capability['block_wise'] = self.block_wise[::-1] if self.block_wise else None
+        
         quant_datatypes = self.query_handler.get_quant_datatypes()
-
         if self.approach == "quant_aware_training":
             capability_pair = [(self.query_handler.get_quantization_capability()['quant_aware'], 'static')]
             fp32_config = {'activation': {'dtype': 'fp32'}, 'weight': {'dtype': 'fp32'}}
             # Ignore LayerNorm, InstanceNorm3d and Embedding quantizable ops,
             # due to huge accuracy regression in PyTorch.
             if isinstance(self, PyTorch_IPEXAdaptor):
                 additional_skipped_module_classes = {}
@@ -1254,36 +1270,40 @@
         Returns:
             model: A modified fp32 model, inplace=True.
         """
         # Note: we should make sure smoothquant is only executed once with inplacing fp32 model.
         if hasattr(model._model, '_smoothquant_optimized') and model._model._smoothquant_optimized:
             logger.info("The model is already optimized by SmoothQuant algorithm, skip it.")
             return model
-        if self.__class__.__name__ == 'PyTorch_IPEXAdaptor' and folding is None:
-            if self.version.release < Version("2.1").release:
+        if self.__class__.__name__ == 'PyTorch_IPEXAdaptor' and self.version.release < \
+          Version("2.1").release:
+            if folding is None:
                 folding = True
                 logger.info(
                     "IPEX version >= 2.1 is required for SmoothQuant folding=False, reset folding=True.")
+            else:
+                assert folding, "IPEX version >= 2.1 is required for SmoothQuant folding=False."
 
         if not hasattr(self, 'sq') or force_re_smooth:
-            self.sq = TorchSmoothQuant(model._model, dataloader=dataloader)
+            from .torch_utils.smooth_quant import TorchSmoothQuant
+            self.sq = TorchSmoothQuant(model._model, dataloader=dataloader,
+                                       example_inputs=self.example_inputs, q_func=self.q_func)
         kwargs = {}  ##different backends may have different default values
         if op_types != None:
             kwargs["op_types"] = op_types
         if percentile != None:
             kwargs['percentile'] = percentile
         if scales_per_op != None:
             kwargs['scales_per_op'] = scales_per_op
         model._model = self.sq.transform(
             alpha=alpha,
             folding=folding,
             calib_iter=calib_iter,
             **kwargs
         )
-        model._model._smoothquant_optimized = True
         return model
 
     def qdq_quantize(self, model, tune_cfg):
         """insert quant, dequant pairs before linear to simulate quantization.
 
         Args:
             model (torch.nn.Module): smoothquant optimized model.
@@ -1300,64 +1320,77 @@
         stats_result = {}
         for (op_name, op_type), qconfig in tune_cfg['op'].items():
             if op_type == 'Linear' and qconfig['weight']['dtype'] != 'int8':
                 # rstrip is for auto strategy, the model passed to the second strategy is already optimized.
                 op_name = op_name.rstrip('.sq_linear') 
                 fallback_op_name_list.append(op_name)
 
+        smoothquant_op_info = {'sq_linear': {}, 'qdq_linear': []}
         stats_result['SQLinearWrapper'] = {'INT8(QDQ)': 0, 'BF16': 0, 'FP32': 0}
         for name, module in q_model.named_modules():
             if isinstance(module, SQLinearWrapper):
+                smoothquant_op_info['sq_linear'][name] = module.input_scale
                 if name not in fallback_op_name_list:
                     smoothquant_scale_info[name] = {
                         'input_scale_for_mul': module.input_scale,
                         'quant_scale': module.scale,
                         'quant_zero_point': module.zero_point,
                         'quant_dtype': module.dtype,
                         }
+                    smoothquant_op_info['qdq_linear'].append(name+'.sq_linear')
                     new_module = QDQLinear(module.sq_linear, module.scale, module.zero_point, module.dtype)
                     set_module(q_model, name+'.sq_linear', new_module)
                     stats_result['SQLinearWrapper']['INT8(QDQ)'] += 1
                 else:
                     stats_result['SQLinearWrapper']['FP32'] += 1
 
-        tune_cfg['recipe_cfgs']['smoothquant_scale_info'] = smoothquant_scale_info
+        tune_cfg['recipe_cfgs']['smoothquant_op_info'] = smoothquant_op_info
         model._model = q_model
         model.q_config = copy.deepcopy(tune_cfg)
         field_names=["Op Type", "Total", "INT8", "BF16", "FP32"]
         output_data = [[
                 op_type, sum(stats_result[op_type].values()), stats_result[op_type]['INT8(QDQ)'], 
                 stats_result[op_type]['BF16'], stats_result[op_type]['FP32']]
                     for op_type in stats_result.keys()]
         Statistics(output_data,
                    header='Mixed Precision Statistics',
                    field_names=field_names).print_stat()
 
         return model
 
-    def _wrapper_sq_linear(self, tmp_model):
+    def _wrapper_sq_linear(self, tmp_model, recover=False):
         """Help function for _get_quantizable_ops_recursively to align smoothquant processed model"""
         class SQLinearWrapper(torch.nn.Module):
             def __init__(self, module):
                 super().__init__()
                 self.add_module('sq_linear', module)
 
             def forward(self, X):
                 return self.sq_linear(X)
+            
+            @property 
+            def weight(self):
+                return self.sq_linear.weight
 
-        module_name_list = []
         from .torch_utils.smooth_quant import get_module, set_module
-        for name, module in tmp_model.named_modules():
-            if 'Linear' == str(module.__class__.__name__):
-                module_name_list.append(name)
-        for name in module_name_list:
-            module = get_module(tmp_model, name)
-            new_module = SQLinearWrapper(module)
-            set_module(tmp_model, name, new_module)
-        return tmp_model
+        if recover:
+            for name in self.sq_module_name_list:
+                new_module = get_module(tmp_model, name+'.sq_linear')
+                set_module(tmp_model, name, new_module)
+            return tmp_model
+        else:
+            self.sq_module_name_list = []
+            for name, module in tmp_model.named_modules():
+                if 'Linear' == str(module.__class__.__name__):
+                    self.sq_module_name_list.append(name)
+            for name in self.sq_module_name_list:
+                module = get_module(tmp_model, name)
+                new_module = SQLinearWrapper(module)
+                set_module(tmp_model, name, new_module)
+            return tmp_model
 
 
 unify_op_type_mapping = {
     "ConvReLU2d": "Conv2d",
     "ConvReLU3d": "Conv3d",
     "LinearReLU": "Linear",
     "ConvBn2d": "Conv2d",
@@ -1445,15 +1478,16 @@
                 q_model = model
 
         # For smoothquant optimized model
         recipe_cfgs = tune_cfg.get('recipe_cfgs', None)
         if recipe_cfgs and recipe_cfgs.get('smooth_quant', False) \
           and not recipe_cfgs['smooth_quant_args']['folding'] \
           and self.approach != 'post_training_dynamic_quant':
-            return self.qdq_quantize(q_model, tune_cfg)
+            if model._model._smoothquant_optimized:
+                return self.qdq_quantize(q_model, tune_cfg)
 
         # For tensorboard display
         self.tune_cfg = tune_cfg
         self.tune_cfg["approach"] = self.approach
         self.tune_cfg["reduce_range"] = REDUCE_RANGE
         self.tune_cfg["framework"] = "pytorch"
         op_cfgs = _cfg_to_qconfig(tune_cfg, self.approach)
@@ -1988,15 +2022,15 @@
             if not inplace:
                 model = copy.deepcopy(model)
             _propagate_qconfig_helper(model,
                                       qconfig_dict={},
                                       white_list=white_list,
                                       qconfig_parent=model.qconfig)
             # sanity check common API misusage
-            if not any(hasattr(m, 'qconfig') and m.qconfig for m in model.modules()):
+            if not any(hasattr(m, 'qconfig') and m.qconfig for m in model.modules()): # pragma: no cover
                 logger.warn("None of the submodule got qconfig applied. Make sure you "
                             "passed correct configuration through `qconfig_dict` or "
                             "by assigning the `.qconfig` attribute directly on submodules")
             _add_observer_(model, op_list=op_list)
             return model
 
         # create properties
@@ -2410,31 +2444,34 @@
         if self.approach == 'post_training_dynamic_quant':
             additional_skipped_module_classes.remove('Embedding')
         custom_non_quant_dict['skipped_module_classes'] += additional_skipped_module_classes
         return custom_non_quant_dict
 
 
 unify_op_type_mapping_ipex = {
-    "Convolution_Relu": "conv2d",
-    "Convolution_Sum_Relu": "conv2d",
-    "Convolution_BatchNorm": "conv2d",
-    "<class 'torch.nn.modules.conv.Conv1d'>": "conv1d",
-    "<class 'torch.nn.modules.conv.Conv2d'>": "conv2d",
-    "<class 'torch.nn.modules.conv.Conv3d'>": "conv3d",
-    "<class 'torch.nn.modules.activation.ReLU'>": "relu",
+    "Convolution_Relu": "Conv2d",
+    "Convolution_Sum_Relu": "Conv2d",
+    "Convolution_BatchNorm": "Conv2d",
+    "<class 'torch.nn.modules.conv.Conv1d'>": "Conv1d",
+    "<class 'torch.nn.modules.conv.Conv2d'>": "Conv2d",
+    "<class 'torch.nn.modules.conv.Conv3d'>": "Conv3d",
+    "<class 'torch.nn.modules.activation.ReLU'>": "ReLU",
     "<method 'add' of 'torch._C._TensorBase' objects>": "add",
-    "<class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>": "adaptiveavgpool2d",
-    "Linear_Relu": "linear",
-    "<class 'torch.nn.modules.linear.Linear'>": "linear",
-    "<class 'torch.nn.modules.pooling.MaxPool2d'>": "maxpool2d"
+    "<class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>": "AdaptiveAvgPool2d",
+    "Linear_Relu": "Linear",
+    "<class 'torch.nn.modules.linear.Linear'>": "Linear",
+    "<class 'torch.nn.modules.pooling.MaxPool2d'>": "MaxPool2d",
+    're': {
+        "<built-in method matmul of type object at": "matmul"
+    }
 }
 
 
 @adaptor_registry
-class PyTorch_IPEXAdaptor(TemplateAdaptor):  # pragma: no cover
+class PyTorch_IPEXAdaptor(TemplateAdaptor):
     """Adaptor of PyTorch framework with Intel PyTorch Extension,
        all PyTorch IPEX API is in this class.
 
     Args:
         framework_specific_info (dict): dictionary of tuning configure from yaml file.
     """
     def __init__(self, framework_specific_info):
@@ -2471,16 +2508,18 @@
             (dict): quantized model
         """
 
         # For smoothquant optimized model
         recipe_cfgs = tune_cfg.get('recipe_cfgs', None)
         if recipe_cfgs and recipe_cfgs.get('smooth_quant', False) \
           and self.version.release >= Version("2.1").release \
+          and not recipe_cfgs['smooth_quant_args']['folding'] \
           and self.approach != 'post_training_dynamic_quant':
-            return self.qdq_quantize(model, tune_cfg, dataloader)
+            if model._model._smoothquant_optimized:
+                return self.qdq_quantize(model, tune_cfg, dataloader, q_func)
 
         assert self.approach != 'quant_aware_training', \
             "Intel PyTorch Extension didn't support quantization aware training mode"
         assert not self.version.release < Version("1.10.0").release, \
                 "INC support IPEX version >= 1.10.0"
 
         qscheme = self._cfg_to_qconfig(tune_cfg)
@@ -2499,32 +2538,46 @@
                 q_model.save_qconf_summary(qconf_summary=self.ipex_config_path)
                 if self.use_bf16 and (CpuInfo().bf16 or os.getenv('FORCE_BF16') == '1') and \
                     (self.version.release >= Version("1.11.0").release):
                     with torch.no_grad():
                         with torch.cpu.amp.autocast():
                             q_model = ipex.quantization.convert(q_model, inplace=True)
                             try:
-                                q_model = torch.jit.trace(q_model, self.example_inputs)
+                                if isinstance(self.example_inputs, dict):
+                                    q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs)
+                                else:
+                                    q_model = torch.jit.trace(q_model, self.example_inputs)
                                 q_model = torch.jit.freeze(q_model.eval())
                             except:
-                                q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
+                                if isinstance(self.example_inputs, dict):
+                                    q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs,
+                                                              strict=False)
+                                else:
+                                    q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
                                 q_model = torch.jit.freeze(q_model.eval())
                 else:
                     q_model = ipex.quantization.convert(q_model, inplace=True)
                     with torch.no_grad():
                         try:
-                            q_model = torch.jit.trace(q_model, self.example_inputs)
+                            if isinstance(self.example_inputs, dict):
+                                q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs)
+                            else:
+                                q_model = torch.jit.trace(q_model, self.example_inputs)
                             q_model = torch.jit.freeze(q_model.eval())
                         except:
-                            q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
+                            if isinstance(self.example_inputs, dict):
+                                q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs,
+                                                          strict=False)
+                            else:
+                                q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
                             q_model = torch.jit.freeze(q_model.eval())
                 # After freezing, run 1 time to warm up the profiling graph executor to insert prim::profile
                 # At the 2nd run, the llga pass will be triggered and the model is turned into
                 # an int8 model: prim::profile will be removed and will have LlgaFusionGroup in the graph
-                self.calib_func(q_model, dataloader, tmp_iterations=2)
+                self._simple_inference(q_model, dataloader, iterations=2)
             else:
                 assert not self.version.release < Version("1.10.0").release, \
                     "INC support IPEX version >= 1.10.0"
                 if self.approach in ['post_training_static_quant', 'post_training_auto_quant']:
                     q_model = model.model
                     if self.version.release < Version("1.12.0").release:
                         ipex_conf = ipex.quantization.QuantConf(configure_file=self.ipex_config_path,  # pylint: disable=E1101
@@ -2535,56 +2588,82 @@
                         ipex_conf = ipex.quantization.QuantConf(self.ipex_config_path)   # pylint: disable=E1101
                         q_model = ipex.quantization.convert(q_model,
                                                             ipex_conf,
                                                             self.example_inputs,
                                                             inplace=True)  # pylint: disable=E1121
                     else:
                         from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig
-                        static_qconfig = QConfig(activation=MinMaxObserver.with_args(
-                            qscheme=torch.per_tensor_affine, dtype=torch.quint8),
-                            weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, \
-                                        qscheme=torch.per_channel_symmetric))
+                        if self.version.release >= Version("2.1").release:
+                            static_qconfig = ipex.quantization.default_static_qconfig_mapping
+                        else:
+                            static_qconfig = QConfig(activation=MinMaxObserver.with_args(
+                                qscheme=torch.per_tensor_affine, dtype=torch.quint8),
+                                weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, \
+                                            qscheme=torch.per_channel_symmetric))
+
+                        if isinstance(self.example_inputs, dict):
+                            q_model = ipex.quantization.prepare(model._model, static_qconfig, \
+                                                                example_kwarg_inputs=self.example_inputs, inplace=True)
+                        else:
+                            q_model = ipex.quantization.prepare(model._model, static_qconfig, \
+                                                                example_inputs=self.example_inputs, inplace=True)
 
-                        q_model = ipex.quantization.prepare(model._model, static_qconfig, \
-                                                example_inputs=self.example_inputs, inplace=True)
                         q_model.load_qconf_summary(qconf_summary=self.ipex_config_path)
                         if q_func is not None:
                             q_func(q_model)
                         else:
                             self.model_calibration(q_model, dataloader, iterations, None,
                                                    tune_cfg.get('calib_sampling_size', 1))
                         q_model.save_qconf_summary(qconf_summary=self.ipex_config_path)
                         if self.use_bf16 and (CpuInfo().bf16 or os.getenv('FORCE_BF16') == '1') and \
                             (self.version.release >= Version("1.11.0").release):
                             with torch.no_grad():
                                 with torch.cpu.amp.autocast():
                                     q_model = ipex.quantization.convert(q_model, inplace=True)
                                     try:
-                                        q_model = torch.jit.trace(q_model, self.example_inputs)
+                                        if isinstance(self.example_inputs, dict):
+                                            q_model = torch.jit.trace(q_model,
+                                                                      example_kwarg_inputs=self.example_inputs)
+                                        else:
+                                            q_model = torch.jit.trace(q_model, self.example_inputs)
                                         q_model = torch.jit.freeze(q_model.eval())
                                     except:
-                                        q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
+                                        if isinstance(self.example_inputs, dict):
+                                            q_model = torch.jit.trace(q_model,
+                                                                      example_kwarg_inputs=self.example_inputs,
+                                                                      strict=False)
+                                        else:
+                                            q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
                                         q_model = torch.jit.freeze(q_model.eval())
                         else:
                             q_model = ipex.quantization.convert(q_model, inplace=True)
                             with torch.no_grad():
                                 try:
-                                    q_model = torch.jit.trace(q_model, self.example_inputs)
+                                    if isinstance(self.example_inputs, dict):
+                                        q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs)
+                                    else:
+                                        q_model = torch.jit.trace(q_model, self.example_inputs)
                                     q_model = torch.jit.freeze(q_model.eval())
                                 except:
-                                    q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
+                                    if isinstance(self.example_inputs, dict):
+                                        q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs,
+                                                                  strict=False)
+                                    else:
+                                        q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
                                     q_model = torch.jit.freeze(q_model.eval())
                         # After freezing, run 1 time to warm up the profiling graph executor to insert prim::profile
                         # At the 2nd run, the llga pass will be triggered and the model is turned into
                         # an int8 model: prim::profile will be removed and will have LlgaFusionGroup in the graph
-                        self.calib_func(q_model, dataloader, tmp_iterations=2)
+                        self._simple_inference(q_model, dataloader, iterations=2)
             model._model = q_model
             with open(self.ipex_config_path, 'r') as f:
                 model.tune_cfg = json.load(f)
             model.ipex_config_path = self.ipex_config_path
+            if self.version.release >= Version("1.12.0").release:
+                self._dump_model_op_stats(tune_cfg)
             return model
         else:
             if self.tmp_model is None:
                 try:
                     self.tmp_model = copy.deepcopy(model)
                 except Exception as e:  # pragma: no cover
                     logger.warning("Fail to deep copy the model due to {}, inplace is used now.".format(
@@ -2608,35 +2687,49 @@
                 q_model.save_qconf_summary(qconf_summary=self.ipex_config_path)
                 if self.use_bf16 and (CpuInfo().bf16 or os.getenv('FORCE_BF16') == '1') and \
                     (self.version.release >= Version("1.11.0").release):
                     with torch.no_grad():
                         with torch.cpu.amp.autocast():
                             q_model = ipex.quantization.convert(q_model, inplace=False)
                             try:
-                                q_model = torch.jit.trace(q_model, self.example_inputs)
+                                if isinstance(self.example_inputs, dict):
+                                    q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs)
+                                else:
+                                    q_model = torch.jit.trace(q_model, self.example_inputs)
                                 q_model = torch.jit.freeze(q_model.eval())
                             except:
-                                q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
+                                if isinstance(self.example_inputs, dict):
+                                    q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs,
+                                                              strict=False)
+                                else:
+                                    q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
                                 q_model = torch.jit.freeze(q_model.eval())
                 else:
                     q_model = ipex.quantization.convert(q_model, inplace=False)
                     with torch.no_grad():
                         try:
-                            q_model = torch.jit.trace(q_model, self.example_inputs)
+                            if isinstance(self.example_inputs, dict):
+                                q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs)
+                            else:
+                                q_model = torch.jit.trace(q_model, self.example_inputs)
                             q_model = torch.jit.freeze(q_model.eval())
                         except:
-                            q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
+                            if isinstance(self.example_inputs, dict):
+                                q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs,
+                                                          strict=False)
+                            else:
+                                q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
                             q_model = torch.jit.freeze(q_model.eval())
                 # After freezing, run 1 time to warm up the profiling graph executor to insert prim::profile
                 # At the 2nd run, the llga pass will be triggered and the model is turned into
                 # an int8 model: prim::profile will be removed and will have LlgaFusionGroup in the graph
-                self.calib_func(q_model, dataloader, tmp_iterations=2)
+                self._simple_inference(q_model, dataloader, iterations=2)
             else:
                 if self.approach in ['post_training_static_quant', 'post_training_auto_quant']:
-                    if self.version.release < Version("1.12.0").release:
+                    if self.version.release < Version("1.12.0").release: # pragma: no cover
                         try:
                             self.tmp_model = copy.deepcopy(model)
                         except Exception as e:  # pragma: no cover
                             logger.warning("Fail to deep copy the model due to {}, inplace is used now.".format(
                                 repr(e)))
                             self.tmp_model = model
                         ipex_conf = ipex.quantization.QuantConf(configure_file=self.ipex_config_path,  # pylint: disable=E1101
@@ -2654,59 +2747,127 @@
                             try:
                                 self.tmp_model = copy.deepcopy(model)
                             except Exception as e:  # pragma: no cover
                                 logger.warning("Fail to deep copy the model due to {}, inplace is used now.".format(
                                     repr(e)))
                                 self.tmp_model = model
                         from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig
-                        static_qconfig = QConfig(activation=MinMaxObserver.with_args(
-                            qscheme=torch.per_tensor_affine, dtype=torch.quint8),
-                            weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, \
-                                        qscheme=torch.per_channel_symmetric))
-
-                        q_model = ipex.quantization.prepare(model._model, static_qconfig, \
-                                                example_inputs=self.example_inputs, inplace=False)
+                        if self.version.release >= Version("2.1").release:
+                            static_qconfig = ipex.quantization.default_static_qconfig_mapping
+                        else:
+                            static_qconfig = QConfig(activation=MinMaxObserver.with_args(
+                                qscheme=torch.per_tensor_affine, dtype=torch.quint8),
+                                weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, \
+                                            qscheme=torch.per_channel_symmetric))
+                        if isinstance(self.example_inputs, dict):
+                            q_model = ipex.quantization.prepare(model._model, static_qconfig,
+                                                                example_kwarg_inputs=self.example_inputs,
+                                                                inplace=False)
+                        else:
+                            q_model = ipex.quantization.prepare(model._model, static_qconfig,
+                                                                example_inputs=self.example_inputs, inplace=False)
                         q_model.load_qconf_summary(qconf_summary=self.ipex_config_path)
                         if q_func is not None:
                             q_func(q_model)
                         else:
                             self.model_calibration(q_model, dataloader, iterations, None,
                                                    tune_cfg.get('calib_sampling_size', 1))
                         q_model.save_qconf_summary(qconf_summary=self.ipex_config_path)
                         if self.use_bf16 and (CpuInfo().bf16 or os.getenv('FORCE_BF16') == '1') and \
                             (self.version.release >= Version("1.11.0").release):
                             with torch.no_grad():
                                 with torch.cpu.amp.autocast():
                                     q_model = ipex.quantization.convert(q_model, inplace=True)
                                     try:
-                                        q_model = torch.jit.trace(q_model, self.example_inputs)
+                                        if isinstance(self.example_inputs, dict):
+                                            q_model = torch.jit.trace(q_model,
+                                                                      example_kwarg_inputs=self.example_inputs)
+                                        else:
+                                            q_model = torch.jit.trace(q_model, self.example_inputs)
                                         q_model = torch.jit.freeze(q_model.eval())
                                     except:
-                                        q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
+                                        if isinstance(self.example_inputs, dict):
+                                            q_model = torch.jit.trace(q_model,
+                                                                      example_kwarg_inputs=self.example_inputs,
+                                                                      strict=False)
+                                        else:
+                                            q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
                                         q_model = torch.jit.freeze(q_model.eval())
                         else:
                             q_model = ipex.quantization.convert(q_model, inplace=True)
                             with torch.no_grad():
                                 try:
-                                    q_model = torch.jit.trace(q_model, self.example_inputs)
+                                    if isinstance(self.example_inputs, dict):
+                                        q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs)
+                                    else:
+                                        q_model = torch.jit.trace(q_model, self.example_inputs)
                                     q_model = torch.jit.freeze(q_model.eval())
                                 except:
-                                    q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
+                                    if isinstance(self.example_inputs, dict):
+                                        q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs,
+                                                                  strict=False)
+                                    else:
+                                        q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
                                     q_model = torch.jit.freeze(q_model.eval())
                         # After freezing, run 1 time to warm up the profiling graph executor to insert prim::profile
                         # At the 2nd run, the llga pass will be triggered and the model is turned into
                         # an int8 model: prim::profile will be removed and will have LlgaFusionGroup in the graph
-                        self.calib_func(q_model, dataloader, tmp_iterations=2)
+                        self._simple_inference(q_model, dataloader, iterations=2)
 
             self.tmp_model._model = q_model
             with open(self.ipex_config_path, 'r') as f:
                 self.tmp_model.tune_cfg = json.load(f)
             self.tmp_model.ipex_config_path = self.ipex_config_path
+            if self.version.release >= Version("1.12.0").release:
+                self._dump_model_op_stats(tune_cfg)
             return self.tmp_model
 
+    def _dump_model_op_stats(self, tune_cfg):
+        """This is a function to dump quantizable ops of model to user.
+        Args:
+            tune_cfg (dict): quantization config
+        Returns:
+            None
+        """
+        res = dict()
+        for k, v in tune_cfg["op"].items():
+            op_type_list = k[-1].split("><")
+            op_type = ""
+            for op in op_type_list:
+                if "class" in op:
+                    op_type = op[op.rfind(".") + 1: op.rfind("'")] \
+                        if op_type == "" else op_type + "&" + op[op.rfind(".") + 1: op.rfind("'")]
+                elif "method" in op:
+                    start = op.find("'") + 1
+                    if start > 1:
+                        op_type = op[start: op.find("'", start)] \
+                            if op_type == "" else op_type + "&" + op[start: op.find("'", start)]
+                    else:
+                        start = op.find("method") + 7
+                        op_type = op[start: op.find(" ", start)] \
+                            if op_type == "" else op_type + "&" + op[start: op.find(" ", start)]
+                else:
+                    op_type = op if op_type == "" else op_type + "&" + op
+            if op_type not in res.keys():
+                res[op_type] = {"INT8": 0, "BF16": 0, "FP32": 0}
+            if v["weight"]["dtype"] == "int8":
+                res[op_type]["INT8"] += 1
+            elif v["weight"]["dtype"] == "fp32":
+                res[op_type]["FP32"] += 1
+
+        output_data = [[
+            op_type,
+            sum(res[op_type].values()), res[op_type]['INT8'], res[op_type]['BF16'],
+            res[op_type]['FP32']
+        ] for op_type in res.keys()]
+
+        Statistics(output_data,
+                   header='Mixed Precision Statistics',
+                   field_names=["Op Type", "Total", "INT8", "BF16", "FP32"]).print_stat()
+
     def _cfg_to_qconfig(self, tune_cfg):
         """Convert tune configure to quantization config for each op.
 
             Args:
                 tune_cfg (dict): dictionary of tune configure for each op
                 ipex_config_path: configure file of Intel PyTorch Extension
 
@@ -2735,15 +2896,15 @@
                    'weight': {'dtype': 'fp32'}
                  },
                  ...
               }
             }
         """
         assert self.cfgs is not None, "No configure for IPEX int8 model..."
-        if self.version.release < Version("1.12.0").release:
+        if self.version.release < Version("1.12.0").release: # pragma: no cover
             for key in tune_cfg['op']:
                 try:
                     scheme = tune_cfg['op'][key]['activation']['scheme']
                 except:
                     scheme = 'asym'
                 if scheme not in ['asym', 'sym']:
                     scheme = 'asym'
@@ -2788,24 +2949,25 @@
             with open(self.ipex_config_path, "w") as write_f:
                 json.dump(self.cfgs, write_f)
             if scheme == "asym":
                 return torch.per_tensor_affine
             else:
                 return torch.per_tensor_symmetric
         else:
+            op_infos = copy.deepcopy(self.op_infos_from_cfgs)
             self.cfgs = torch_utils.util.check_cfg_and_qconfig(tune_cfg['op'],
                                               self.cfgs,
-                                              self.op_infos_from_cfgs,
+                                              op_infos,
                                               self.output_tensor_id_op_name)
 
             with open(self.ipex_config_path, "w") as write_f:
                 json.dump(self.cfgs, write_f, indent=4)
             return None
 
-    def get_pattern(self, fallback_op, fuse_ops):
+    def get_pattern(self, fallback_op, fuse_ops): # pragma: no cover
         for fuse_pattern in fuse_ops:
             if fuse_pattern[0] == fallback_op:
                 if fuse_pattern[1] in ['relu_', 'add_']:
                     return None
                 else:
                     return fuse_pattern[1]
         return None
@@ -2874,27 +3036,46 @@
         Args:
             model (object): input model
             prefix (string): prefix of op name
             quantizable_ops (list): list of quantizable ops from model include op name and type.
         Returns:
             None
         """
-
+        
+        # group ops by postion for transform-based model
+        from .torch_utils.pattern_detector import TransformerBasedModelBlockPatternDetector
+        detector = TransformerBasedModelBlockPatternDetector(model)
+        detect_result = detector.detect_block()
+        attention_block = detect_result.get("attention_blocks", None)
+        ffn_blocks = detect_result.get("ffn_blocks", None) 
+        logger.info(f"Attention Blocks: {len(attention_block)}")
+        logger.info(f"FFN Blocks: {len(ffn_blocks)}")
         if not os.path.exists(self.ipex_config_path):
             assert isinstance(model, torch.nn.Module), \
                     "The model passed in is not the instance of torch.nn.Module"
 
         if hasattr(model, "save_qconf_summary"):
             os.makedirs(os.path.dirname(self.ipex_config_path), exist_ok=True)
             model.save_qconf_summary(qconf_summary=self.ipex_config_path)
             if self.example_inputs is None:
                 self.example_inputs = get_example_inputs(model, self.q_dataloader)
         else:
             if self.performance_only:
-                tmp_model = model
+                if self.recipes and self.recipes.get('smooth_quant', False):  # pragma: no cover
+                    logger.warning("Smoothquant for ipex requires a deepcopy of model"
+                                    + ", please avoid out of memory.")
+                    try:
+                        # Deepcopy due to model changed when `ipex.quantization.prepare`
+                        tmp_model = copy.deepcopy(model)
+                    except Exception as e:  # pragma: no cover
+                        logger.warning("Fail to deep copy the model due to {}, inplace is used now.".format(
+                            repr(e)))
+                        raise
+                else:
+                    tmp_model = model
             else:
                 try:
                     tmp_model = copy.deepcopy(model)
                 except Exception as e:  # pragma: no cover
                     logger.warning("Fail to deep copy the model due to {}, inplace is used now.".format(
                         repr(e)))
                     raise
@@ -2913,84 +3094,146 @@
                     tmp_model,
                     self.q_dataloader,
                     conf=ipex_conf,
                 )
                 ipex_conf.save(self.ipex_config_path)
             else:
                 if self.approach in ['post_training_static_quant', 'post_training_auto_quant']:
-                    assert self.q_dataloader is not None, "IPEX need q_dataloader to prepare the model"
+                    assert self.q_dataloader or self.example_inputs, \
+                            "IPEX need q_dataloader or example_inputs to prepare the model"
                     from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig
-                    # For smoothquant optimized model
-                    if self.recipes and self.recipes.get('smooth_quant', False) \
-                      and self.version.release >= Version("2.1").release:
-                        static_qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping(alpha=0.5)
-                        if not hasattr(tmp_model, '_smoothquant_optimized') \
-                          or not tmp_model._smoothquant_optimized:
-                            # to make sure ipex_config.json is based on pre-optimized model
-                            tmp_model = self._wrapper_sq_linear(tmp_model)
+                    if self.version.release >= Version("2.1").release:
+                        # HistogramObserver will cause a performance issue.
+                        # static_qconfig = ipex.quantization.default_static_qconfig_mapping
+                        qconfig = QConfig(activation=MinMaxObserver.with_args(
+                            qscheme=torch.per_tensor_affine, dtype=torch.quint8),
+                            weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, \
+                                    qscheme=torch.per_channel_symmetric))
+                        from torch.ao.quantization import QConfigMapping
+                        static_qconfig = QConfigMapping().set_global(qconfig)
                     else:
                         static_qconfig = QConfig(activation=MinMaxObserver.with_args(
                             qscheme=torch.per_tensor_affine, dtype=torch.quint8),
                             weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, \
                                     qscheme=torch.per_channel_symmetric))
+                    # For smoothquant optimized model, need ipex version >= 2.1
+                    if self.recipes and self.recipes.get('smooth_quant', False) \
+                      and self.version.release >= Version("2.1").release:  # pragma: no cover
+                        smooth_quant_args = self.recipes.get('smooth_quant_args', {})
+                        folding = smooth_quant_args.get('folding', False)
+                        if not folding:
+                            static_qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping(alpha=0.5)
+                            if not hasattr(tmp_model, '_smoothquant_optimized') \
+                              or not tmp_model._smoothquant_optimized:
+                                # to make sure ipex_config.json is based on pre-optimized model
+                                tmp_model = self._wrapper_sq_linear(tmp_model)
                     if self.example_inputs is None:
                         self.example_inputs = get_example_inputs(tmp_model, self.q_dataloader)
-                    tmp_model = ipex.quantization.prepare(tmp_model, static_qconfig, \
-                                            example_inputs=self.example_inputs, inplace=True)
-                if self.q_func is None:
-                    self.model_calibration(tmp_model, self.q_dataloader)
+                    if isinstance(self.example_inputs, dict):
+                        tmp_model = ipex.quantization.prepare(tmp_model, static_qconfig,
+                                                              example_kwarg_inputs=self.example_inputs, inplace=True)
+                    else:
+                        tmp_model = ipex.quantization.prepare(tmp_model, static_qconfig,
+                                                              example_inputs=self.example_inputs, inplace=True)
+
+                if self.q_dataloader or self.example_inputs:
+                    self._simple_inference(tmp_model, self.q_dataloader, iterations=1)
                 else:
-                    self.q_func(tmp_model)
+                    try:
+                        self.q_func(tmp_model)
+                    except Exception as e:
+                        logger.error("Calibration with IPEX failed due to:{}".format(e))
+                        assert False, "Please pass in example_inputs or calib_dataloader to bypass."
+
                 tmp_model.save_qconf_summary(qconf_summary=self.ipex_config_path)
             if isinstance(self.q_dataloader, BaseDataLoader):
                 self.q_dataloader.batch(batch_size)
                 logger.info('Recovery `calibration.dataloader.batchsize` {} according \
                             to config.yaml'.format(batch_size))
             if not self.performance_only:
                 del tmp_model
                 import gc
                 gc.collect()
-
+        map_op_name_to_fqn = {}
+        
         with open(self.ipex_config_path, 'r') as f:
             self.cfgs = json.load(f)
-            if self.version.release < Version("1.12.0").release:
+            if self.version.release < Version("1.12.0").release: # pragma: no cover
                 self.default_cfgs = copy.deepcopy(self.cfgs)
                 self.fuse_ops = self.get_fuse_ops(self.cfgs)
                 for op_cfg in self.cfgs:
-                    quantizable_ops.append(
-                        (op_cfg["id"], unify_op_type_mapping_ipex[op_cfg["name"]]
-                         if op_cfg["name"] in unify_op_type_mapping_ipex else op_cfg["name"]))
+                    if op_cfg["name"] in unify_op_type_mapping_ipex:
+                        quantizable_ops.append((op_cfg["id"], 
+                                                unify_op_type_mapping_ipex[op_cfg["name"]]))
+                    else:
+                        re_flag = False
+                        for pattern, unify_op_type in unify_op_type_mapping_ipex['re'].items():
+                            if re.match(pattern, op_cfg["name"]):
+                                re_flag = True
+                                quantizable_ops.append((op_cfg["id"], unify_op_type))
+                                break
+                        if not re_flag:
+                            quantizable_ops.append((op_cfg["id"], op_cfg["name"]))
             else:
                 ops_name, op_infos_from_cfgs, input_tensor_id_op_name, \
                                 output_tensor_id_op_name = torch_utils.util.paser_cfgs(self.cfgs)
                 quantizable_op_names = torch_utils.util.get_quantizable_ops_from_cfgs(ops_name,
                                                                      op_infos_from_cfgs,
                                                                      input_tensor_id_op_name)
                 for name in quantizable_op_names:
                     # name : list
                     if len(name) == 1:
                         module_key = name[0][0]
                         op_cfg_id = name[0][2]
-                        quantizable_ops.append((tuple(name), unify_op_type_mapping_ipex \
-                                               [self.cfgs[module_key]['q_op_infos'][op_cfg_id]['op_type']] \
-                                               if self.cfgs[module_key]['q_op_infos'][op_cfg_id]['op_type'] \
-                                               in unify_op_type_mapping_ipex else \
-                                               self.cfgs[module_key]['q_op_infos'][op_cfg_id]['op_type']))
+                        ipex_op_type = self.cfgs[module_key]['q_op_infos'][op_cfg_id]['op_type']
+                        module_fqn = self.cfgs[module_key]['q_op_infos'][op_cfg_id].get('fqn', None)
+                        
+                        if ipex_op_type in unify_op_type_mapping_ipex:
+                            quantizable_ops.append((tuple(name), 
+                                                    unify_op_type_mapping_ipex[ipex_op_type]))
+                            map_op_name_to_fqn[(tuple(name), ipex_op_type)] = module_fqn
+                        else:
+                            re_flag = False
+                            for pattern, unify_op_type in unify_op_type_mapping_ipex['re'].items():
+                                if re.match(pattern, ipex_op_type):
+                                    re_flag = True
+                                    quantizable_ops.append((tuple(name), unify_op_type))
+                                    map_op_name_to_fqn[(tuple(name), unify_op_type)] = module_fqn
+                                    break
+                            if not re_flag:
+                                quantizable_ops.append((tuple(name), ipex_op_type))
+                                map_op_name_to_fqn[(tuple(name), ipex_op_type)] = module_fqn
                     else:
                         op_type = ""
                         for op_name in name:
                             module_key = op_name[0]
                             op_cfg_id = op_name[2]
-                            op_type += self.cfgs[module_key]['q_op_infos'][op_cfg_id]['op_type']
+                            single_op_type = self.cfgs[module_key]['q_op_infos'][op_cfg_id]['op_type']
+                            if single_op_type in unify_op_type_mapping_ipex:
+                                single_op_type = unify_op_type_mapping_ipex[single_op_type]
+                            op_type += "&" + single_op_type if op_type else single_op_type
                         quantizable_ops.append((tuple(name), op_type))
+                        _module_key = name[0][0]
+                        _op_cfg_id = name[0][2]
+                        module_fqn = self.cfgs[_module_key]['q_op_infos'][_op_cfg_id]['fqn']
+                        map_op_name_to_fqn[(tuple(name), op_type)] = module_fqn
                 self.op_infos_from_cfgs = op_infos_from_cfgs
                 self.output_tensor_id_op_name = output_tensor_id_op_name
+        logger.debug("Map op name to fqn: ")
+        logger.debug(map_op_name_to_fqn)
+        logger.info("Attention Blocks : ")
+        logger.info(attention_block)
+        logger.info("FFN Blocks : ")
+        logger.info(ffn_blocks)
+        self.block_wise = ffn_blocks
+        
+
         os.remove(self.ipex_config_path)
 
-    def get_fuse_ops(self, default_cfgs):
+    def get_fuse_ops(self, default_cfgs): # pragma: no cover
         elt_wise = ['relu', 'sigmoid', 'gelu']
         inplace_ops = ['relu_', 'add_']
         op_patterns = []
         num_ops = len(default_cfgs)
         for cur_id in range(num_ops):
             cur_op = default_cfgs[cur_id]['name']
             if cur_op == 'dropout':
@@ -3017,15 +3260,15 @@
             if len(pre_ops) > 0:
                 for key, value in pre_ops.items():
                     if value[1] in ['conv2d', 'conv3d', 'linear'] and \
                             default_cfgs[cur_id]['inputs_quantized'][key] == False:
                         op_patterns.append([(value[0], value[1]), (cur_id, cur_op)])
         return op_patterns
 
-    def qdq_quantize(self, model, tune_cfg, dataloader):
+    def qdq_quantize(self, model, tune_cfg, dataloader, q_func):
         assert not self.version.release < Version("2.1").release, \
             "IPEX version >= 2.1 is required for SmoothQuant."
 
         if not self.performance_only:
             try:
                 self.tmp_model = copy.deepcopy(model)
             except Exception as e:  # pragma: no cover
@@ -3034,76 +3277,116 @@
                 self.tmp_model = model
         else:
             self.tmp_model = model
         q_model = self.tmp_model._model
 
         # fetch SmoothQuant scale info from pre-optimized model
         from .torch_utils.model_wrapper import SQLinearWrapper
-        from .torch_utils.smooth_quant import update_sq_scale
+        from .torch_utils.util import update_sq_scale
         smoothquant_scale_info = {}
         for name, module in q_model.named_modules():
             if isinstance(module, SQLinearWrapper):
                 weight_scale = module._get_weight_scale()
                 smoothquant_scale_info[name + '.sq_linear'] = {
+                    'alpha': module.alpha,
                     'input_scale_for_mul': module.input_scale,
                     'input_scale_after_mul': module.scale,
                     'input_zero_point_after_mul': module.zero_point,
                     'input_dtype': module.dtype,
                     'weight_scale_after_mul': weight_scale,
-                    }
+                }
                 module.ipex = True
                 # Note: save weight scale before recover
                 module._recover_sq_linear()
 
         # Rebuild the config json after pre-optimize algo (SmoothQuant), model is changed.
         static_qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping(alpha=0.5)
-        q_model = ipex.quantization.prepare(q_model, static_qconfig, \
-                                example_inputs=self.example_inputs, inplace=True)
-        self.calib_func(q_model, dataloader, tmp_iterations=1) # fake calibration
+        if isinstance(self.example_inputs, dict):
+            q_model = ipex.quantization.prepare(q_model, static_qconfig,
+                                                example_kwarg_inputs=self.example_inputs, inplace=True)
+        else:
+            q_model = ipex.quantization.prepare(q_model, static_qconfig,
+                                                example_inputs=self.example_inputs, inplace=True)
+
+        # enable fallback
+        self._simple_inference(q_model, dataloader, iterations=1)   # fake calibration for save qconf
         q_model.save_qconf_summary(qconf_summary=self.ipex_config_path)
+        self._cfg_to_qconfig(tune_cfg)
+        # TODO: update_sq_scale is used to update observer, should fuse in _cfg_to_qconfig
+        update_sq_scale(self.ipex_config_path, smoothquant_scale_info)
+        q_model.load_qconf_summary(qconf_summary=self.ipex_config_path)
+
+        # real calibration for other operators
+        try:
+            # IPEX may raise an error on the second iteration.
+            # OverflowError: cannot convert float infinity to integer
+            if q_func is not None:
+                q_func(q_model)
+            else:
+                iterations = tune_cfg.get('calib_iteration', 1)
+                self.model_calibration(q_model, dataloader, iterations, None,
+                                    tune_cfg.get('calib_sampling_size', 1))
+        except:
+            logger.warning("The calibration failed when calibrating with ipex, "+\
+                           "using scale info from SmoothQuant for Linear and " +\
+                           "one iter calibration for other ops.")
 
         # update ipex_config.json with smoothquant_scale_info
+        q_model.save_qconf_summary(qconf_summary=self.ipex_config_path)
         update_sq_scale(self.ipex_config_path, smoothquant_scale_info)
-        # enable fallback
-        self._cfg_to_qconfig(tune_cfg)
         q_model.load_qconf_summary(qconf_summary=self.ipex_config_path)
 
         if self.use_bf16 and (CpuInfo().bf16 or os.getenv('FORCE_BF16') == '1') and \
             (self.version.release >= Version("1.11.0").release):
+            logger.warning("SmoothQuant folding=False with bf16 may cause accuracy=0! " +\
+                            "Please consider setting excluded_precisions=['bf16'] in your config.")
             with torch.no_grad():
                 with torch.cpu.amp.autocast():
                     q_model = ipex.quantization.convert(q_model, inplace=True)
                     # inference once after convert for SmoothQuant
-                    self.calib_func(q_model, dataloader, tmp_iterations=1)
+                    self._simple_inference(q_model, dataloader, iterations=1)
                     try:
-                        q_model = torch.jit.trace(q_model, self.example_inputs)
+                        if isinstance(self.example_inputs, dict):
+                            q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs)
+                        else:
+                            q_model = torch.jit.trace(q_model, self.example_inputs)
                         q_model = torch.jit.freeze(q_model.eval())
                     except:
-                        q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
+                        if isinstance(self.example_inputs, dict):
+                            q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs, strict=False)
+                        else:
+                            q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
                         q_model = torch.jit.freeze(q_model.eval())
         else:
             q_model = ipex.quantization.convert(q_model, inplace=True)
             # inference once after convert for SmoothQuant
-            self.calib_func(q_model, dataloader, tmp_iterations=1)
+            self._simple_inference(q_model, dataloader, iterations=1)
             with torch.no_grad():
                 try:
-                    q_model = torch.jit.trace(q_model, self.example_inputs)
+                    if isinstance(self.example_inputs, dict):
+                        q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs)
+                    else:
+                        q_model = torch.jit.trace(q_model, self.example_inputs)
                     q_model = torch.jit.freeze(q_model.eval())
                 except:
-                    q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
+                    if isinstance(self.example_inputs, dict):
+                        q_model = torch.jit.trace(q_model, example_kwarg_inputs=self.example_inputs, strict=False)
+                    else:
+                        q_model = torch.jit.trace(q_model, self.example_inputs, strict=False)
                     q_model = torch.jit.freeze(q_model.eval())
         # After freezing, run 1 time to warm up the profiling graph executor to insert prim::profile
         # At the 2nd run, the llga pass will be triggered and the model is turned into
         # an int8 model: prim::profile will be removed and will have LlgaFusionGroup in the graph
-        self.calib_func(q_model, dataloader, tmp_iterations=2)
+        self._simple_inference(q_model, dataloader, iterations=2)
         self.tmp_model._model = q_model
 
         with open(self.ipex_config_path, 'r') as f:
             self.tmp_model.tune_cfg = json.load(f)
         self.tmp_model.ipex_config_path = self.ipex_config_path
+        self._dump_model_op_stats(tune_cfg)
         return self.tmp_model
 
     @dump_elapsed_time("Pass save quantized model")
     def save(self, model, path=None):
         """The function is used by tune strategy class for set best configure in Neural Compressor model.
 
            Args:
@@ -3121,14 +3404,27 @@
                        dataloader,
                        op_list=None,
                        iteration_list=None,
                        inspect_type='activation',
                        save_to_disk=False):
         assert False, "Inspect_tensor didn't support IPEX backend now!"
 
+    def _simple_inference(self, q_model, dataloader, iterations=1):
+        """The function is used for ipex warm-up inference."""
+        if self.example_inputs is not None:
+            for _ in range(iterations):
+                if isinstance(self.example_inputs, tuple):
+                    q_model(*self.example_inputs)
+                elif isinstance(self.example_inputs, dict):
+                    q_model(**self.example_inputs)
+                else:
+                    q_model(self.example_inputs)
+        else:
+            self.calib_func(q_model, dataloader, iterations)
+
 
 @adaptor_registry
 class PyTorch_FXAdaptor(TemplateAdaptor):
     """Adaptor of PyTorch framework with FX graph mode, all PyTorch API is in this class.
 
     Args:
         framework_specific_info (dict): dictionary of tuning configure from yaml file.
@@ -3278,16 +3574,14 @@
                     is_qat=True,
                     example_inputs=self.example_inputs,
                     custom_config=self.prepare_custom_config_dict
                 )
             # q_func can be created by neural_compressor internal or passed by user. It's critical to
             # distinguish how q_func is passed since neural_compressor built-in functions accept
             # neural_compressor model and user defined func should accept framework model.
-            # For export API
-            hook_list = torch_utils.util._set_input_scale_hook(q_model._model, op_cfgs)
             q_model._model = q_func(
                 q_model if getattr(q_func, 'builtin', None) else q_model._model)
             assert q_model._model is not None, "Please return a trained model in train function!"
             q_model._model.eval()
         else:
             if self.sub_module_list is None:
                 tmp_model = q_model._model
@@ -3313,31 +3607,25 @@
                     self.fx_op_cfgs,
                     q_model._model,
                     prefix='',
                     example_inputs=self.example_inputs,
                     custom_config=self.prepare_custom_config_dict
                 )
             if self.approach in ['post_training_static_quant', 'post_training_auto_quant']:
-                # For export API
-                hook_list = torch_utils.util._set_input_scale_hook(q_model._model, op_cfgs)
                 iterations = tune_cfg.get('calib_iteration', 1)
                 if q_func is not None:
                     q_func(q_model._model)
                 else:
                     self.model_calibration(
                         q_model._model,
                         dataloader,
                         iterations,
                         calib_sampling_size=tune_cfg.get('calib_sampling_size', 1)
                     )
 
-        if self.approach != 'post_training_dynamic_quant':
-            # For export API
-            scale_info = torch_utils.util._get_input_scale(q_model._model, hook_list)
-
         if self.sub_module_list is None:
             if self.version.release >= Version("1.13.0").release:  # pragma: no cover
                 # pylint: disable=E1123
                 q_model._model = convert_fx(
                     q_model._model,
                     convert_custom_config=self.convert_custom_config_dict
                 )
@@ -3361,15 +3649,14 @@
             self.version.release >= Version("1.11.0").release and self.use_bf16 and \
             (CpuInfo().bf16 or os.getenv('FORCE_BF16') == '1'): # pragma: no cover
             q_model._model = torch_utils.bf16_convert.Convert(q_model._model, self.tune_cfg)
 
         q_model.q_config = copy.deepcopy(self.tune_cfg)
         if self.approach != 'post_training_dynamic_quant':
             self._get_scale_zeropoint(q_model._model, q_model.q_config)
-            q_model.q_config['scale_info'] = scale_info
 
         self._dump_model_op_stats(q_model._model, q_model.q_config, self.approach)
         torch_utils.util.get_embedding_contiguous(q_model._model)
         return q_model
 
     def evaluate(self,
                  model,
@@ -3425,14 +3712,15 @@
                                        dtype=torch.quint8,
                                        qscheme=torch.per_tensor_affine,
                                        reduce_range=REDUCE_RANGE),
                             weight=torch.quantization.default_fused_per_channel_wt_fake_quant)
         quantizable_ops = []
         tmp_model = self.fuse_fx_model(self.model, is_qat=True)
         self._get_quantizable_ops_recursively(tmp_model, '', quantizable_ops)
+        self._remove_fallback_ops_for_qat(quantizable_ops)
         bf16_ops = []
         if self.version.release >= Version("1.11.0").release and self.use_bf16 and \
             (CpuInfo().bf16 or os.getenv('FORCE_BF16') == '1'): # pragma: no cover
             self.bf16_ops = self.query_handler.get_op_types_by_precision("bf16")
             self._get_bf16_ops_recursively(tmp_model, '', bf16_ops)
         bf16_ops_list = [(op) for op in bf16_ops if op not in quantizable_ops]
         quantized_ops = OrderedDict()
@@ -3444,15 +3732,15 @@
             else:
                 quantized_ops[op[0]] = q_cfgs
         # build op_config_dict to save module scale and zeropoint
         op_config_dict = {}
         for op in quantizable_ops:
             op_config_dict[op] = {'weight': {'dtype': 'int8'}, 'activation': {'dtype': 'uint8'}}
 
-        if self.version.release < Version("1.11.0").release:
+        if self.version.release < Version("1.11.0").release: # pragma: no cover
             quantized_ops["default_qconfig"] = None
         else:
             from torch.ao.quantization import default_embedding_qat_qconfig
             for op in quantizable_ops:
                 if op[1] in ['Embedding', 'EmbeddingBag']:
                     quantized_ops[op[0]] = default_embedding_qat_qconfig
         from torch.quantization.quantize_fx import prepare_qat_fx
@@ -3502,22 +3790,16 @@
             'reduce_range': REDUCE_RANGE,
             'quantizable_ops': quantizable_ops,
             'bf16_ops_list': bf16_ops_list,
             'op': op_config_dict,
             'sub_module_list': self.sub_module_list,
             'approach': 'quant_aware_training'
         }
-        # For export API
-        global hook_list
-        hook_list = torch_utils.util._set_input_scale_hook(self.model._model, quantized_ops)
 
     def _post_hook_for_qat(self):
-        # For export API
-        scale_info = torch_utils.util._get_input_scale(self.model._model, hook_list)
-        self.model.q_config['scale_info'] = scale_info
         from torch.quantization.quantize_fx import convert_fx
         if self.sub_module_list is None:
             if self.version > Version("1.12.1"):  # pragma: no cover
                 # pylint: disable=E1123
                 self.model._model = convert_fx(
                     self.model._model,
                     convert_custom_config=self.model.kwargs.get(
@@ -3536,14 +3818,37 @@
         if len(self.model.q_config['bf16_ops_list']) > 0 and \
             self.version.release >= Version("1.11.0").release and self.use_bf16 and \
             (CpuInfo().bf16 or os.getenv('FORCE_BF16') == '1'): # pragma: no cover
             self.model._model = torch_utils.bf16_convert.Convert(self.model._model, self.model.q_config)
         self._dump_model_op_stats(self.model._model, self.model.q_config, self.approach)
         torch_utils.util.get_embedding_contiguous(self.model._model)
 
+    def _get_fallback_ops_for_qat(self):
+        # get fallback ops for quant aware training approach
+        fallback_ops = {'op_wise': [], 'optype_wise': []}
+        if self.qat_optype_wise is not None: # pragma: no cover
+            for optype, optype_config in self.qat_optype_wise.items():
+                if 'weight' in optype_config and optype_config['weight']['dtype'] == ['fp32']:
+                    fallback_ops['optype_wise'].append(optype)
+        if self.qat_op_wise is not None: # pragma: no cover
+            for op, op_config in self.qat_op_wise.items():
+                if 'weight' in op_config and op_config['weight']['dtype'] == ['fp32']:
+                    fallback_ops['op_wise'].append(op)
+        return fallback_ops
+    
+    def _remove_fallback_ops_for_qat(self, quantizable_ops):
+        # remove fallback ops from quantizable_ops for quant aware training approach
+        fallback_ops = self._get_fallback_ops_for_qat()
+        remove_ops = []
+        for (op_name, op_type) in quantizable_ops:
+            if op_name in fallback_ops['op_wise'] or op_type in fallback_ops['optype_wise']:
+                remove_ops.append((op_name, op_type))
+        for (op_name, op_type) in remove_ops:
+            quantizable_ops.remove((op_name, op_type))
+
     def train(self, model, dataloader, optimizer_tuple, criterion_tuple, hooks, **kwargs):
         """Execute the train process on the specified model.
 
         Args:
             model (object): model to run evaluation.
             dataloader (object): training dataset.
             optimizer (tuple): It is a tuple of (cls, parameters) for optimizer.
@@ -3621,15 +3926,15 @@
         res = dict()
 
         if approach == 'post_training_dynamic_quant':
             # fetch int8 and fp32 ops set by Neural Compressor from tune_cfg
             for key in tune_cfg['op']:
                 op_type = key[1]
                 #build initial dict
-                if op_type not in res.keys():
+                if op_type not in res.keys(): # pragma: no cover
                     res[op_type] = {'INT8': 0, 'BF16': 0, 'FP32': 0}
                 value = tune_cfg['op'][key]
                 # Special cases: QuantStub, Embedding
                 if ('weight' in value and value['weight']['dtype'] == 'fp32') or \
                   ('weight' not in value and value['activation']['dtype'] == 'fp32'):
                     res[op_type]['FP32'] += 1
                 elif value['activation']['dtype'] == 'bf16':  # pragma: no cover
@@ -3640,15 +3945,16 @@
             quantized_mode = False
             for node in model.graph.nodes:
                 if node.op == 'call_module':
                     if node.target not in modules:  # pragma: no cover
                         continue
                     op_class = type(modules[node.target])
                     op_type = str(op_class.__name__)
-                    if 'quantized' in str(op_class) or quantized_mode:
+                    if 'quantized' in str(op_class) \
+                      or (quantized_mode and 'pooling' in str(op_class)):
                         if op_type not in res.keys():
                             res[op_type] = {'INT8': 0, 'BF16': 0, 'FP32': 0}
                         res[op_type]['INT8'] += 1
                     elif op_class in self.white_list:
                         if op_type not in res.keys():
                             res[op_type] = {'INT8': 0, 'BF16': 0, 'FP32': 0}
                         res[op_type]['FP32'] += 1
@@ -3709,26 +4015,26 @@
         if self.sub_module_list is None or \
           self.approach == 'post_training_dynamic_quant':
             res = self._get_module_op_stats(model, tune_cfg, approach)
         else:
             res = dict()
             self._get_sub_module_op_stats(model, tune_cfg, approach, res)
 
-        if self.use_bf16 and (self.version.release >= Version("1.11.0").release) and \
-            (CpuInfo().bf16 or os.getenv('FORCE_BF16') == '1'): # pragma: no cover
-            bf16_ops_list = tune_cfg['bf16_ops_list']
-            if len(bf16_ops_list) > 0:
-                for bf16_op in bf16_ops_list:
-                    op_type = bf16_op[1]
-                    if op_type in res.keys():
-                        res[op_type]['BF16'] += 1
-                        if res[op_type]['FP32'] > 0:
-                            res[op_type]['FP32'] -= 1
-                    else:
-                        res[op_type] = {'INT8': 0, 'BF16': 1, 'FP32': 0}
+            if self.use_bf16 and (self.version.release >= Version("1.11.0").release) and \
+                (CpuInfo().bf16 or os.getenv('FORCE_BF16') == '1'): # pragma: no cover
+                bf16_ops_list = tune_cfg['bf16_ops_list']
+                if len(bf16_ops_list) > 0:
+                    for bf16_op in bf16_ops_list:
+                        op_type = bf16_op[1]
+                        if op_type in res.keys():
+                            res[op_type]['BF16'] += 1
+                            if res[op_type]['FP32'] > 0:
+                                res[op_type]['FP32'] -= 1
+                        else:
+                            res[op_type] = {'INT8': 0, 'BF16': 1, 'FP32': 0}
 
 
         output_data = [[
             op_type,
             sum(res[op_type].values()), res[op_type]['INT8'], res[op_type]['BF16'],
             res[op_type]['FP32']
         ] for op_type in res.keys()]
@@ -3745,30 +4051,43 @@
             model (object): input model
             prefix (string): prefix of op name
             quantizable_ops (list): list of quantizable ops from model include op name and type.
 
         Returns:
             None
         """
+        from .torch_utils.pattern_detector import TransformerBasedModelBlockPatternDetector
+        from .torch_utils.util import get_op_type_by_name
+        detector = TransformerBasedModelBlockPatternDetector(model)
+        detect_result = detector.detect_block()
+        attention_block = detect_result.get("attention_blocks", None)
+        ffn_blocks = detect_result.get("ffn_blocks", None) 
+        logger.info(f"Attention Blocks: {len(attention_block)}")
+        logger.info(f"FFN Blocks: {len(ffn_blocks)}")
         module_dict = dict(model.named_modules())
         for op_name, child in model.named_modules():
             if self.is_fused_module(child):
                 for name, _ in child.named_children():
                     module_prefix = op_name + '.' + name
                     if module_prefix in module_dict:
                         module_dict.pop(module_prefix)  # remove sub-modules of fused modules
-
+        q_ops_set = set()
         for op_name, child in module_dict.items():
             if type(child) in self.white_list \
                and type(child) != torch.nn.Sequential \
                and type(child) != torch.quantization.stubs.DeQuantStub:
                 quantizable_ops.append(
                     (op_name, unify_op_type_mapping[str(child.__class__.__name__)]
                      if str(child.__class__.__name__) in unify_op_type_mapping else str(
                          child.__class__.__name__)))
+                q_ops_set.add(op_name)
+        # discard the op does not belong to quantizable_ops
+        block_wise = [[(name, get_op_type_by_name(name, quantizable_ops)) for name in block if\
+            get_op_type_by_name(name, quantizable_ops) != None] for block in ffn_blocks]
+        self.block_wise = block_wise
 
     def _get_module_scale_zeropoint(self, model, tune_cfg, prefix=''):
         """get activation scale and zero_point for converted module.
 
         Args:
             model (dir): Int8 model converted from fp32 model.
                          scale and zero_point is set with calibration for each module
@@ -3989,15 +4308,15 @@
             is_qat (bool): check quantization approach is qat or not.
 
         Returns:
             fused_model (GraphModule): fused GraphModule model from torch.fx.
         """
         try:
             tmp_model = copy.deepcopy(model._model)
-        except Exception as e:
+        except Exception as e: # pragma: no cover
             tmp_model = model._model
             logger.warning("Deepcopy failed: {}, inplace=True now!".format(repr(e)))
 
         tmp_model.train() if is_qat else tmp_model.eval()
         from torch.fx import GraphModule
         from torch.quantization.quantize_fx import _fuse_fx, QuantizationTracer
         if model.kwargs is not None:
@@ -4076,15 +4395,14 @@
         Args:
             module (object): input module which is PyTorch Module.
 
         Returns:
             fused_model (GraphModule): fused GraphModule model from torch.fx.
         """
         import inspect
-        import re
         try:
             lines = inspect.getsource(module.forward)
             # Proxy obj. will always be detectd as `not None`.
             # Other situations could be detected by prepare_fx function.
             pattern = "is( not)? None"
             anws = re.search(pattern, lines)
             if anws:
@@ -4172,15 +4490,15 @@
         """
         assert datatype in self.get_quant_datatypes(), \
             f"The target data type should be one of {self.get_quant_datatypes()}"
         return self.cur_config[datatype]
 
     def get_quant_datatypes(self):
         """Got low-precision data types for quantization.
-        
+
         Collects all data types for quantization, such as int8, int4.
         """
         # TODO to handle other data types such FP8, FP8E4M3
         datatype_lst = []
         for key in self.cur_config:
             if key.startswith('int'):
                 datatype_lst.append(key)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/pytorch_cpu.yaml` & `neural_compressor-2.2/neural_compressor/adaptor/pytorch_cpu.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/pytorch_gpu.yaml` & `neural_compressor-2.2/neural_compressor/adaptor/pytorch_gpu.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/pytorch_ipex.yaml` & `neural_compressor-2.2/neural_compressor/adaptor/pytorch_ipex.yaml`

 * *Files 3% similar despite different names*

```diff
@@ -18,31 +18,31 @@
   version:
     name: '1.12'
 
   bf16: []
   fp32: ['*'] # '*' means all op types
   int8: &1_12_capabilities {
     'static': &cap_1_12_s8 {
-            'conv2d': &cap_s8_1_12_Conv2d {
+            'Conv2d': &cap_s8_1_12_Conv2d {
             'weight': {
                         'dtype': ['int8'],
                         'scheme': ['sym'],
                         'granularity': ['per_channel'],
                         'algorithm': ['minmax']
                         },
             'activation': {
                         'dtype': ['uint8'],
                         'scheme': ['asym', 'sym'],
                         'granularity': ['per_tensor'],
                         'algorithm': ['minmax', 'kl']
                         },
                     },
-          'conv1d': *cap_s8_1_12_Conv2d,
-          'conv3d': *cap_s8_1_12_Conv2d,
-          'linear': *cap_s8_1_12_Conv2d,
+          'Conv1d': *cap_s8_1_12_Conv2d,
+          'Conv3d': *cap_s8_1_12_Conv2d,
+          'Linear': *cap_s8_1_12_Conv2d,
           'default': {
             'weight': {
                         'dtype': ['int8'],
                         'scheme': ['sym'],
                         'granularity': ['per_channel'],
                         'algorithm': ['minmax']
                         },
@@ -63,32 +63,32 @@
   version:
     name: '1.10'
 
   bf16: []
   fp32: ['*'] # '*' means all op types
   int8: &1_10_capabilities {
     'static': &cap_1_10_s8 {
-          'conv2d': &cap_s8_1_10_Conv2d {
+          'Conv2d': &cap_s8_1_10_Conv2d {
             'weight': {
                         'dtype': ['int8'],
                         'scheme': ['sym'],
                         'granularity': ['per_channel','per_tensor'],
                         'algorithm': ['minmax']
                         },
             'activation': {
                         'dtype': ['uint8'],
                         'scheme': ['asym', 'sym'],
                         'granularity': ['per_tensor'],
                         'algorithm': ['minmax']
                         },
                     },
-          'conv1d': *cap_s8_1_10_Conv2d,
-          'conv3d': *cap_s8_1_10_Conv2d,
-          'linear': *cap_s8_1_10_Conv2d,
-          'default': {
+          'Conv1d': *cap_s8_1_10_Conv2d,
+          'Conv3d': *cap_s8_1_10_Conv2d,
+          'Linear': *cap_s8_1_10_Conv2d,
+          'default': &cap_s8_1_10_default {
             'weight': {
                         'dtype': ['int8'],
                         'scheme': ['sym'],
                         'granularity': ['per_channel'],
                         'algorithm': ['minmax']
                         },
             'activation': {
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/query.py` & `neural_compressor-2.2/neural_compressor/adaptor/query.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tensorflow.py` & `neural_compressor-2.2/neural_compressor/adaptor/tensorflow.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,15 +29,15 @@
 from ..utils.utility import Statistics, GLOBAL_STATE, MODE
 from ..utils.utility import version1_lt_version2, version1_gte_version2, version1_eq_version2
 from ..utils import logger
 from ..conf.dotdict import deep_get
 from ..data.dataloaders.base_dataloader import BaseDataLoader
 
 tensorflow = LazyImport('tensorflow')
-spr_base_verions = ('2.11.0202242', '2.11.0202250')
+spr_base_verions = ('2.11.0202242', '2.11.0202250', '2.11.0202317')
 
 @adaptor_registry
 class TensorFlowAdaptor(Adaptor):
     """Adaptor Layer for stock tensorflow and spr-base."""
 
     unify_op_type_mapping = {
         "Conv2D": "conv2d",
@@ -80,14 +80,15 @@
 
         self.model = None
         self.pre_optimized_model = None
         self.pre_optimizer_handle = None
 
         self.bf16_ops = []
         self.fp32_ops = []
+        self.smooth_quant_mul_ops = []
         self.dump_times = 0   # for tensorboard
 
         cfg_yaml_name = "{}.yaml".format(self.__class__.__name__[:-len('Adaptor')].lower())
         self.itex_mode = self.backend == 'itex' or cfg_yaml_name == 'tensorflow_itex.yaml'
         self.query_handler = TensorflowQuery(local_config_file=os.path.join(
             os.path.dirname(__file__), cfg_yaml_name),
             performance_only=self.performance_only,
@@ -103,14 +104,15 @@
         self.fp32_preds_as_label = False
         self.benchmark = (GLOBAL_STATE.STATE == MODE.BENCHMARK)
         self.callbacks = []
 
         self.optype_statistics = None
 
         self._last_dequantize_ops = None
+        self.smooth_quant_model = None
 
     def _log_histogram(self, writer, tag, values, step=0, bins=1000):
         """Writes a histogram for later analysis."""
         import tensorflow as tf
         # Convert to a numpy array
         values = np.array(values)
 
@@ -530,26 +532,30 @@
             data_loader (generator): generator the data and labels
             q_func (optional): training function for quantization aware training mode,
                                 which not enabled for tensorflow yet.
 
         Returns:
             tf.compat.v1.GraphDef: the quantized model
         """
+        assert self.approach != "post_training_dynamic_quant", \
+            "Dynamic quantization is not supported on TensorFlow framework now!"
+
         if self.approach == "quant_aware_training": # pragma: no cover
             assert q_func is not None, "quantization aware training mode \
                 is not configured correctly"
 
             from neural_compressor.model import Model
             qat_model = q_func(model)
 
             return self.convert(Model(qat_model), 'QAT', 'default')
 
         assert q_func is None, \
             "post-training quantization mode is not support calibration function for Tensorflow!"
         self._tuning_cfg_to_fw(tune_cfg)
+        self.bf16_ops.extend(self.smooth_quant_mul_ops)
         logger.debug("Dump quantization configurations:")
         logger.debug(self.quantize_config)
         from .tf_utils.graph_converter import GraphConverter
         calib_sampling_size = tune_cfg.get('calib_sampling_size', 1)
         if isinstance(data_loader, BaseDataLoader):
             batch_size = data_loader.batch_size
             try:
@@ -681,18 +687,18 @@
                 elif i.op == 'Cast':
                     if i.attr['DstT'].type == dtypes.bfloat16:
                         res[i.op]['BF16'] += 1
                     elif i.attr['DstT'].type == dtypes.float32:
                         res[i.op]['FP32'] += 1
                 else:
                     res[i.op]['FP32'] += 1
-        
+
         field_names = ["Op Type", "Total", "INT8", "BF16", "FP32"]
         output_data = [[
-            op_type, sum(res[op_type].values()), 
+            op_type, sum(res[op_type].values()),
             res[op_type]['INT8'], res[op_type]['BF16'], res[op_type]['FP32']]
         for op_type in fp32_op_list]
 
         Statistics(output_data,
                    header='Mixed Precision Statistics',
                    field_names=field_names).print_stat()
         self.optype_statistics = field_names, output_data
@@ -718,14 +724,15 @@
         Returns:
             OrderDict: op-wise configuration.
         """
         bf16_common_config = {'weight': {'dtype': 'bf16'}, 'activation': {'dtype': 'bf16'}}
         fp32_common_config = {'weight': {'dtype': 'fp32'}, 'activation': {'dtype': 'fp32'}}
         uint8_type = self.query_handler.get_op_types_by_precision(precision='uint8')
         int8_type = self.query_handler.get_op_types_by_precision(precision='int8')
+        bf16_type = self.query_handler.get_op_types_by_precision(precision='bf16')
         tf_quantizable_op_type = list(set(uint8_type).union(set(int8_type)))
 
         valid_precision = self.query_handler.get_mixed_precision_combination()
         op_capability = self.query_handler.get_quantization_capability()
         conv_config = copy.deepcopy(op_capability['Conv2D'])
         conv3d_config = copy.deepcopy(op_capability['Conv3D']) if 'Conv3D' in op_capability else None
         matmul_config = copy.deepcopy(op_capability['MatMul'])
@@ -788,15 +795,16 @@
                     self.quantizable_op_details[(
                         node_name, self.unify_op_type_mapping[node_op]
                     )] = [matmul_int8_config, fp32_common_config]
                 else:
                     self.quantizable_op_details[(
                         node_name, self.unify_op_type_mapping[node_op]
                     )] = [copy.deepcopy(other_config), fp32_common_config]
-                if ('bf16' in valid_precision and CpuInfo().bf16) or os.getenv('FORCE_BF16') == '1':
+                if node_op in bf16_type and (('bf16' in valid_precision and CpuInfo().bf16) \
+                         or os.getenv('FORCE_BF16') == '1'):
                     self.quantizable_op_details[(
                         node_name, self.unify_op_type_mapping[node_op]
                     )].insert(1, bf16_common_config)
 
                 self.quantize_config['op_wise_config'][node_name] = (False, "minmax", False)
         return self.quantizable_op_details
 
@@ -1022,15 +1030,15 @@
                 g.add_node(max_node, [], [current_node.name])
                 g.replace_constant_graph_with_constant_node(qint8_const_node, tensor_name)
                 g.replace_constant_graph_with_constant_node(min_node, current_node.input[5])
                 g.replace_constant_graph_with_constant_node(max_node, current_node.input[6])
 
     def inspect_weight_and_bias(self, node_list, graph_def, graph_info, graph_node_name_mapping):
         """Inspect the weights and biases."""
-        from neural_compressor.utils.utility import DequantizeWeight
+        from neural_compressor.utils.utility import dequantize_weight
         from neural_compressor.adaptor.tf_utils.util import get_tensor_val_from_graph_node
         from .tf_utils.util import int8_node_name_reverse
         import tensorflow as tf
         weights_result = {}
         inspect_nodes = []
         node_set = set(node_list)
         for node in graph_def.node:
@@ -1057,32 +1065,16 @@
                 max_filter_node = weight_node_name_pre + '_max'
                 if graph_info[min_filter_node].node.attr['value'].tensor.float_val:
                     min_filter_val = graph_info[min_filter_node].node.attr['value'].tensor.float_val
                     max_filter_val = graph_info[max_filter_node].node.attr['value'].tensor.float_val
                 else:
                     min_filter_val = get_tensor_val_from_graph_node(graph_node_name_mapping, min_filter_node)
                     max_filter_val = get_tensor_val_from_graph_node(graph_node_name_mapping, max_filter_node)
-                DequantizeWeight(weight_node_val, min_filter_val, max_filter_val)
+                weight_node_val = dequantize_weight(weight_node_val, min_filter_val, max_filter_val)
             weights_result[node_name] = {weight_node_name: weight_node_val}
-            
-            # get bias from quantized model directly
-            if 'Quantized' in node.op:
-                if 'Bias' in node.op:
-                    bias_node_name = node.input[2]
-                    bias_val = get_tensor_val_from_graph_node(graph_node_name_mapping, bias_node_name)
-                    weights_result[node_name][bias_node_name] = bias_val.astype('float32')
-            # get bias from fp32 model
-            else:
-                bias_add_node = None
-                if graph_info[node.name].outputs:
-                    bias_add_node = graph_info[graph_info[node.name].outputs[0]].node
-                if bias_add_node and bias_add_node.op == 'BiasAdd':
-                    bias_node_name = bias_add_node.input[1]
-                    bias_node_val = get_tensor_val_from_graph_node(graph_node_name_mapping, bias_node_name)
-                    weights_result[node_name][bias_node_name] = bias_node_val
         return weights_result
 
     def fused_node_mapping(self, node_list, pattern_mapping, graph_info, graph_node_name_mapping):
         """Create the mapping between first node and last node in fused sequence.
 
         Args:
             node_list: node name list
@@ -1157,24 +1149,24 @@
             node = graph_node_name_mapping[node_name]
             if 'Quantized' in node.op and 'Dequantize' in node.op:
                 inspect_node_dict['qdq_node'].append(node.name)
             elif 'Quantized' in node.op or '_Quantized' in node.op or 'Requantize' in node.op:
                 inspect_node_dict['qreq_node'].append(node.name)
             else:
                 inspect_node_dict['f_node'].append(node_name)
-        pattern_mapping = {}  
+        pattern_mapping = {}
         node_dict = quantization_cfg['op']
         for node_name_and_type in node_dict.keys():
             node_name, _ = node_name_and_type
             if 'pattern' in node_dict[node_name_and_type]:
                 pattern_mapping[node_name] = node_dict[node_name_and_type]['pattern']
             else:
                 pattern_mapping[node_name] = {'sequence': node_name}
         if inspect_node_dict['f_node']:
-            fuse_map, fuse_map_reverse = self.fused_node_mapping(inspect_node_dict['f_node'], pattern_mapping, 
+            fuse_map, fuse_map_reverse = self.fused_node_mapping(inspect_node_dict['f_node'], pattern_mapping,
                                                                  graph_info, graph_node_name_mapping)
             inspect_node_dict['f_node'] = [fuse_map[n] for n in inspect_node_dict['f_node']]
         # build model and do inference
         model = Model(graph_def)
         activation_result = self._inspect_tensor_inference(inspect_node_dict, model, dataloader, iteration_list)
         final_result = []
         int8_postfix = '_eightbit'
@@ -1257,21 +1249,21 @@
             logger.info('Dump the tensor for quantized model.')
 
         # create the mapping between node name and node detail
         g = GraphAnalyzer()
         g.graph = model
         graph_info = g.parse_graph()
         inspect_result = {}
-        
+
         # inspect weight
         if inspect_type == 'weight' or inspect_type == 'all':
             logger.info('Start to inspect weight and bias.')
             weights_result = self.inspect_weight_and_bias(node_list, model, graph_info, graph_node_name_mapping)
             inspect_result['weight'] = weights_result
-            
+
         # inspect activation
         if inspect_type == 'activation' or inspect_type == 'all':
             logger.info('Start to inspect activation.')
             activation_result = self.inspect_activation(node_list, model, graph_node_name_mapping, quantization_cfg,
                                                         dataloader, iteration_list, graph_info)
             inspect_result['activation'] = activation_result
 
@@ -1552,26 +1544,26 @@
 
                     opname = next_opnames[0]
 
         output_op_names = list(output_op_names)
         logger.debug(f"output op names: {output_op_names}")
         return output_op_names
 
-    def calculate_op_sensitivity(self, model, dataloader, tune_cfg, output_op_names, 
+    def calculate_op_sensitivity(self, model, dataloader, tune_cfg, output_op_names,
                                  confidence_batches, fallback=True, requantize_cfgs=None):
         """Compute the op sensitivity.
-        
-        The sensitivity metric is the mse between the output of the last quantized op of 
+
+        The sensitivity metric is the mse between the output of the last quantized op of
         the quantized model and the output of its corresponding op in the fp32 model.
-        
+
           1. Backup the tune cfg
           2. Fallback each int8 op and compute its mse if use fallback (with 'fallback == True'),
             or re-quantize each fp32 op(fallen back in the previous stage) and compute its MSE if not.
           3. Sorted op name list according to its MSE
-        
+
         Args:
           fp32_model: The fp32 model.
           dataloader: the dataloader with full dataset.
           tune_cfg: tuning config
           fallback: denote fallback stage or re-quantize stage
           requantize_cfgs: the dict of tuning configs for all re-quantizable ops
 
@@ -1584,43 +1576,43 @@
                        'weight': {'dtype': 'fp32'}}
 
         if fallback:
             ops_list = [op for op, config in tune_cfg['op'].items()
                        if config['activation']['quant_mode'] in ('static', 'dynamic')]
             replace_cfgs = {op : fp32_op_cfg for op in tune_cfg['op']}
         else:
-            ops_list = [op for op, config in tune_cfg['op'].items() 
+            ops_list = [op for op, config in tune_cfg['op'].items()
                        if config['activation']['quant_mode'] == 'fp32' and op in requantize_cfgs]
             replace_cfgs = requantize_cfgs
 
         # Step2. compute mse
         mse_result = self._get_mse_order(
-            model, deepcopy(tune_cfg), replace_cfgs, ops_list, dataloader, 
+            model, deepcopy(tune_cfg), replace_cfgs, ops_list, dataloader,
             output_op_names, confidence_batches)
 
         # Step3. sort
         mse_order = [op for op, _ in sorted(mse_result.items(), key=lambda i: i[1])]
         logger.debug("Dump MSE order:")
         for op in mse_order:
             logger.debug(f"{op}: {mse_result[op]}")
         return mse_order
 
-    def _get_mse_order(self, fp32_model, tune_cfg, replace_cfgs, ops_lst, dataloader, 
+    def _get_mse_order(self, fp32_model, tune_cfg, replace_cfgs, ops_lst, dataloader,
                        output_op_names, confidence_batches):
         """Compute MSE."""
         op_cfg = tune_cfg['op']
         mse_result = {}
         partial_dataloader = self._partial_dataloader(dataloader, confidence_batches)
-        
+
         fp32_output = self._inference_model_on_batches(
             fp32_model, tune_cfg, partial_dataloader, output_op_names)
 
         for op in ops_lst:
             # backup and set replace tuning config
-            backup_cfg = op_cfg[op] 
+            backup_cfg = op_cfg[op]
             op_cfg[op] = replace_cfgs[op]
 
             # quantize and inference the model
             q_model = self.quantize(tune_cfg, fp32_model, partial_dataloader)
             q_output = self._inference_model_on_batches(
                 q_model, tune_cfg, partial_dataloader, output_op_names)
 
@@ -1674,27 +1666,75 @@
         for op in output_op_names:
             for tensor in model.graph.get_operation_by_name(op).outputs:
                 output_tensors.append(tensor)
 
         predictions = []
         for index, (inputs, _) in enumerate(dataloader):
             feed_dict = generate_feed_dict(input_tensors, inputs)
-            
+
             pred = model.sess.run(output_tensors, feed_dict)
             for item in pred:
                 predictions.append(item)
 
         return predictions
 
+    def smooth_quant(self, model, dataloader, calib_iter=1, tune_cfg=None, alpha=0.5, folding=False,
+                     percentile=99.999, op_types=['MatMul', 'Conv2D'], scales_per_op=True):
+        """Convert the model by smooth quant.
+
+        Args:
+            model: original model
+            dataloader: the calibration dataloader
+            calib_iter: how many steps of iterations on the dataloader to move forward
+            tune_cfg: quantization config
+            alpha: smooth alpha in SmoothQuant, 1.0 will fallback to SPIQ
+            folding: whether insert mul(False) or just allow foldable layers(True) for SmoothQuant
+            percentile: percentile of calibration to remove outliers
+            op_types: The op types whose input tensor will be dumped
+            scales_per_op: True, each op will have an individual scale, mainly for accuracy
+                           False, ops with the same input will share a scale, mainly for performance
+
+        Returns:
+            model: A smoothed Tensorflow model
+        """
+        logger.info("Start Smoothing process for Smooth Quantization.")
+        if self.smooth_quant_model is not None:
+            return self.smooth_quant_model
+
+        # Get the nodes list which can't be quantized from tune_cfg
+        black_nodes = []
+        if tune_cfg is not None:
+            self._tuning_cfg_to_fw(tune_cfg)
+            black_nodes = [node for node in self.quantize_config if self.quantize_config[node] == 'fp32']
+
+        # Run calibration to get max values per channel
+        from .tf_utils.smooth_quant_calibration import SmoothQuantCalibration
+        calibration = SmoothQuantCalibration(model, dataloader, calib_iter, op_types, percentile, black_nodes)
+        max_vals_per_channel, sq_weight_node_names = calibration()
+
+        # Get weight tensors and weight nodes based on the input tensor
+        from neural_compressor.adaptor.tf_utils.util import get_weight_from_input_tensor
+        sq_weight_tensors, sq_weights_nodes = get_weight_from_input_tensor(
+            model, max_vals_per_channel.keys(), op_types)
+
+        # Calculate the smooth quant scaler and insert Mul op into the graph
+        from .tf_utils.smooth_quant_scaler import SmoothQuantScaler
+        scaler = SmoothQuantScaler(model, dataloader, alpha, scales_per_op)
+        model, mul_list = scaler.transform(max_vals_per_channel, sq_weight_tensors,
+                                           sq_weights_nodes, sq_weight_node_names)
+        self.smooth_quant_mul_ops.extend(mul_list)
+        self.smooth_quant_model = model
+        return self.smooth_quant_model
+
 @adaptor_registry
 class Tensorflow_ITEXAdaptor(TensorFlowAdaptor):
     """Tensorflow ITEX Adaptor Class."""
 
     def __init__(self, framework_specific_info):
-        """Initilization.
+        """Initialization.
 
         Args:
             framework_specific_info: framework specific information.
         """
         super().__init__(framework_specific_info)
 
     @dump_elapsed_time("Pass quantize model")
@@ -1790,15 +1830,15 @@
         self._dump_model_op_stats(converted_model.graph_def)
 
         return converted_model
 
 class TensorflowQuery(QueryBackendCapability):
     """Tensorflow Query Capability Class."""
     def __init__(self, local_config_file=None, performance_only=False, itex_mode=False, quant_mode='static'):
-        """Initilization.
+        """Initialization.
 
         Args:
             local_config_file: local configuration file name.
             performance_only: oob performance only mode.
             itex_mode: check if itex mode.
             quant_mode: quantization mode, static or dynamic.
         """
@@ -1843,15 +1883,15 @@
                 assert config == None, "Only one default config " \
                        "is allowed in framework yaml file."
                 config = sub_data
 
             if self.version in sub_data['version']['name']:
                 return sub_data
             else:
-                if sub_data['version']['name'] == ['2.11.0202242', '2.11.0202250']:
+                if sub_data['version']['name'] == ['2.11.0202242', '2.11.0202250', '2.11.0202317']:
                     continue
                 sorted_list = copy.deepcopy(sub_data['version']['name'])
                 sorted_list.remove('default') if 'default' in sorted_list else None
                 if isinstance(sorted_list, list):
                     # TensorFlow 1.15.0-up1/up2/up3 release versions are abnoraml release naming
                     # convention. Replacing them with dot for version comparision.
                     sorted_list = [i.replace('-up', '.') for i in sorted_list]
@@ -2224,15 +2264,15 @@
                         'AvgPool', 'DepthwiseConv2dNative']
             return ['Conv2D', 'MatMul', 'ConcatV2', 'MaxPool', 'AvgPool']
         if precision == 'bf16':
             if tf.version.VERSION in spr_base_verions:
                 return self.cur_config[precision]
             if version1_gte_version2(tf.version.VERSION, '2.1.0') or \
                version1_eq_version2(tf.version.VERSION, '1.15.0-up3'):
-                return ['Conv2D']
+                return self.cur_config[precision]
             return []
 
     def get_mixed_precision_combination(self):
         """Get the valid mixed precisions.
 
         Returns:
             [string list]: valid precision list.
@@ -2241,15 +2281,15 @@
         if version1_gte_version2(tf.version.VERSION, '2.1.0') or \
            version1_eq_version2(tf.version.VERSION, '1.15.0-up3'):
             return ['int8', 'uint8', 'bf16', 'fp32']
         return ['uint8', 'fp32']
 
     def get_bf16_patterns(self):
         """Get BF16 pattern list.
-        
+
         Returns:
             [List]: bf16 pattern list.
         """
         bf16_op_types = [i for i in self.get_op_types_by_precision('bf16')]
         res = []
         for i in bf16_op_types:
             res.append([[i]])
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tensorflow.yaml` & `neural_compressor-2.2/neural_compressor/adaptor/tensorflow.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 ---
 -
   version:
-    name: ['2.11.0202242', '2.11.0202250']
+    name: ['2.11.0202242', '2.11.0202250', '2.11.0202317']
 
   bf16: ["_MklLayerNorm", "Conv2D", "Conv2DBackpropFilter", "Conv2DBackpropInput", "Conv3D", "Conv3DBackpropFilterV2", "Conv3DBackpropInputV2",
           "DepthwiseConv2dNative", "DepthwiseConv2dNativeBackpropFilter", "DepthwiseConv2dNativeBackpropInput", "GRUBlockCell",
           "AUGRUBlockCell", "MklGRU", "MklAUGRU", "MatMul", "BatchMatMul", "BatchMatMulV2", "_MklFusedBatchMatMulV2", "Einsum", # allow_list
           "Add", "AddN", "AddV2", "AvgPool", "AvgPool3D", "AvgPool3DGrad", "AvgPoolGrad", "BiasAdd", "BiasAddGrad", "BiasAddV1",
           "Erf", "FusedBatchNormV2", "FusedBatchNormGradV2", "FusedBatchNormV3", "FusedBatchNormGradV3", "LeakyRelu", "LeakyReluGrad",
           "Mean", "Mul", "Sub", "Elu", "EluGrad", "FloorDiv", "_FusedBatchNormEx", "Log", "Log1p", "LogSoftmax", "Prod", "RealDiv",
@@ -149,15 +149,15 @@
   }
 
 
 -
   version:
     name: ['2.1.0', '2.2.0', '2.3.0', '2.4.0', '2.5.0', '2.6.0', '2.6.1', '2.6.2', '2.7.0', '2.8.0', '2.9.0', '2.9.1', '2.10.0', '2.11.0', '1.15.0-up1', '1.15.0-up2', 1.15.0-up3]
 
-  bf16: ['Conv2D']
+  bf16: ['Conv2D', 'Conv3D', 'MatMul', 'BatchMatMul', 'MaxPool', 'MaxPool3D', 'AvgPool', 'AvgPool3D', 'DepthwiseConv2dNative']
   fp32: ['*'] # '*' means all op types
 
   int8: {
     'static': {
       'Conv2D': {
         'weight': {
                     'dtype': ['int8'],
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tensorflow_itex.yaml` & `neural_compressor-2.2/neural_compressor/adaptor/tensorflow_itex.yaml`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_converter.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_converter.py`

 * *Files 2% similar despite different names*

```diff
@@ -122,15 +122,20 @@
         self.quantized_node_info = []
         self._calibration_data = []
         self._fp32_print_data = []
         self.data_loader = data_loader
         self._check_tf_version()
         self._check_args()
 
-        self._fp32_model = Model(self.model._model, **self.model.kwargs)
+        if "backend" in self.model.kwargs:
+            self._fp32_model = Model(self.model._model, **self.model.kwargs)
+        else:
+            self._fp32_model = Model(self.model._model,
+                                     **self.model.kwargs,
+                                     backend="itex" if itex_mode else "default")
         self._fp32_model.graph_def = self.model.graph_def
         self._fp32_model.output_tensor_names = self.output_tensor_names
         self._fp32_model.input_tensor_names = self.input_tensor_names
 
         self._gen_tmp_filenames()
         self._kl_op_dict = {}
         self._kl_keys = []
@@ -141,24 +146,29 @@
         self.scale_info = {}
         self.scale_info.update(qt_config)
         self.scale_info.update({'recipes': self.recipes})
         self.scale_info.update({'int8_sequences': self.int8_sequences})
         self.scale_info.update({'bf16_ops': self.bf16_ops})
         self.scale_info.update({'fp32_ops': self.fp32_ops})
 
-        self._sampling_model = Model(self.model._model, **self.model.kwargs)
+        if "backend" in self.model.kwargs:
+            self._sampling_model = Model(self.model._model, **self.model.kwargs)
+        else:
+            self._sampling_model = Model(self.model._model,
+                                         **self.model.kwargs,
+                                         backend="itex" if itex_mode else "default")
         self._sampling_model.output_tensor_names = self.output_tensor_names
         self._sampling_model.input_tensor_names = self.input_tensor_names
 
         if self.performance_only:
             # reuse the fp32 model for performance only mode
             self._tmp_graph_def = self.model.graph_def
         else:
             self._tmp_graph_def = copy.deepcopy(self.model.graph_def)
-        self.new_api = new_api #bool(version1_gte_version2(tf.version.VERSION, '2.8.0'))
+        self.new_api = new_api  # bool(version1_gte_version2(tf.version.VERSION, '2.8.0'))
         self.use_bf16 = use_bf16
         self.exclude_node_names = []
 
     # pylint: disable=no-member
     def _inference(self, model):
         """Run the calibration on the input graph.
 
@@ -227,15 +237,15 @@
                             if tensor_dim is not None and tensor_dim != data_dim:
                                 return False
                         return True
 
                     disorder_tensors = []
                     disorder_inputs = []
                     for idx, sort_tensor in enumerate(input_tensor):
-                        sort_input = inputs[idx] 
+                        sort_input = inputs[idx]
                         if check_shape(sort_tensor, sort_input):
                             feed_dict.update({sort_tensor: sort_input})
                         else:
                             disorder_tensors.append(sort_tensor)
                             disorder_inputs.append(sort_input)
                     for i, dis_tensor in enumerate(disorder_tensors):
                         for j, dis_input in enumerate(disorder_inputs):
@@ -322,15 +332,20 @@
 
         self.output_graph = os.path.join(self._output_path, 'int8_final_fused_graph')
         if self.performance_only:
             # reuse the fp32 model for performance only mode
             self._tmp_model = self._fp32_model
         else:
             # to keep temp model
-            self._tmp_model = Model(self.model._model, **self.model.kwargs)
+            if "backend" in self.model.kwargs:
+                self._tmp_model = Model(self.model._model, **self.model.kwargs)
+            else:
+                self._tmp_model = Model(self.model._model,
+                                        **self.model.kwargs,
+                                        backend="itex" if self.itex_mode else "default")
             self._tmp_model.graph_def = self.model.graph_def
             self._tmp_model.output_tensor_names = self.output_tensor_names
             self._tmp_model.input_tensor_names = self.input_tensor_names
 
     def convert(self):
         """Do convertion.
 
@@ -479,27 +494,27 @@
 
         :return:
         """
         try:
             self._quantize_graph()
             self.quantized_node_info = [tuple(i) for i in self.quantized_node_info]
 
-            if self.fake_quant: # pragma: no cover
+            if self.fake_quant:  # pragma: no cover
                 self._fuse_requantize_with_fused_quantized_node()
             else:
                 if self._enable_kl_op_names:
                     self._get_fp32_print_node_names(self._enable_kl_op_names)
                     self._generate_calibration_data(self._fp32_logged_model_path,
                                                     self._fp32_print_data,
                                                     True)
 
                 output_tensor_names = copy.deepcopy(self.model.output_tensor_names)
                 sampling_graph_def = copy.deepcopy(self._fp32_model.graph_def)
 
-                # TODO: this is a workaround to make Min/Max node be completly eliminated in int8 graph 
+                # TODO: this is a workaround to make Min/Max node be completly eliminated in int8 graph
                 # after enabling pad+conv2d in new API.
                 non_pad_ops = list(list(set(self.fp32_ops).union(set(self.bf16_ops))))
                 sampling_graph_def = FusePadWithFP32Conv2DOptimizer(
                     sampling_graph_def,
                     non_pad_ops,
                     self._tmp_model.input_node_names,
                     self.op_wise_config,
@@ -598,15 +613,18 @@
 
     def _generate_calibration_data(self, tmp_path, output_data, enable_kl_algo=False):
         """Generate the calibration data."""
         tmp_dump_file = os.path.join(os.path.dirname(self.output_graph), 'requant_min_max.log')
 
         logger.debug("Generate calibration data and save to {}.".format(tmp_dump_file))
 
-        model = Model(tmp_path, **self._tmp_model.kwargs)
+        if "backend" in self._tmp_model.kwargs:
+            model = Model(tmp_path, **self._tmp_model.kwargs)
+        else:
+            model = Model(tmp_path, **self._tmp_model.kwargs, backend="itex" if self.itex_mode else "default")
         model.output_tensor_names = self.output_tensor_names
         model.input_tensor_names = self.input_tensor_names
 
         with CaptureOutputToFile(tmp_dump_file):
             self._inference(model)
 
         with open(tmp_dump_file, errors='ignore') as f:
@@ -664,24 +682,24 @@
             self._tmp_graph_def,
             self.device, self.new_api).do_transformation()
 
         if not self.fake_quant:
             if self.qdq_enabled:
                 self._tmp_graph_def = FuseMatMulRequantizeNewAPITransformer(
                     self._tmp_graph_def).do_transformation()
-            
+
                 self._tmp_graph_def = FuseMatMulRequantizeDequantizeNewAPITransformer(
                     self._tmp_graph_def).do_transformation()
             else:
                 self._tmp_graph_def = FuseMatMulRequantizeTransformer(
                             self._tmp_graph_def).do_transformation()
 
                 self._tmp_graph_def = FuseMatMulRequantizeDequantizeTransformer(
                             self._tmp_graph_def).do_transformation()
-        
+
         self._tmp_graph_def = StripUnusedNodesOptimizer(
             self._tmp_graph_def,
             self._tmp_model.input_node_names,
             self._tmp_model.output_node_names).do_transformation()
 
         input_output_names = self._tmp_model.input_node_names + self._tmp_model.output_node_names
         self._tmp_graph_def = RemoveTrainingNodesOptimizer(
@@ -747,15 +765,15 @@
         finally:
             if not debug:
                 self._post_clean()
         return self._tmp_model
 
     def _insert_qdq_pairs(self):
         """Insert QDQ pairs before Conv/MatMul/Pooling Ops."""
-        # Fuse Pad into Conv2D, Conv3D, DepthwiseConv2dNative 
+        # Fuse Pad into Conv2D, Conv3D, DepthwiseConv2dNative
         non_pad_ops = list(list(set(self.fp32_ops).union(set(self.bf16_ops))))
         self._tmp_graph_def = FusePadWithConv2DOptimizer(
                     self._tmp_graph_def,
                     non_pad_ops,
                     self._tmp_model.input_node_names,
                     self.op_wise_config,
                     self.new_api,
@@ -824,15 +842,15 @@
         del self._sampling_model
         import gc
         gc.collect()
 
         # Insert QDQ pattern
         self._tmp_graph_def = GenerateGraphWithQDQPattern(
               self._tmp_graph_def, self._calibration_data, self.op_wise_config,
-              self.fake_quant, self.fp32_ops, self.bf16_ops, self.quantized_node_info, 
+              self.fake_quant, self.fp32_ops, self.bf16_ops, self.quantized_node_info,
               self.device, self.performance_only, self.itex_mode).do_transformation()
 
     def _convert_qdq(self):
         """Convert Dequantize + Op + QuantizeV2 into QuantizedOps."""
         if self.itex_mode:
             self._tmp_graph_def, quantizev2_max = FreezeValueTransformer(
                 self._tmp_graph_def,
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_converter_without_calib.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_converter_without_calib.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/bf16_convert.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/bf16_convert.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,14 +30,16 @@
 from tensorflow.python.framework.kernels import get_registered_kernels_for_op
 
 from ..graph_base import GraphRewriterBase
 from neural_compressor.adaptor.tf_utils.graph_util import GraphAnalyzer
 from neural_compressor.adaptor.tf_utils.graph_util import GraphRewriterHelper as Helper
 from ..generic.graph_cse_optimizer import GraphCseOptimizer
 from ..generic.dequantize_cast_optimizer import DequantizeCastOptimizer
+import tensorflow as tf
+from neural_compressor.adaptor.tf_utils.util import TF_SPR_BASE_VERSIONS
 
 DT_FLOAT32  = attr_value_pb2.AttrValue(type=dtypes.float32.as_datatype_enum)
 DT_BFLOAT16 = attr_value_pb2.AttrValue(type=dtypes.bfloat16.as_datatype_enum)
 
 class BF16Convert(GraphRewriterBase):
     """BF16 node convert transformation."""
 
@@ -175,15 +177,16 @@
             elif input_node.op == "Const" and len(input_node_outputs) == 1:
                 fp32_value = tensor_util.MakeNdarray(input_node.attr.get('value').tensor)
                 Helper.set_attr_dtype(input_node, "dtype", dtypes.bfloat16)
                 input_node.attr['value'].CopyFrom(attr_value_pb2.AttrValue(
                     tensor=tensor_util.make_tensor_proto(
                         fp32_value, dtypes.bfloat16, fp32_value.shape)))
             elif 'Dequantize' == input_node.op and len(input_node_outputs) == 1 \
-                                                        and input_node.attr['mode'].s != b'MIN_FIRST':
+                                and input_node.attr['mode'].s != b'MIN_FIRST' \
+                                and tf.version.VERSION in TF_SPR_BASE_VERSIONS:
                 # Dequantize with mode MIN_FIRST does not support bf16 in both eigen and mkl
                 _, outputs_dt_input_node = self._dtype(input_node)
                 allowed_input_node_dt_val = self._allowed_dtype_val(input_node)
                 if outputs_dt_input_node[0] in allowed_input_node_dt_val and \
                         dtypes.bfloat16.as_datatype_enum in allowed_input_node_dt_val[outputs_dt_input_node[0]]:
                     input_node.attr[outputs_dt_input_node[0]].CopyFrom(DT_BFLOAT16)
             # ResizeBilinear input can be of different types but output is always float
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_add_to_biasadd.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_add_to_biasadd.py`

 * *Files 1% similar despite different names*

```diff
@@ -31,15 +31,15 @@
     def do_transformation(self):
         """Execute convertion Add to BiasAdd."""
         g = GraphAnalyzer()
         g.graph = self.model
         graph_info = g.parse_graph()
 
         import tensorflow as tf
-        if tf.version.VERSION not in ('2.11.0202242', '2.11.0202250'):
+        if tf.version.VERSION not in ('2.11.0202242', '2.11.0202250', '2.11.0202317'):
             target_nodes = g.query_fusion_pattern_nodes([['MatMul', 'Conv2D'], ['Add', 'AddV2']])
         else:
             target_nodes = g.query_fusion_pattern_nodes([['MatMul'], ['Add', 'AddV2']])
         for i in target_nodes:
             successor_node_names = graph_info[i[1]].outputs
             matmul_input_name = graph_info[i[0]].node.input[0]
             matmul_input_node = graph_info[Helper.node_name_from_input(matmul_input_name)].node
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_layout.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_layout.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_leakyrelu.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_leakyrelu.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_nan_to_random.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_nan_to_random.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_placeholder_to_const.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/convert_placeholder_to_const.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dequantize_cast_optimizer.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dequantize_cast_optimizer.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,29 +17,36 @@
 """Dequantize Cast Graph Rerewriter."""
 
 from tensorflow.core.framework import attr_value_pb2
 from tensorflow.python.framework import dtypes
 
 from ..graph_base import GraphRewriterBase
 from neural_compressor.adaptor.tf_utils.graph_util import GraphAnalyzer
-from neural_compressor.adaptor.tf_utils.graph_util import GraphRewriterHelper as Helper
 from neural_compressor.utils.utility import dump_elapsed_time
+import tensorflow as tf
+from neural_compressor.adaptor.tf_utils.util import TF_SPR_BASE_VERSIONS
+
 class DequantizeCastOptimizer(GraphRewriterBase):
     """Remove the Cast OP and set Dequantize output to B16 if the Cast OP output is BF16."""
 
     @dump_elapsed_time("Pass DequantizeCastOptimizer")
     def do_transformation(self):
         """Remove the redundant Cast OP after Dequantize.
 
         Args:
             input_graph_def (graphdef): graphdef object
 
         Returns:
             [graphdef]: optimized graph
         """
+        # stock TF _MklDequantize doesn't support BF16 currently.
+        # TODO remove this when spr-base upstream to stock TF.
+        if not tf.version.VERSION in TF_SPR_BASE_VERSIONS:
+            return self.model
+
         DT_BFLOAT16 = attr_value_pb2.AttrValue(type=dtypes.bfloat16.as_datatype_enum)
         cur_graph = GraphAnalyzer()
         cur_graph.graph = self.model
         graph_info = cur_graph.parse_graph()
         target_nodes = cur_graph.query_fusion_pattern_nodes([["Dequantize"], ["Cast"]])
         for i in target_nodes:
             dq_node = graph_info[i[0]].node
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dilated_contraction.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dilated_contraction.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dummy_biasadd.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/dummy_biasadd.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/expanddims_optimizer.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/expanddims_optimizer.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fetch_weight_from_reshape.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fetch_weight_from_reshape.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fold_batch_norm.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fold_batch_norm.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fold_constant.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fold_constant.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_biasadd_add.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_biasadd_add.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_column_wise_mul.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_column_wise_mul.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_conv_with_math.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_conv_with_math.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_decomposed_bn.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_decomposed_bn.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_decomposed_in.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_decomposed_in.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_gelu.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_gelu.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_layer_norm.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_layer_norm.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_pad_with_conv.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_pad_with_conv.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_pad_with_fp32_conv.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_pad_with_fp32_conv.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_reshape_transpose.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/fuse_reshape_transpose.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/graph_cse_optimizer.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/graph_cse_optimizer.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/grappler_pass.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/grappler_pass.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/insert_print_node.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/insert_print_node.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/move_squeeze_after_relu.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/move_squeeze_after_relu.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/pre_optimize.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/pre_optimize.py`

 * *Files 15% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Pre Optimization Entrance."""
 
 import logging
+import copy
 import tensorflow as tf
 from neural_compressor.adaptor.tf_utils.graph_util import GraphAnalyzer
 from neural_compressor.utils.utility import dump_elapsed_time
 from .fuse_column_wise_mul import FuseColumnWiseMulOptimizer
 from .remove_training_nodes import RemoveTrainingNodesOptimizer
 from .split_shared_input import SplitSharedInputOptimizer
 from .strip_unused_nodes import StripUnusedNodesOptimizer
@@ -45,14 +46,15 @@
 from .fuse_decomposed_bn import FuseDecomposedBNOptimizer
 from .fuse_decomposed_in import FuseDecomposedINOptimizer
 from .fuse_layer_norm import FuseLayerNormOptimizer
 from .strip_equivalent_nodes import StripEquivalentNodesOptimizer
 from .dilated_contraction import DilatedContraction
 from .convert_placeholder_to_const import ConvertPlaceholderToConst
 from neural_compressor.adaptor.tf_utils.util import version1_gte_version2, version1_eq_version2
+from neural_compressor.adaptor.tf_utils.util import version1_lt_version2
 
 class PreOptimization():
     """Pre optimization for the FP32 models."""
 
     def __init__(self, model, new_api, device):
         """Initilization."""
         self.model = model
@@ -102,15 +104,15 @@
         7. Fold the BN node into the previous Conv2D if possible.
 
         Returns:
             [graphdef]: the optimized graphdef object.
         """
         from neural_compressor.model import Model
 
-        origin_model = Model(self.model._model, **self.model.kwargs)
+        origin_model = Model(self.model._model, **self.model.kwargs, backend="itex" if itex_mode else "default")
         origin_model.name = self.model.name
         origin_model.model_type = self.model.model_type
         origin_model.output_tensor_names = self.model.output_tensor_names
         origin_model.input_tensor_names = self.model.input_tensor_names
         origin_model.workspace_path = self.model.workspace_path
 
         output_node_names = self.model.output_node_names
@@ -127,39 +129,33 @@
 
             if self.device == 'cpu':
                 cpus = tf.config.list_physical_devices("CPU")
                 node_device = cpus[0].name.replace('physical_device:', '')
             else:
                 gpus = tf.config.list_physical_devices("GPU")
                 if len(gpus) == 0:
-                    cpus = tf.config.list_physical_devices("CPU")
-                    node_device = cpus[0].name.replace('physical_device:', '')
+                    xpus = tf.config.list_physical_devices("XPU")
+                    if len(xpus) == 0:
+                        cpus = tf.config.list_physical_devices("CPU")
+                        node_device = cpus[0].name.replace('physical_device:', '')
+                    else:
+                        node_device = xpus[0].name.replace('physical_device:', '')
                 else:
                     node_device = gpus[0].name.replace('physical_device:', '')
             for node_name in list(graph_info.keys()):
                 node = graph_info[node_name].node
                 node.device = node_device
             self._tmp_graph_def = cur_graph.dump_graph()
 
             self._tmp_graph_def = ConvertLayoutOptimizer(
                 self._tmp_graph_def, output_node_names).do_transformation()
         else:
             self._tmp_graph_def = ConvertLayoutOptimizer(
                 self.model.graph_def, output_node_names).do_transformation()
 
-        # Remove device info after convert layout
-        if version1_gte_version2(tf.version.VERSION, '2.10.0'):
-            cur_graph = GraphAnalyzer()
-            cur_graph.graph = self._tmp_graph_def
-            graph_info = cur_graph.parse_graph()
-            for node_name in list(graph_info.keys()):
-                node = graph_info[node_name].node
-                node.device = ''
-            self._tmp_graph_def = cur_graph.dump_graph()
-
         self._tmp_graph_def = ConvertPlaceholderToConst(self._tmp_graph_def).do_transformation()
 
         self._tmp_graph_def = SwitchOptimizer(self._tmp_graph_def).do_transformation()
 
         self._tmp_graph_def = GrapplerOptimizer(
             self._tmp_graph_def, input_output_names, self.optimization).do_transformation()
 
@@ -235,18 +231,51 @@
 
         self._tmp_graph_def = StripEquivalentNodesOptimizer(
             self._tmp_graph_def, output_node_names).do_transformation()
 
         if self.new_api or itex_mode:
             self._tmp_graph_def = DilatedContraction(
                 self._tmp_graph_def).do_transformation()
+
+        # node device info will be removed by GrapplerOptimizer, insert it again.
+        if version1_lt_version2(tf.version.VERSION, '2.0.0'): # pragma: no cover
+            from tensorflow._api.v1.config import experimental
+            list_physical_devices = experimental.list_physical_devices
+        else:
+            list_physical_devices = tf.config.list_physical_devices
+        cur_graph = GraphAnalyzer()
+        cur_graph.graph = self._tmp_graph_def
+        graph_info = cur_graph.parse_graph()
+
+        if self.device == 'cpu':
+            cpus = list_physical_devices("CPU")
+            node_device = cpus[0].name.replace('physical_device:', '')
+        else:
+            gpus = list_physical_devices("GPU")
+            if len(gpus) == 0:
+                xpus = list_physical_devices("XPU")
+                if len(xpus) == 0:
+                    cpus = list_physical_devices("CPU")
+                    node_device = cpus[0].name.replace('physical_device:', '')
+                else:
+                    node_device = xpus[0].name.replace('physical_device:', '')
+            else:
+                node_device = gpus[0].name.replace('physical_device:', '')
+        for node_name in list(graph_info.keys()):
+            node = graph_info[node_name].node
+            node.device = node_device
+        self._tmp_graph_def = cur_graph.dump_graph()
+
         self._tmp_graph_def.library.CopyFrom(self.model.graph_def.library)
 
-        origin_model.graph_def = self._tmp_graph_def
+        for function_def in self.model.graph_def.library.function:
+            if function_def.signature.name == 'swish_f32':
+                self._tmp_graph_def.library.function.extend([copy.deepcopy(function_def)])
 
+        origin_model.graph_def = self._tmp_graph_def
         return origin_model
 
     def get_matched_nodes(self, patterns):
         """Searche the matched nodes with the specified patterns.
 
         Args:
             patterns ([string list]): The patterns should be illustrated as below.
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/remove_training_nodes.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/remove_training_nodes.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/rename_batch_norm.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/rename_batch_norm.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/split_shared_input.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/split_shared_input.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/strip_equivalent_nodes.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/strip_equivalent_nodes.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/strip_unused_nodes.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/strip_unused_nodes.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/switch_optimizer.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/generic/switch_optimizer.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/graph_base.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/graph_base.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_fake_quant.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_fake_quant.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_value.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_value.py`

 * *Files 2% similar despite different names*

```diff
@@ -239,17 +239,21 @@
             if not self.graph_info.get(bn_node_name) or \
                 not bn_node_name.endswith('_eightbit_quantized_bn'):
                 bn_node_name = None
             if not self.graph_info.get(in_node_name) or \
                 not in_node_name.endswith('_eightbit_quantized_in'):
                 in_node_name = None
 
-            if self.itex_mode and ('FusedBatchNormV3_eightbit_requant_range' in node_name or \
-                'FusedBatchNorm_eightbit_requant_range' in node_name):
+            if self.itex_mode and 'BatchNorm' in node_name:
                 bn_node_name = node_name[:-len("_eightbit_requant_range")]
+                if bn_node_name not in self.graph_info:
+                    bn_node_name = None
+                else:
+                    if 'FusedBatchNorm' not in self.graph_info[bn_node_name].node.op:
+                        bn_node_name = None
 
             if node_name not in self.graph_info \
                 and bn_node_name not in self.graph_info \
                     and in_node_name not in self.graph_info:
                 continue
 
             min_node = node_def_pb2.NodeDef()
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_value_without_calib.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/freeze_value_without_calib.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_conv_redundant_dequantize.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_conv_redundant_dequantize.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_conv_requantize.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_conv_requantize.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_matmul_redundant_dequantize.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_matmul_redundant_dequantize.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,15 +23,15 @@
 from ..graph_base import GraphRewriterBase
 from neural_compressor.adaptor.tf_utils.graph_util import GraphAnalyzer
 from neural_compressor.adaptor.tf_utils.graph_util import GraphRewriterHelper as Helper
 
 class FuseMatMulRedundantDequantizeTransformer(GraphRewriterBase):
     """Fuse _QuantizedMatMul with the successor Dequantize Op."""
     fuse_patterns = [[
-        "_QuantizedMatMul"
+        "_QuantizedMatMul", "_QuantizedBatchMatMul"
     ], ['Dequantize', 'Cast']]
 
     def __init__(self, model, device='cpu'):
         """Initilization."""
         super().__init__(model)
         self.device = device
         self.graph_analyzer = GraphAnalyzer()
@@ -118,21 +118,25 @@
             if 'fused_ops' in quantized_node.attr:
                 new_node.attr["fused_ops"].CopyFrom(quantized_node.attr['fused_ops'])
 
             # update Tbias for single MatMul withou bias case, same as Tout.
             if dequantize_node.op == "Dequantize":
                 Helper.set_attr_type_list(new_node, 'Thost_outputs', [dequantize_node.attr['dtype'].type])
                 new_node.attr["Tout"].CopyFrom(attr_value_pb2.AttrValue(type=dequantize_node.attr['dtype'].type))
+                if new_node.op == '_QuantizedBatchMatMul':
+                    new_node.attr["U"].CopyFrom(attr_value_pb2.AttrValue(type=dequantize_node.attr['DstT'].type))
                 if str(quantized_node.attr['fused_ops'].list.s) == str([b"Requantize"]):
                     new_node.attr["Tbias"].CopyFrom( \
                         attr_value_pb2.AttrValue(type=dequantize_node.attr['dtype'].type))
                 Helper.set_attr_string_list(new_node, 'fused_ops', eval(fused_ops))
             else:
                 Helper.set_attr_type_list(new_node, 'Thost_outputs', [dequantize_node.attr['DstT'].type])
                 new_node.attr["Tout"].CopyFrom(attr_value_pb2.AttrValue(type=dequantize_node.attr['DstT'].type))
+                if new_node.op == '_QuantizedBatchMatMul':
+                    new_node.attr["U"].CopyFrom(attr_value_pb2.AttrValue(type=dequantize_node.attr['DstT'].type))
 
             top_node_name = Helper.node_name_from_input(quantized_node.input[0])
             if self.graph_info[dequantize_node_name].outputs:
                 self.graph_analyzer.replace_single_node(
                     new_node, [top_node_name], quantized_node_name,
                     self.graph_info[dequantize_node_name].outputs, dequantize_node_name)
                 self.graph_analyzer.remove_node(dequantize_node_name)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_matmul_requantize.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/fuse_matmul_requantize.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/meta_op_optimizer.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/meta_op_optimizer.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/post_hostconst_converter.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/post_hostconst_converter.py`

 * *Files 8% similar despite different names*

```diff
@@ -29,13 +29,14 @@
         """Convert Const to HostConst as default."""
         if os.environ.get("DISABLE_HOSTCONST") == '1':
             return self.model
         output_graph_def = graph_pb2.GraphDef()
         for node in self.model.node:
             new_node = node_def_pb2.NodeDef()
             new_node.CopyFrom(node)
+            new_node.device = ''
             if node.op == "Const" and node.attr['dtype'].type in [1, 3] \
                 and (node.name.endswith('_min') or node.name.endswith('_max') \
                     or node.name.endswith('_max_only') or node.name.endswith('_min_only')):
                 new_node.op = "HostConst"
             output_graph_def.node.extend([new_node])
         return output_graph_def
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/post_quantized_op_cse.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/post_quantized_op_cse.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/rnn_convert.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/rnn_convert.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/scale_propagation.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/int8/scale_propagation.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_graph.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_graph.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,14 +16,15 @@
 #  limitations under the License.
 #
 """ONNX Graph wrapper for Tensorflow model converting to ONNX model."""
 
 import collections
 import logging
 import six
+import re
 import numpy as np
 
 from onnx import helper, numpy_helper, AttributeProto, TensorProto
 from . import tf2onnx_utils as utils
 from .onnx_node import OnnxNode
 
 logger = logging.getLogger("neural_compressor")
@@ -1248,14 +1249,18 @@
                 bias_tensor = np.squeeze(bias_tensor)
                 self.get_node_by_name(node.input[1]).set_tensor_value(bias_tensor)
         else:
             return []
 
         input_dequantize_node = self.get_node_by_output(conv_node.input[0])
         weight_dequantize_node = self.get_node_by_output(conv_node.input[1])
+        if re.search(r"\w+:\d+", input_dequantize_node.input[1]):
+            input_dequantize_node.input[1] = input_dequantize_node.input[1].rsplit(':', 1)[0]
+        if re.search(r"\w+:\d+", weight_dequantize_node.input[1]):
+            weight_dequantize_node.input[1] = weight_dequantize_node.input[1].rsplit(':', 1)[0]
         input_scale = self.get_node_by_name(
             input_dequantize_node.input[1]).get_tensor_value(as_list=False)
         weight_scale = self.get_node_by_name(
             weight_dequantize_node.input[1]).get_tensor_value(as_list=False)
         bias_scale_val = input_scale * weight_scale
         bias_zp_val = np.zeros(bias_scale_val.shape, dtype=np.int32).reshape(-1)
         quantized_bias = (bias_tensor / bias_scale_val).round().astype(np.int32)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_node.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_node.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_schema.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/onnx_schema.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/tf2onnx_utils.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/onnx/tf2onnx_utils.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/insert_qdq_pattern.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/insert_qdq_pattern.py`

 * *Files 2% similar despite different names*

```diff
@@ -212,21 +212,18 @@
                 input0_node = self.node_name_mapping[node.input[0].rsplit(':', 1)[0]].node
             else:
                 input0_node = self.node_name_mapping[node.input[0]].node
             if re.search(r"\w+:\d+", node.input[1]):
                 input1_node = self.node_name_mapping[node.input[1].rsplit(':', 1)[0]].node
             else:
                 input1_node = self.node_name_mapping[node.input[1]].node
-            if input0_node.op in ("AvgPool", "MaxPool"):
-                return self._find_relu_node(input0_node)
-            if input1_node.op in ("AvgPool", "MaxPool"):
-                return self._find_relu_node(input1_node)
-            if input1_node.op in ('BiasAdd', 'Add', 'AddV2', 'AddN'):
+            if input0_node.op in ('BiasAdd', 'Add', 'AddV2', 'AddN') or \
+               input1_node.op in ('BiasAdd', 'Add', 'AddV2', 'AddN'):
                 return False
-            return self._find_relu_node(input1_node)
+            return self._find_relu_node(input0_node) and self._find_relu_node(input1_node)
         elif self._check_op_list(node.op) or (self.itex_mode and node.op in ('Add', 'AddV2')):
             if node.op == 'ConcatV2':
                 find_relu = False
                 for i in range(0,node.attr['N'].i):
                     if re.search(r"\w+:\d+", node.input[i]):
                         input_node = self.node_name_mapping[node.input[i].rsplit(':', 1)[0]].node
                     else:
@@ -248,25 +245,37 @@
             all_inputs = self.node_name_mapping[original_node.name].node.input[-1:]
         else:
             all_inputs = self.node_name_mapping[original_node.name].node.input[:1]
         for each_input_name in all_inputs:
             if each_input_name[0] == '^':
                 continue
 
-            if self.node_name_mapping[original_node.name].node.op == "MatMul":
+            # if dq+maxpool is detected as input of this node
+            # the qdq in pattern dq+maxpool+q should be with the same dtype in the itex mode
+            if self.itex_mode and each_input_name in self.node_name_mapping \
+                and self.node_name_mapping[each_input_name].node.op == "MaxPool" \
+                and self.graph_info[self.graph_info[each_input_name].node.input[0]].node.op == "Dequantize":
+                maxpool_node = self.graph_info[each_input_name].node
+                dtype = dtypes.DType(self.graph_info[maxpool_node.input[0]].node.attr["T"].type)
+            elif self.node_name_mapping[original_node.name].node.op == "MatMul":
                 dtype = dtypes.quint8
             elif self.node_name_mapping[original_node.name].node.op == "BatchMatMulV2" \
                 or self.node_name_mapping[original_node.name].node.op == "BatchMatMul":
                 dtype = dtypes.qint8
+            # the qdq in pattern dq+bn+relu+q and dq+bn+q should be s8 in itex mode
+            elif self.node_name_mapping[original_node.name].node.op == "FusedBatchNormV3":
+                dtype = dtypes.qint8
             else:
                 input_node_name = Helper.node_name_from_input(each_input_name)
                 if input_node_name in self.graph_info:
                     if self.graph_info[input_node_name].node.op == "Dequantize":
                         dtype = dtypes.DType(
                             self.graph_info[input_node_name].node.attr["T"].type)
+                    elif self.graph_info[input_node_name].node.op == "FusedBatchNormV3":
+                        dtype = dtypes.qint8
                     elif self._find_relu_node(self.node_name_mapping[original_node.name].node):
                         dtype = dtypes.quint8
                     else:
                         dtype = dtypes.qint8
                 else:
                     dtype = dtypes.quint8 if self._find_relu_node(
                         self.node_name_mapping[original_node.name].node
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/merge_duplicated_qdq.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/merge_duplicated_qdq.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/share_qdq_y_pattern.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_rewriter/qdq/share_qdq_y_pattern.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/graph_util.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/graph_util.py`

 * *Files 0% similar despite different names*

```diff
@@ -421,14 +421,16 @@
         """
         new_const_node_name = new_const_node.name
 
         self.node_name_details[new_const_node_name] = self.node_details(node=new_const_node,
                                                                         outputs=target_node)
 
         for sub_node in target_node:
+            if not sub_node in self.node_name_details:
+                continue
             for index, each_node_name in enumerate(self.node_name_details[sub_node].node.input):
                 if each_node_name + ':0' == old_constant_node_name \
                         or each_node_name == old_constant_node_name:
                     new_input_name = self.node_name_details[sub_node].node.input[:index] + [
                         new_const_node_name
                     ] + self.node_name_details[sub_node].node.input[index + 1:]
                     self.node_name_details[sub_node].node.ClearField('input')
@@ -653,15 +655,15 @@
 
             each_node = self.node_details(node=node, outputs=[])
 
             if node_name not in self.node_name_details:
                 self.node_name_details[node_name] = each_node
 
         for node_name, node_details in self.node_name_details.items():
-            # update the upper node's output infomation.
+            # update the upper node's output information.
             for each_input in node_details.node.input:
                 self.node_name_details[GraphRewriterHelper.node_name_from_input(
                     each_input)].outputs.append(node_name)
 
         return self.node_name_details
 
 
@@ -958,14 +960,18 @@
 
         iterations = 0
         for i in valid_data:
             if i.startswith(first_line):
                 iterations += 1
 
         step = int(len(valid_data) / iterations)
+        if step % 2 == 1:
+            step -= 1
+            iterations = int(len(valid_data) / step) + int(len(valid_data) % step > 0)
+
         final_res = []
 
         for i in range(iterations):
             final_res.extend(gen_per_iter(valid_data[int(i*step): int(step*( i+ 1))]))
             if i + 1 == iterations and int(step*( i+ 1)) < len(valid_data):
                 final_res.extend(gen_per_iter(valid_data[int(step*( i+ 1)): len(valid_data)]))
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/fake_quantize.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/fake_quantize.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_config.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_config.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_helper.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_helper.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/optimize_layer.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/optimize_layer.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_add.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_add.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_base.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_base.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_bn.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_layers/quantize_layer_bn.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_wrapper.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qat/quantize_wrapper.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_bn.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_bn.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_concatv2.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_concatv2.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_conv.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_conv.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_deconv.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_deconv.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_in.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_in.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_matmul.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_matmul.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_pooling.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/fuse_qdq_pooling.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/optimize_qdq.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/qdq/optimize_qdq.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_base.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_base.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_bn.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_bn.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_concatv2.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_concatv2.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_conv.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_conv.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_for_intel_cpu.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_for_intel_cpu.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_matmul.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_matmul.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_pooling.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph/quantize_graph_pooling.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/quantize_graph_common.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/quantize_graph_common.py`

 * *Files 0% similar despite different names*

```diff
@@ -55,16 +55,16 @@
         """Get op list by recursive sorting the graph."""
         for output_name in output_node_names:
             self._recursive_graph_sorting(output_name)
 
     def get_sorted_graph(self, input_graph, input_node_names, output_node_names):
         """Return a sorted graphdef object.
 
-        Sometimes the input graphdef was composed of the randome nodedef objects,
-        we reorder the graph to make the parsing more easier.
+        Sometimes the input graphdef was composed of the random nodedef objects,
+        we reorder the graph to make the parsing easier.
 
         Args:
             input_graph (graphdef]): the input graphdef object
             input_node_names (string list): the input node names
             output_node_names (string list): the output node names
 
         Returns:
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/tf2onnx_converter.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/tf2onnx_converter.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/bias_correction.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/bias_correction.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/graph_transform_base.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/graph_transform_base.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/insert_logging.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/insert_logging.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/transform_graph/rerange_quantized_concat.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/transform_graph/rerange_quantized_concat.py`

 * *Files 2% similar despite different names*

```diff
@@ -282,14 +282,17 @@
 
                 min_freezed_output_node = self.node_mapping[
                     another_conv_node.input[offset_value]]
                 max_freezed_output_node = self.node_mapping[
                     another_conv_node.input[offset_value + 1]]
                 min_input = min_freezed_output_node.attr['value'].tensor.float_val[0]
                 max_input = max_freezed_output_node.attr['value'].tensor.float_val[0]
+                # To avoid generating int32 bias exception for corner case
+                if min_input == 0 and max_input == 0:
+                    continue
 
                 bias_tensor = (tensor_util.MakeNdarray(bias_node.attr['value'].tensor))
 
                 activation_range = 127.0 if current_node.attr['out_type'].type == dtypes.qint8  \
                     else 255.0
 
                 int32_bias = Helper.generate_int32_bias_for_conv(
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/tf_utils/util.py` & `neural_compressor-2.2/neural_compressor/adaptor/tf_utils/util.py`

 * *Files 5% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 from tensorflow.core.framework import node_def_pb2
 from tensorflow.core.framework import attr_value_pb2
 from neural_compressor.utils import logger
 from .graph_util import GraphAnalyzer
 from .graph_util import GraphRewriterHelper
 from pkg_resources import parse_version
 
-TF_SPR_BASE_VERSIONS = ('2.11.0202242', '2.11.0202250')
+TF_SPR_BASE_VERSIONS = ('2.11.0202242', '2.11.0202250', '2.11.0202317')
 
 def version1_lt_version2(version1, version2):
     """Check if version1 is less than version2."""
     return parse_version(version1) < parse_version(version2)
 
 def version1_gt_version2(version1, version2):
     """Check if version1 is greater than version2."""
@@ -498,33 +498,33 @@
         if 'Quantized' in node.op:
             int8_node_lst.add(node_name)
         elif node.attr['value'].tensor.dtype == tf.dtypes.bfloat16.as_datatype_enum:  # pragma: no cover
             bf16_node_lst.add(node.name)
         else:
             continue
     inspect_node_lst = fp32_node_lst.intersection(bf16_node_lst.union(int8_node_lst))
-    dequan_min_max, updated_cfg = _parse_config(quan_model.q_config, tune_cfg, inspect_node_lst)
-    dump_data_to_local(dequan_min_max, save_path, 'dequan_min_max.pkl')
+    activation_min_max, updated_cfg = _parse_config(quan_model.q_config, tune_cfg, inspect_node_lst)
+    dump_data_to_local(activation_min_max, save_path, 'activation_min_max.pkl')
     dump_data_to_local(updated_cfg, save_path, 'cfg.pkl')
 
     return inspect_node_lst, updated_cfg
 
 def _parse_config(q_config, cfg, op_list):
     """Parse q_config and get dequantize min max value."""
-    dequan_min_max = {}
+    activation_min_max = {}
     if '__requant_min_max' in q_config:
         for node_name, val in q_config['__requant_min_max'].items():
             node_name = node_name.split('_eightbit_requant_range')[0]
             if node_name in op_list:
-                dequan_min_max[node_name] = {'min': val[0], 'max': val[1]}
+                activation_min_max[node_name] = {'min': val[0], 'max': val[1]}
     updated_cfg = {'op' : {}}
     for op_name_and_type in cfg['op'].keys():
         if op_name_and_type[0] in op_list:
             updated_cfg['op'][op_name_and_type] = cfg['op'][op_name_and_type]
-    return dequan_min_max, updated_cfg
+    return activation_min_max, updated_cfg
 
 def generate_feed_dict(input_tensor, inputs):
     """Generate feed dict helper function."""
     if len(input_tensor) == 1:
         feed_dict = {}
         if isinstance(inputs, dict) or isinstance(inputs, OrderedDict) \
             or isinstance(inputs, UserDict):
@@ -576,8 +576,45 @@
                     disorder_tensors.append(sort_tensor)
                     disorder_inputs.append(sort_input)
             for i, dis_tensor in enumerate(disorder_tensors):
                 for j, dis_input in enumerate(disorder_inputs):  
                     if check_shape(dis_tensor, dis_input):
                         feed_dict.update({dis_tensor: dis_input})    
                         break
-    return feed_dict
+    return feed_dict
+
+def get_weight_from_input_tensor(model, input_tensor_names, op_types):
+    """Extracts weight tensors and their associated nodes from a smooth quant node's input tensor.
+
+    Args:
+        model: A TensorFlow model containing a `graph_def` attribute.
+        input_tensor_names: A list of input tensor names to search for weight tensors.
+        op_types: A list of operation types to search for when looking for weight tensors.
+
+    Returns:
+        A tuple of two dictionaries:
+        - sq_weight_tensors: A dictionary mapping each input tensor name to a list of its associated weight tensors.
+        - sq_weights_nodes: A dictionary mapping each input tensor name to a list of its associated weight nodes.
+    """
+    graph_info = GraphAnalyzer(model.graph_def).parse_graph()
+
+    sq_weight_tensors = {}
+    sq_weights_nodes = {}
+
+    from tensorflow.python.framework import tensor_util
+    for name in input_tensor_names:
+        curr_weight_tensors = []
+        curr_weights_nodes = []
+        next_node_names = graph_info[name].outputs
+        for node_name in next_node_names:
+            curr_node = graph_info[node_name].node
+            if curr_node.op not in op_types:
+                continue
+            if len(curr_node.input) >= 2:
+                weight_name = curr_node.input[1]
+                weight_node = graph_info[weight_name].node
+                weight_tensor = tensor_util.MakeNdarray(weight_node.attr["value"].tensor)
+                curr_weight_tensors.append(weight_tensor)
+                curr_weights_nodes.append(weight_node)
+        sq_weight_tensors[name] = curr_weight_tensors
+        sq_weights_nodes[name] = curr_weights_nodes
+    return sq_weight_tensors, sq_weights_nodes
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/__init__.py` & `neural_compressor-2.2/neural_compressor/adaptor/torch_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/bf16_convert.py` & `neural_compressor-2.2/neural_compressor/adaptor/torch_utils/bf16_convert.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/hawq_metric.py` & `neural_compressor-2.2/neural_compressor/adaptor/torch_utils/hawq_metric.py`

 * *Files 1% similar despite different names*

```diff
@@ -48,17 +48,17 @@
         """Remove handle."""
         self.handle.remove()
 
 
 class HessianTrace:
     """HessianTrace Class.
 
-    Please refer to Yao, Zhewei, et al. "Pyhessian: Neural networks through the lens of the hessian." 
+    Please refer to Yao, Zhewei, et al. "Pyhessian: Neural networks through the lens of the hessian."
     2020 IEEE international conference on big data (Big data). IEEE, 2020.
-    Dong, Zhen, et al. "Hawq-v2: Hessian aware trace-weighted quantization of neural networks." 
+    Dong, Zhen, et al. "Hawq-v2: Hessian aware trace-weighted quantization of neural networks."
     Advances in neural information processing systems 33 (2020): 18518-18529.
     https://github.com/openvinotoolkit/nncf/blob/develop/nncf/torch/quantization/hessian_trace.py
     """
 
     def __init__(self, model, dataloader, q_model, criterion=None):
         """Init a HessianTrace object."""
         self.unfused_model = model.model
@@ -169,15 +169,15 @@
                 self.layer_acts_grads[name] = grad_input[0]
 
         return act_grad_hook
 
     def _get_enable_act_grad_hook(self, name):
         def enable_act_grad_hook(model, inputs, outputs):
             input = inputs[0]
-            if input.requires_grad is False:
+            if input.requires_grad is False: #
                 input.requires_grad = True
             self.layer_acts[name] = input
 
         return enable_act_grad_hook
 
     # def _get_disable_input_grad_hook(self, name):
     #     def disable_input_grad_hook(model, inputs, outputs):
@@ -247,21 +247,21 @@
     def _sample_rademacher(self, params):
         samples = []
         for param in params:
             r = torch.randint_like(param, high=2, device=self.device)
             r.masked_fill_(r == 0, -1)
             samples.append(r)
         return samples
-    
+
     def _sample_rademacher_like_params(self):
         def sample(parameter):
             r = torch.randint_like(parameter, high=2, device=self.device)
             return r.masked_fill_(r == 0, -1)
         return [sample(p) for p in self.params]
-    
+
     def _sample_normal_like_params(self):
         return [torch.randn(p.size(), device=self.device) for p in self.params]
 
     def get_vtHv_weight(self, params, num_samples):
         """Get vtHv weight."""
         v = self._sample_rademacher(params)
         H_v=self._sample_normal_like_params()
@@ -387,15 +387,15 @@
         return res_dict
 
     def _insert_hook(self, model, target_module_list):
         intern_outputs = []
         for layer, module in model.named_modules():
             for target_module in target_module_list:
                 # print("layer:",layer)
-                # print("target_model:",target_module)   
+                # print("target_model:",target_module)
                 if layer == target_module:
                     logging.debug("Collect: %s" % (module))
                     # print("Collect: %s" % (module))
                     intern_outputs.append(Node_collector(module))
 
         logging.info("Total %d hook inserted" % (len(intern_outputs)))
         # print("Total %d hook inserted" % (len(intern_outputs)))
@@ -404,15 +404,15 @@
     def _insert_hook_quantize(self, model, target_module_list):
         intern_outputs = []
         for layer, module in model.named_modules():
             for target_module in target_module_list:
                 # print("layer:",layer)
                 length = len("_model.")
                 new_key = layer[length:]
-                # print("target_model:",target_module)   
+                # print("target_model:",target_module)
                 if new_key == target_module:
                     logging.debug("Collect: %s" % (module))
                     # print("Collect: %s" % (module))
                     intern_outputs.append(Node_collector(module))
         logging.info("Total %d hook inserted" % (len(intern_outputs)))
         # print("Total %d hook inserted" % (len(intern_outputs)))
         return model, intern_outputs
@@ -517,15 +517,15 @@
 
 
 ##copy form torch.quantization._numeric_suite
 def compare_weights(
         float_dict: Dict[str, Any], quantized_dict: Dict[str, Any]
 ) -> Dict[str, Dict[str, torch.Tensor]]:
     r"""Compare the weights of the float module with its corresponding quantized module.
-    
+
     Returns a dict with key corresponding to module names and each entry being
     a dictionary with two keys 'float' and 'quantized', containing the float and
     quantized weights. This dict can be used to compare and compute the quantization
     error of the weights of float and quantized models.
 
     Example::
 
@@ -604,15 +604,15 @@
     weight_quant_loss = compare_weights(ht.model.state_dict(), q_model_state_dict)
     pertur_lst = {}
     for key in weight_quant_loss:
         op_float_tensor = weight_quant_loss[key]['float']
         op_qnt_tensor = weight_quant_loss[key]['quantized'].dequantize()
         diff_l2 = (torch.norm(op_float_tensor - op_qnt_tensor, p=2) ** 2)
         pertur_lst[key] = diff_l2
-    
+
     if enable_act:
         act_to_traces = traces['activation']
         for trace_i, pertur_i, act_i in zip(op_to_traces.keys(), pertur_lst.keys(), act_to_traces.keys()):
             # Formula:Omig=Trace*L2+act_trace
             op_to_traces[trace_i] = pertur_lst[pertur_i] * op_to_traces[trace_i] + act_to_traces[act_i]
     else:
         for trace_i, pertur_i in zip(op_to_traces.keys(), pertur_lst.keys()):
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/smooth_quant.py` & `neural_compressor-2.2/neural_compressor/adaptor/torch_utils/smooth_quant.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,20 +11,26 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-from neural_compressor.utils.utility import LazyImport
-import json
 
-torch = LazyImport('torch')
-from ...utils import logger
-from collections import UserDict
+try:
+    from neural_compressor.utils.utility import LazyImport
+
+    torch = LazyImport('torch')
+    from ...utils import logger
+except:
+    import torch
+    import logging
+
+    logger = logging.getLogger()
+from collections import UserDict, defaultdict
 
 
 def forward_wrapper(model, input, device='cpu'):
     if isinstance(input, dict) or isinstance(input, UserDict):
         if device == 'cpu':
             output = model(**input)
         else:  # pragma: no cover
@@ -33,16 +39,16 @@
                     if isinstance(input[inp], torch.Tensor) else input[inp]
             output = model(**input)
     elif isinstance(input, list) or isinstance(input, tuple):
         if device == 'cpu':
             output = model(*input)
         else:  # pragma: no cover
             input = [inp.to(device) \
-                    if isinstance(inp, torch.Tensor) else inp
-                    for inp in input] # pylint: disable=E1133
+                         if isinstance(inp, torch.Tensor) else inp
+                     for inp in input]  # pylint: disable=E1133
             output = model(*input)
     else:
         if device == 'cpu' or not isinstance(input, torch.Tensor):
             output = model(input)
         else:  # pragma: no cover
             input = input.to(device)  # pylint: disable=no-member
             output = model(input)
@@ -62,14 +68,24 @@
         for idx, input in enumerate(dataloader):
             output = forward_wrapper(model, input, device)
             cnt += 1
             if cnt >= iters:
                 break
 
 
+def model_forward_per_sample(model, sample, device):
+    try:
+        output = forward_wrapper(model, sample, device)
+        return output
+
+    except Exception as e:
+        output = forward_wrapper(model, sample[0], device)
+        return output
+
+
 def quant_dequant_w(m, num_bits=8, scheme='asym'):  ##TODO take sym as default
     eps = torch.finfo(torch.float32).eps
     if isinstance(m, torch.nn.Linear):
         x = m.weight
         if scheme == 'sym':
             q_min, q_max = -2. ** (num_bits - 1), 2. ** (num_bits - 1) - 1.
             scale = torch.max(torch.abs(x), dim=1).values / (float(q_max - q_min) / 2)
@@ -171,27 +187,29 @@
     Post-Training Quantization for Large Language Models
     [2] SPIQ: Data-Free Per-Channel Static Input Quantization
     Currently, we only handle the layers whose smooth scale could be absorbed, we will support other layers later.
     We only support inplace mode which means the model weights will be changed, you can call recover function
     to recover the weights if needed
     """
 
-    def __init__(self, model, dataloader, traced_model=None):
+    def __init__(self, model, dataloader, example_inputs=None, q_func=None, traced_model=None):
         """
         :param model: Torch model :param dataloader: Calibration dataloader :param traced_model: A specific model
         shares the same architecture as the model and could be traced by torch.jit. If not supplied, we use model
         instead.
         """
         self.model = model
         device, dtype = self._get_device()
         self.model = self.model.to(device)
         self.model.eval()
         self.device = device
         self.dtype = dtype
         self.dataloader = dataloader
+        self.example_inputs = example_inputs
+        self.q_func = q_func
         self.input_values = {}
         self.output_values = {}
         self.input_maxes = {}
         self.input_mins = {}
         self.input_abs_maxes = {}
         self.hook_layer_names = []
         self.hook_values_handles = []
@@ -217,28 +235,27 @@
         """
         A forward hook to save input max of a module
         :param name: the module name
         :return: A hook function
         """
 
         def save_input_hook(module, inputs, outputs):
-            if name not in self.input_maxes.keys():
-                self.input_maxes[name] = []
-                self.input_mins[name] = []
             input = inputs[0]
             ##TODO check input channel is correct
             if len(module.weight.shape) == 4:  ##conv3d or conv1d not supported now, need better way
                 input = input.permute(0, 2, 3, 1)
             input = input.reshape(-1, input.shape[-1])
             max_tensor = torch.max(input, dim=0)[0]
             min_tensor = torch.min(input, dim=0)[0]
-            self.input_maxes[name].append(max_tensor)
-            self.input_mins[name].append(min_tensor)
-            # self.input_values[name] = input
-            # self.output_values[name] = outputs
+            if name not in self.input_maxes.keys():
+                self.input_maxes[name] = max_tensor
+                self.input_mins[name] = min_tensor
+            else:
+                self.input_maxes[name] = torch.max(max_tensor, self.input_maxes[name])
+                self.input_mins[name] = torch.min(min_tensor, self.input_mins[name])
 
         return save_input_hook
 
     def _save_input_output_hook(self, name):
         """
         A forward hook to save input and output values of a module
             param name: the module name
@@ -262,15 +279,17 @@
         :return:
         """
         self.hook_handles = []
         for key in modules.keys():
             hook_func = self._save_input_pc_hook(key)
             hook_handle = modules[key].register_forward_hook(hook_func)
             self.hook_handles.append(hook_handle)
-        if input_output_modules:
+        if self.alpha == 'auto' and input_output_modules:
+            logger.warning("Auto alpha for Smoothquant records input & output"
+                           + ", please avoid out of memory.")
             for key in input_output_modules.keys():
                 hook_func = self._save_input_output_hook(key)
                 hook_handle = input_output_modules[key].register_forward_hook(hook_func)
                 self.hook_values_handles.append(hook_handle)
 
     def _remove_observer(self):
         """
@@ -320,23 +339,21 @@
     def _dump_min_max(self, calibration_method="min_max", calib_iter=100):
         """
         Dump min max per channel information, the min max value will be saved in input_maxes attribute
         :param calibration_method: only support min_max currently
         :param calib_iter: Sample size for calibration
         :return:
         """
-        model_forward(self.model, self.dataloader, calib_iter, self.device)
+        if self.q_func:
+            self.q_func(self.model)
+        else:
+            assert self.dataloader, "Please set dataloader for calibration."
+            model_forward(self.model, self.dataloader, calib_iter, self.device)
         ##stack
         for key in self.input_maxes.keys():
-            max_val = self.input_maxes[key]
-            max_val = torch.stack(max_val, dim=0)
-            min_val = self.input_mins[key]
-            min_val = torch.stack(min_val, dim=0)
-            self.input_maxes[key] = torch.max(max_val, dim=0)[0]
-            self.input_mins[key] = torch.min(min_val, dim=0)[0]
             abs_max_val = torch.max(torch.abs(self.input_mins[key]), torch.abs(self.input_maxes[key]))
             self.input_abs_maxes[key] = abs_max_val
         for key in self.input_values.keys():
             self.input_values[key] = torch.cat(self.input_values[key], dim=0)  ##this may introduce memory issue
             self.output_values[key] = torch.cat(self.output_values[key], dim=0)
 
     def _reshape_in_channel_to_last(self, layer_name):
@@ -388,56 +405,61 @@
         """
         Scale the layer weights at input channel, depthwise conv output channel
         :param layer_name: The layer name
         :param scale: The scale to be multiplied
         :return:
         """
         layer = get_module(self.model, layer_name)
-        from .model_wrapper import SQLinearWrapper
-        if isinstance(layer, SQLinearWrapper):
+        if layer.__class__.__name__ == "SQLinearWrapper":
+            from .model_wrapper import SQLinearWrapper
             layer = layer.sq_linear
         scale = self._reshape_scale_for_weight(layer, scale)
-        layer.weight *= scale
+        layer.weight = torch.nn.Parameter(layer.weight * scale)
         return scale
 
-    def _absorb_scales(self, layer_name, scale):  ##output channel
+    def _absorb_scales(self, layer_name, scale, alpha=0.5):  ##output channel
         """
         Absorb the scale to the layer at output channel
         :param layer_name: The module name
         :param scale: The scale to be absorbed
+        :param alpha_key: The alpha passed to SQLinearWrapper
         :return:
         """
-        from .model_wrapper import SQLinearWrapper
-        layer = get_module(self.model, layer_name)
+
         if self.insert_mul:
+            from .model_wrapper import SQLinearWrapper
+            layer = get_module(self.model, layer_name)
             if isinstance(layer, SQLinearWrapper):
                 set_module(self.model, layer_name, layer.sq_linear)  ##recover
             else:
                 input_minmax = [self.input_mins[layer_name], self.input_maxes[layer_name]]
-                new_module = SQLinearWrapper(layer, scale, input_minmax)
+                new_module = SQLinearWrapper(layer, scale, input_minmax, alpha)
                 set_module(self.model, layer_name, new_module)
 
         elif self.allow_absorb:
+            layer = get_module(self.model, layer_name)
             if isinstance(layer, torch.nn.BatchNorm2d) or isinstance(layer, torch.nn.GroupNorm) or \
                     isinstance(layer, torch.nn.InstanceNorm2d):
                 if layer.affine:
                     layer.weight *= scale
-                    layer.bias *= scale
+                    if layer.bias != None:
+                        layer.bias *= scale
                 else:
                     layer.affine = True
                     weight = torch.ones(layer.num_features, device=self.device, dtype=self.dtype) * scale
                     layer.weight = torch.nn.Parameter(
                         weight, requires_grad=False)
                     bias = torch.zeros(layer.num_features, device=self.device, dtype=self.dtype)
                     layer.bias = torch.nn.Parameter(bias, requires_grad=False
                                                     )
-            elif isinstance(layer, torch.nn.LayerNorm):
+            elif isinstance(layer, torch.nn.LayerNorm) or layer.__class__.__name__ == "LPLayerNorm":
                 if layer.elementwise_affine:
                     layer.weight *= scale
-                    layer.bias *= scale
+                    if layer.bias != None:
+                        layer.bias *= scale
                 else:
                     layer.elementwise_affine = True
                     weight = torch.ones(layer.num_features, device=self.device, dtype=self.dtype) * scale
                     layer.weight = torch.nn.Parameter(
                         torch.ones(weight, requires_grad=False))
                     bias = torch.zeros(layer.num_features, device=self.device, dtype=self.dtype)
                     layer.bias = torch.nn.Parameter(
@@ -453,20 +475,20 @@
             elif isinstance(layer, torch.nn.Linear):
                 if hasattr(layer, "bias") and (layer.bias != None):
                     layer.bias *= scale
                 scale = scale.view(scale.shape[0], 1)
                 layer.weight *= scale
 
             elif layer.__class__.__name__ == "LlamaRMSNorm" \
-              or layer.__class__.__name__ == "T5LayerNorm":  ##quite tricky
+                    or layer.__class__.__name__ == "T5LayerNorm":  ##quite tricky
                 layer.weight *= scale
 
             else:
                 logger.warning(f"found unsupported layer {type(layer)}, try to multiply scale to "
-                  f"weight and bias directly, this may introduce accuracy issue, please have a check ")
+                               f"weight and bias directly, this may introduce accuracy issue, please have a check ")
                 if hasattr(layer, "weight") and layer.weight != None:
                     layer.weight *= scale
                 if hasattr(layer, "bias") and layer.bias != None:
                     layer.bias *= scale
 
     def _adjust_parameters(self, absorb_to_layer, input_maxes, alpha=0.5):
         """
@@ -503,15 +525,15 @@
             weight_power = torch.pow(weight_max_per_channel, 1 - alpha_key)
             # logger.info(f"{absorb_to_layer[key][0]} layer sparsity is
             # {1.0-torch.count_nonzero(input_power)/input_power.numel()}")
 
             scale = torch.clip(input_power / weight_power, min=1e-5)
             scale[input_power == 0] = 1.0
 
-            self._absorb_scales(key, 1.0 / scale)
+            self._absorb_scales(key, 1.0 / scale, alpha_key)
             absorb_scales_info[key] = 1.0 / scale
             layer_names = absorb_to_layer[key]
             for layer_name in layer_names:
                 self._scale_layer_weight(layer_name, scale)
                 weight_scales_info[layer_name] = scale
         return weight_scales_info, absorb_scales_info
 
@@ -564,15 +586,14 @@
         alpha_min: min value of alpha search space.
         alpha_max: max value of alpha search space.
         alpha_step: step size of alpha search space.
         attn_method: criterion method used on attention ops; currently min, max and mean are supported.
         """
         logger.info("auto tuning alpha")
         import copy
-        from .model_wrapper import SQLinearWrapper
         alpha_scale = 100
         alpha_space = list(range(round(alpha_min * alpha_scale), round((alpha_max + alpha_step) * alpha_scale),
                                  round(alpha_step * alpha_scale)))
         alpha_space = [alpha / alpha_scale for alpha in alpha_space]
 
         ans_layer2absorb, self.layer_to_absorb, ans = {}, {}, {}
         ## Searching optimal alphas
@@ -594,15 +615,16 @@
                     self.weight_scale_info, self.absorb_scales_info = self._adjust_parameters(absorb_to_layer_sample,
                                                                                               input_max_op, alpha)
                     input_of_op, output_of_op = self.input_values[layer_key], self.output_values[layer_key]
                     input_scale = self._reshape_scale_for_input(get_module(self.model, layer_key),
                                                                 self.absorb_scales_info[absorb_key])
                     input_of_op_q = quant_dequant_x(input_of_op * input_scale)
                     layer = get_module(self.model, layer_key)
-                    if isinstance(layer, SQLinearWrapper):
+
+                    if layer.__class__.__name__ == "SQLinearWrapper":
                         layer = layer.sq_linear
                     weight_qdq = quant_dequant_w(layer)
                     layer_cp = copy.deepcopy(layer)
                     layer_cp.weight.data = weight_qdq
                     output_of_op_q = layer_cp(input_of_op_q)
                     self.recover()
                     loss = torch.sum(torch.abs(output_of_op - output_of_op_q) ** 2)
@@ -657,33 +679,44 @@
             if alpha < 0:
                 alpha = 0
                 logger.warning("reset alpha to 0 ")
             if alpha > 1.0:
                 alpha = 1.0
                 logger.warning("reset alpha to 1.0 ")
 
+        self.alpha = alpha
         if not isinstance(self.model, torch.nn.Module):
             logger.warning("smooth quant is ignored since the model is not a torch module")
             return self.model
         self.recover()
         need_calibration = self._check_need_calibration(alpha, percentile, op_types, scales_per_op, calib_iter)
         with torch.no_grad():
             input_maxes = self.input_maxes
             if need_calibration:  ##avoid multiple calibaration during tuning if the only difference is alpha
                 if self.insert_mul:
-                    self.self_absorb_layers = self._get_all_layer_names()   # TODO: only support linear now.
+                    self.self_absorb_layers = self._get_all_layer_names()  # TODO: only support linear now.
                 if self.allow_absorb:
                     self.absorb_to_layer, no_absorb_layers = self._trace(
                         op_types)  ##TODO we need to insert mul layer for no_absorb_layers later
                     if self.absorb_to_layer == None and no_absorb_layers == None:
-                        logger.warning("sorry, could not trace the model, smooth quant is ignored")
+                        logger.warning("sorry, could not trace the model, smooth quant is skipped")
                         logger.warning("if you are using huggingface model,"
-                                    "you could set torchscript to True "
-                                    "when loading the model or set the return_dict to False")
+                                       "you could set torchscript to True "
+                                       "when loading the model or set the return_dict to False")
                         return self.model
+                    elif self.absorb_to_layer == {}:
+                        logger.warning("could not find any layer to be absorbed")
+                    else:
+                        to_absorb_cnt = 0
+                        for key, item in self.absorb_to_layer.items():
+                            to_absorb_cnt += len(item)
+
+                        logger.info(
+                            f" {to_absorb_cnt} out of {to_absorb_cnt + len(no_absorb_layers)} "
+                            f"layers could be absorbed in smooth quant")
 
                 # remove self.self_absorb_layers if it exists in self.absorb_to_layer
                 for k, v in self.absorb_to_layer.items():
                     for i in v:
                         if i in self.self_absorb_layers:
                             self.self_absorb_layers.pop(i)
                 self.absorb_to_layer.update(self.self_absorb_layers)
@@ -696,73 +729,128 @@
                                    "you could set torchscript to True ")
                     return self.model
                 save_input_output = False
                 if alpha == "auto":
                     save_input_output = True
 
                 input_maxes = self._calibrate(self.absorb_to_layer, calib_iter, save_input_output)
+
+                # Check if input_maxes match self.absorb_to_layer 
+                # (due to self._get_all_layer_names use layer tree instead of forward_path)
+                if not folding:
+                    diff_modules = set(self.absorb_to_layer.keys()).difference(input_maxes.keys())
+                    for d in diff_modules:
+                        del self.absorb_to_layer[d]
+                        
                 if alpha == 'auto':
                     self.alpha_per_layer = self._auto_tune_alpha(input_maxes, **auto_alpha_args)  ##save the alpha
 
             if alpha == 'auto':
                 alpha = self.alpha_per_layer
+            example_inputs = self._get_example_input()
+            if example_inputs != None:
+                out_pre_sq = model_forward_per_sample(self.model, example_inputs, self.device)
 
             self.weight_scale_info, self.absorb_scales_info = self._adjust_parameters(self.absorb_to_layer,
                                                                                       input_maxes, alpha)
+
+            self.model._smoothquant_optimized = True
+            if example_inputs != None:
+                # Check mathematical equivelancy
+                out_post_sq = model_forward_per_sample(self.model, example_inputs, self.device)
+
+                if not self.output_is_equal(out_post_sq, out_pre_sq):
+                    logger.warning(
+                        "Mathematical equivelancy of Smoothquant is not preserved. "
+                        "Please kindly report this issue to https://github.com/intel/neural-compressor.")
+                    # self.recover()
+                    # self.model._smoothquant_optimized = False
+            else:
+                logger.warning(" Could not get example input, equivelancy check is skipped")
+
             self.input_values, self.output_values = {}, {}
             return self.model
 
+    def output_is_equal(self, out1, out2, atol=1e-04):
+        try:
+            if isinstance(out1, tuple):
+                return all(torch.all(torch.isclose(out1[i], out2[i], atol=atol)) for i in range(len(out1)))
+            elif isinstance(out1, dict):
+                return all(torch.all(torch.isclose(out1[k], out2[k], atol=atol)) for k in out1.keys())
+            elif isinstance(out1, torch.Tensor):
+                return torch.all(torch.isclose(out1, out2, atol=atol))
+            return False
+        except:
+            logger.warning("Automatically check failed, Please check equivelancy manually "
+                           "between out_pre_sq and out_post_sq if necessary.")
+            return True
+
     def recover(self):
         """
         recover the model weights
         :return:
         """
         with torch.no_grad():
             for key in self.weight_scale_info:
                 self._scale_layer_weight(key, 1.0 / self.weight_scale_info[key])
             for key in self.absorb_scales_info:
                 self._absorb_scales(key, 1.0 / self.absorb_scales_info[key])
             self.weight_scale_info = {}  ##clear the data
             self.absorb_scales_info = {}
- 
+
     def _get_all_layer_names(self, op_types=['Linear']):
         """
         Try the model to find the layers which can be smooth quantized.
         :param op_types: The op types to be smooth quantized
         :return:
         self_absorb_layer: A dict, absorb layer name (itself): layers to be smooth quantized
         """
         self_absorb_layer = {}
         for name, module in self.model.named_modules():
             for op_type in op_types:
                 if op_type == str(module.__class__.__name__):
                     self_absorb_layer[name] = [name]
         return self_absorb_layer
 
+    def _get_example_input(self):
+        if self.dataloader == None and self.example_inputs == None:
+            return None
+        if self.example_inputs is None:
+            ##assert self.dataloader, "Please provide dataloader or example_inputs"
+            for idx, input in enumerate(self.dataloader):
+                self.example_inputs = input
+
+        return self.example_inputs
+
     def _trace(self, op_types):
         """
         Try the model to find the layers which can be smooth quantized.
         :param op_types: The op types to be smooth quantized
         :return:
         absorb_to_layer: A dict, absorb layer name:layers to be smooth quantized
         no_absorb_layers: A list saving the layers which could not find the absorb layer
         """
         tg = GraphTrace()
-        for idx, input in enumerate(self.dataloader):
-            example_inputs = input
-            break
-        absorb_to_layer, no_absorb_layers = tg.get_absorb_to_layer(self.traced_model, example_inputs, op_types)
+        self._get_example_input()
+        absorb_to_layer, no_absorb_layers = tg.get_absorb_to_layer(self.traced_model, self.example_inputs, op_types)
         return absorb_to_layer, no_absorb_layers
 
 
 def get_parent(node):
     if node.inputs() == None:
         return None
     return list(node.inputs())[0].node()
 
+def get_parents(node):
+    if node.inputs() == None:
+        return None
+    elif len(list(node.inputs())) == 0:
+        return None
+    return list(node.inputs())
+
 
 class GraphTrace:
     """
     """
 
     def __init__(self):
         self.supported_torch_module_to_aten = {
@@ -771,14 +859,15 @@
             "ConvTranspose2d": "aten::_convolution",
             "LayerNorm": "aten::layer_norm",
             "BatchNorm2d": "aten::batch_norm",
             "GroupNorm": "aten::group_norm",
             "InstanceNorm2d": "aten::instance_norm",
             "LlamaRMSNorm": "aten::mul",
             "T5LayerNorm": "aten::mul",
+            "LPLayerNorm": "aten::layer_norm"  ##mpt_chat
         }
         ##TODO, must statisfy af(x)=f(ax),current skip layer may be incomplete
         self.skip_ops_to_find_absorb = ["aten::to",
                                         "aten::relu",
                                         "aten::leaky_relu",
                                         "aten::hardtanh"
                                         ]
@@ -787,17 +876,17 @@
                                     "aten::group_norm",
                                     "aten::instance_norm",
                                     "aten::mul"]  ##TODO,suppport more norm
 
     def trace(self, model, dummy_input):
         traced_model = None
         optimize_numerics = False
-        if isinstance(dummy_input, dict):
+        if isinstance(dummy_input, dict) or isinstance(dummy_input, UserDict):
             try:
-                traced_model = torch.jit.trace(model, dummy_input["input_ids"], strict=False)
+                traced_model = torch.jit.trace(model, example_kwarg_inputs=dict(dummy_input), strict=False)
                 traced_model = torch.jit.freeze(traced_model.eval(), optimize_numerics=optimize_numerics)
             except:
                 pass
         else:
             try:
                 traced_model = torch.jit.trace(model, dummy_input, strict=False)
                 traced_model = torch.jit.freeze(traced_model.eval(), optimize_numerics=optimize_numerics)
@@ -817,24 +906,35 @@
             node_type = node.kind()
             for op_type in op_types:
                 if node_type == op_type:
                     nodes.append((node, op_type))
                     break
         return nodes
 
-    def get_prev_absorb_layer(self, nodes):
+    def get_prev_absorb_layer(self, nodes, dict_parent_kind=None):
         prev_absorb_layer = []
         for node in nodes:
             parent = get_parent(node)
+            parent_scopeName = parent.scopeName()
             while 1:
                 if parent.kind() in self.skip_ops_to_find_absorb:
                     parent = get_parent(parent)
                     continue
                 if parent.kind() in self.could_absorb_layers:
-                    prev_absorb_layer.append(parent)
+                    if dict_parent_kind:
+                        parent_out_kinds = set(dict_parent_kind[parent_scopeName])
+                        parent_out_kinds.discard('aten::size')
+                        if parent_out_kinds == parent_out_kinds.intersection(self.could_absorb_layers):
+                            prev_absorb_layer.append(parent)
+                        elif parent_out_kinds.intersection(self.skip_ops_to_find_absorb):
+                            prev_absorb_layer.append(parent) ##TODO: check other scenarios
+                        else: # When parent to multiple ops, sq transformation could be wrong.
+                            prev_absorb_layer.append(None)
+                    else:
+                        prev_absorb_layer.append(parent)
                 else:
                     prev_absorb_layer.append(None)
                 break
         return prev_absorb_layer
 
     def mapping_torch_module_to_aten(self, op_types):
         res = []
@@ -846,27 +946,41 @@
         res = list(set(res))
         return res
 
     def get_absorb_to_layer(self, model, example_input, op_types):
         traced_model = self.trace(model, example_input)
         if traced_model == None:
             return None, None
+
+        dict_parent_kind = defaultdict(list)
+        for node in traced_model.graph.nodes():
+            parents_list = get_parents(node)
+            node_kind, node_scopeName = node.kind(), node.scopeName()
+            if parents_list: #save input_kinds of all parent nodes
+                for parent_ in parents_list:
+                    parent = parent_.node()
+                    parent_kind = parent.kind()
+                    if 'prim' not in parent_kind and parent.scopeName() != node_scopeName:
+                        dict_parent_kind[parent.scopeName()].append(node_kind)
+
         aten_op_types = self.mapping_torch_module_to_aten(op_types)
         nodes_types = self.get_nodes(traced_model, aten_op_types)
         nodes = [node_type[0] for node_type in nodes_types]
-        nodes_prev_absorb = self.get_prev_absorb_layer(nodes)
+        nodes_prev_absorb = self.get_prev_absorb_layer(nodes, dict_parent_kind)
         absorb_to_layer = {}
         no_absorb_layers = []
         for index, absorb in enumerate(nodes_prev_absorb):
             if absorb == None:
                 no_absorb_layers.append(nodes[index])
                 continue
             node = nodes[index]
             layer_name = '.'.join(node.scopeName().split('/')[-1].split('.')[1:])
             absorb_name = '.'.join(absorb.scopeName().split('/')[-1].split('.')[1:])
+            if layer_name == "" or absorb_name == "":
+                continue
             if absorb_name in absorb_to_layer.keys():
                 absorb_to_layer[absorb_name].append(layer_name)
             else:
                 absorb_to_layer[absorb_name] = [layer_name]
         absorb_to_layer = self.remove_unsupported_layers(model, absorb_to_layer)
         return absorb_to_layer, no_absorb_layers
 
@@ -885,43 +999,7 @@
                 layer_type = layer.__class__.__name__
                 if layer_type not in self.supported_torch_module_to_aten.keys():
                     supported = False
                     break
             if supported:
                 res[key] = absorb_to_layer[key]
         return res
-
-def update_sq_scale(ipex_config_path, smoothquant_scale_info):
-    """update ipex_config.json with smoothquant scale info generated by our algorithm.
-
-    Args:
-        ipex_config_path (str): a path to temporary ipex_config.json file.
-        smoothquant_scale_info (dict): a dict contains smoothquant scale info.
-    """
-    with open(ipex_config_path, 'r') as f:
-        ipex_config = json.load(f)
-        for module_name, v in ipex_config.items():
-            if 'q_op_infos' in v and v['q_op_infos']:
-                for op_num, v1 in v['q_op_infos'].items():
-                    if 'weight_tensor_infos' in v1 and v1['weight_tensor_infos']:
-                        op_name = v1['fqn']
-                        if op_name in smoothquant_scale_info:
-                            input_scale_for_mul = \
-                                    smoothquant_scale_info[op_name]['input_scale_for_mul'].tolist()
-                            input_scale_after_mul = \
-                                    smoothquant_scale_info[op_name]['input_scale_after_mul'].tolist()
-                            input_zero_point_after_mul = \
-                                    smoothquant_scale_info[op_name]['input_zero_point_after_mul'].tolist()
-                            weight_scale_for_mul = \
-                                    (1 / smoothquant_scale_info[op_name]['input_scale_for_mul']).tolist()
-                            weight_scale_after_mul = \
-                                    smoothquant_scale_info[op_name]['weight_scale_after_mul'].tolist()
-                            v1['input_tensor_infos'][0]['smooth_quant_scaling_factor'] = input_scale_for_mul
-                            v1['input_tensor_infos'][0]['scale'] = input_scale_after_mul
-                            v1['input_tensor_infos'][0]['zero_point'] = input_zero_point_after_mul
-                            v1['weight_tensor_infos'][0]['smooth_quant_scaling_factor'] = weight_scale_for_mul
-                            v1['weight_tensor_infos'][0]['scale'] = weight_scale_after_mul
-        f.close()
-    # overwrite ipex_config_path
-    with open(ipex_config_path, 'w') as f1:
-        json.dump(ipex_config, f1, indent = 4)
-        f1.close()
```

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/symbolic_trace.py` & `neural_compressor-2.2/neural_compressor/adaptor/torch_utils/symbolic_trace.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/adaptor/torch_utils/util.py` & `neural_compressor-2.2/neural_compressor/adaptor/torch_utils/util.py`

 * *Files 9% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Util Class and Functions."""
 import copy
 import re
+import json
 import numpy as np
 from collections import UserDict
 from packaging.version import Version
 from ...utils import logger
 from ...utils.utility import LazyImport, CpuInfo
 
 tqdm = LazyImport("tqdm")
@@ -58,129 +59,14 @@
     op_type = str(type(module))
     if 'fused' in op_type:
         return True
     else:
         return False
 
 
-def _set_input_scale_hook(model, op_cfgs):
-    """Insert hooks to observer input scale and zeropoint.
-
-    Args:
-        model (object): the input model
-        op_cfgs (dict): dictionary of quantization configure for each op
-
-    Returns:
-        hook_list (list): the input observer hooks
-    """
-    def input_scale_hook(module, input):
-        assert hasattr(module, 'input_observer'), \
-            'Expect input_observer attribute already attached to the module'
-        module.input_observer(input[0])
-        return input
-
-    def output_scale_hook(module, input, output):
-        assert hasattr(module, 'output_observer'), \
-            'Expect output_observer attribute already attached to the module'
-        module.output_observer(output)
-        return output
-
-    def ConvReLU2d_scale_hook(module, input):
-        assert hasattr(module, 'input_observer'), \
-            'Expect input_observer attribute already attached to the module'
-        assert hasattr(module, 'output_observer'), \
-            'Expect output_observer attribute already attached to the module'
-        module.input_observer(input[0])
-        output = module._conv_forward(input[0], module.weight_fake_quant(module.weight), module.bias)
-        module.output_observer(output)
-        return input
-
-    def LinearReLU_scale_hook(module, input):
-        import torch.nn.functional as F
-        assert hasattr(module, 'input_observer'), \
-            'Expect input_observer attribute already attached to the module'
-        assert hasattr(module, 'output_observer'), \
-            'Expect output_observer attribute already attached to the module'
-        module.input_observer(input[0])
-        output = F.linear(input[0], module.weight_fake_quant(module.weight), module.bias)
-        module.output_observer(output)
-        return input
-
-    hook_list = []
-    for name, module in model.named_modules():
-        if 'Conv' in str(module.__class__.__name__) or \
-          'Linear' in str(module.__class__.__name__):
-            if not hasattr(module, 'qconfig') or not module.qconfig:
-                continue
-            device = next(module.parameters()).device
-            from torch.nn.intrinsic.qat import ConvBn2d, ConvReLU2d, ConvBnReLU2d, LinearReLU
-            if type(module) in [ConvBn2d, ConvBnReLU2d]:
-                module.input_observer = module.qconfig.activation().to(device)
-                handle_in = module.register_forward_pre_hook(input_scale_hook)
-                # module[0] == torch.nn.BatchNorm2d
-                module[0].qconfig = module.qconfig
-                module[0].output_observer = module[0].qconfig.activation().to(device)
-                handle_out = module[0].register_forward_hook(output_scale_hook)
-                hook_list.extend([handle_in, handle_out])
-            elif type(module) in [ConvReLU2d, LinearReLU]:
-                module.input_observer = module.qconfig.activation().to(device)
-                module.output_observer = module.qconfig.activation().to(device)
-                handle_in_out = module.register_forward_pre_hook(ConvReLU2d_scale_hook if \
-                                        type(module) == ConvReLU2d else LinearReLU_scale_hook)
-                hook_list.extend([handle_in_out])
-            else:
-                if is_fused_module(module):
-                    continue
-                module.input_observer = module.qconfig.activation().to(device)
-                module.output_observer = module.qconfig.activation().to(device)
-                handle_in = module.register_forward_pre_hook(input_scale_hook)
-                handle_out = module.register_forward_hook(output_scale_hook)
-                hook_list.extend([handle_in, handle_out])
-    return hook_list
-
-
-def _get_input_scale(model, hook_list):
-    """Fetch input scale and zeropoint from observer.
-
-    Args:
-        model (object): the input model
-        hook_list (list): the input observer hooks
-
-    Returns:
-        input_scale_info (dict): the input scale and zero_point of each modules
-    """
-    scale_info = {}
-    for name, module in model.named_modules():
-        from torch.nn.intrinsic.qat import ConvBn2d, ConvBnReLU2d
-        if type(module) in [ConvBn2d, ConvBnReLU2d]:
-            if hasattr(module, "input_observer") and hasattr(module[0], "output_observer"):
-                scale_in, zero_point_in = module.input_observer.calculate_qparams()
-                scale_out, zero_point_out = module[0].output_observer.calculate_qparams()
-                scale_info[name] = {
-                    'input_scale': float(scale_in),
-                    'input_zeropoint': int(zero_point_in),
-                    'output_scale': float(scale_out),
-                    'output_zeropoint': int(zero_point_out)
-                }
-                del module.input_observer, module[0].output_observer
-        elif hasattr(module, "input_observer") and hasattr(module, "output_observer"):
-            scale_in, zero_point_in = module.input_observer.calculate_qparams()
-            scale_out, zero_point_out = module.output_observer.calculate_qparams()
-            scale_info[name] = {
-                'input_scale': float(scale_in),
-                'input_zeropoint': int(zero_point_in),
-                'output_scale': float(scale_out),
-                'output_zeropoint': int(zero_point_out)
-            }
-            del module.input_observer, module.output_observer
-    for h in hook_list:
-        h.remove()
-    return scale_info
-
-
 def collate_torch_preds(results):
     """Fetch collated results.
 
     Args:
         result (list): input result
 
     Returns:
@@ -364,15 +250,15 @@
                                 if pre_op_output['id'] == input_tensor_id:
                                     pre_op_output_infos[index]['inf_dtype'] = input_tensor_dtype
                                 else:
                                     print('Do not find the input id', input_tensor_id)
                             pre_op_infos['output_tensor_infos'] = pre_op_output_infos
                             cfgs[pre_op_module][pre_op_state][pre_op_index] = pre_op_infos
                         else:
-                            print("Don't track the previous op name for ", name)
+                            pass
             cfgs[name[0]][name[1]][name[2]] = ipex_op_cfg
     return cfgs
 
 def paser_cfgs(cfgs): # pragma: no cover
     """Parse configs.
 
     Args:
@@ -490,14 +376,63 @@
                         next_op_name = None
                     if next_op_name is None:
                         q_ops.append(cur + [cur_name])
             for q_op in q_ops:
                 quantizable_ops.append(q_op)
     return quantizable_ops
 
+def update_sq_scale(ipex_config_path, smoothquant_scale_info):
+    """update ipex_config.json with smoothquant scale info generated by our algorithm.
+
+    Args:
+        ipex_config_path (str): a path to temporary ipex_config.json file.
+        smoothquant_scale_info (dict): a dict contains smoothquant scale info.
+    """
+    with open(ipex_config_path, 'r') as f:
+        ipex_config = json.load(f)
+        for module_name, v in ipex_config.items():
+            if 'q_op_infos' in v and v['q_op_infos']:
+                for op_num, v1 in v['q_op_infos'].items():
+                    # update alpha data instead of updating weight scale
+                    op_name = v1['fqn'] # fqn always exists even it's empty.
+                    if op_name in smoothquant_scale_info:
+                        input_scale_for_mul = \
+                                smoothquant_scale_info[op_name]['input_scale_for_mul'].tolist()
+                        input_scale_after_mul = \
+                                smoothquant_scale_info[op_name]['input_scale_after_mul'].tolist()
+                        input_zero_point_after_mul = \
+                                smoothquant_scale_info[op_name]['input_zero_point_after_mul'].tolist()
+                        weight_scale_for_mul = \
+                                (1 / smoothquant_scale_info[op_name]['input_scale_for_mul']).tolist()
+                        weight_scale_after_mul = \
+                                smoothquant_scale_info[op_name]['weight_scale_after_mul'].tolist()
+                        v1['input_tensor_infos'][0]['smooth_quant_scaling_factor'] = input_scale_for_mul
+                        v1['input_tensor_infos'][0]['scale'] = input_scale_after_mul
+                        v1['input_tensor_infos'][0]['zero_point'] = input_zero_point_after_mul
+                        v1['weight_tensor_infos'][0]['smooth_quant_scaling_factor'] = weight_scale_for_mul
+                        v1['weight_tensor_infos'][0]['scale'] = weight_scale_after_mul
+                        # # observers were overridden by the fallback step, setting it back.
+                        v1['activation_observer'] = {'name': 'SmoothQuantActivationObserver', 
+                                        'smooth_quant_enabled': False, 'dtype': 'torch.quint8', 
+                                        'qscheme': 'torch.per_tensor_affine', 'reduce_range': False,
+                                        'quant_min': 0, 'quant_max': 255, 
+                                        'alpha': smoothquant_scale_info[op_name]['alpha']
+                                        }
+                        v1['weight_observer'] = {'name': 'SmoothQuantWeightObserver', 
+                                        'smooth_quant_enabled': False, 'dtype': 'torch.qint8', 
+                                        'qscheme': 'torch.per_channel_symmetric', 'reduce_range': False, 
+                                        'quant_min': -128, 'quant_max': 127, 
+                                        'alpha': smoothquant_scale_info[op_name]['alpha'] #only update alpha
+                                        }
+        f.close()
+    # overwrite ipex_config_path
+    with open(ipex_config_path, 'w') as f1:
+        json.dump(ipex_config, f1, indent = 4)
+        f1.close()
+
 def auto_copy(module):  # pragma: no cover
     """Get an IPEX prepared model and return a fp32 model.
 
     Args:
         module (object): IPEX prepared model.
 
     Returns:
@@ -630,17 +565,17 @@
         model (object): the input model.
         input (object).
 
     Returns:
         output (object).
     """
     with torch.no_grad():
-        if type(input) is dict:
+        if isinstance(input, (dict, UserDict)):
             output = model(**input)
-        elif type(input) is tuple or type(input) is list:
+        elif isinstance(input, (list, tuple)):
             try:
                 output = model(*input)
             except:
                 output = model(input)
         else:
             output = model(input)
     return output
@@ -947,8 +882,37 @@
     # TODO handle reduce range
     quant_min, quant_max = None, None
     if unsigned:
         quant_min, quant_max =0.0 , 2.0**(num_bits) - 1.0
     else:
         quant_min, quant_max = -1 * 2.0**(num_bits - 1), 2.0**(num_bits - 1) - 1
     return quant_min, quant_max
-    
+
+def get_depth(d) -> int:
+    """Query the depth of the dict."""
+    if isinstance(d, dict):
+        return 1 + max(get_depth(v) for v in d.values())
+    return 0
+
+def get_dict_at_depth(d, target_depth, result, depth=0):
+    """Get all sub-dicts that are at a specified depth in a nested dict."""
+    if depth == target_depth:
+        result.append(d)
+        return
+    elif depth < target_depth and isinstance(d, dict):
+        for k, v in d.items():
+            get_dict_at_depth(v, target_depth, result, depth=depth+1)
+
+def get_element_under_depth(d, ops_lst):
+    """Get all values in a nested dict."""
+    if isinstance(d, dict):
+        for k, v in d.items():
+            get_element_under_depth(v, ops_lst)
+    else:
+        ops_lst.append(d)
+
+def get_op_type_by_name(op_name, quantizable_ops):
+    """Get op type by op name."""
+    for pair in quantizable_ops:
+        if pair[0] == op_name:
+            return pair[1]
+    return None
```

### Comparing `neural_compressor-2.1.1/neural_compressor/algorithm/__init__.py` & `neural_compressor-2.2/neural_compressor/algorithm/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/algorithm/algorithm.py` & `neural_compressor-2.2/neural_compressor/algorithm/algorithm.py`

 * *Files 1% similar despite different names*

```diff
@@ -113,15 +113,14 @@
         Returns:
             model: The framework model.
         """
         assert self._q_model, 'set q_model for algorithm'
         if len(self._exec_algorithms.get(location, [])) == 0:
             return self._q_model
         assert self._origin_model, 'set origin model for algorithm'
-        assert self._dataloader, 'set dataloader for algorithm'
         assert self._adaptor, 'set adaptor for algorithm'
         assert self._calib_iter, 'set calibration iteration for algorithm'
         for algo in self._exec_algorithms.get(location, []):
             self._q_model = algo(self._origin_model,
                                  self._q_model, \
                                  self._adaptor, \
                                  self._dataloader, \
```

### Comparing `neural_compressor-2.1.1/neural_compressor/algorithm/fast_bias_correction.py` & `neural_compressor-2.2/neural_compressor/algorithm/fast_bias_correction.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/algorithm/smooth_quant.py` & `neural_compressor-2.2/neural_compressor/algorithm/smooth_quant.py`

 * *Files 2% similar despite different names*

```diff
@@ -74,17 +74,17 @@
         kwargs = {}  ##different backends may have different default values
         if self.op_types != None:
             kwargs["op_types"] = self.op_types
         if self.percentile != None:
             kwargs['percentile'] = self.percentile
         if self.scales_per_op != None:
             kwargs['scales_per_op'] = self.scales_per_op
+        kwargs['folding'] = self.folding
         q_model = adaptor.smooth_quant(
             origin_model,
             dataloader,
             calib_iter,
             self.tune_cfg,
             alpha=self.alpha,
-            folding=self.folding,
             **kwargs,
         )
         return q_model
```

### Comparing `neural_compressor-2.1.1/neural_compressor/algorithm/weight_correction.py` & `neural_compressor-2.2/neural_compressor/algorithm/weight_correction.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/benchmark.py` & `neural_compressor-2.2/neural_compressor/benchmark.py`

 * *Files 17% similar despite different names*

```diff
@@ -11,37 +11,39 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Benchmark is used for evaluating the model performance."""
-
+import json
 import os
 import re
+import signal
+import subprocess
 import sys
+from threading import Thread
+
 import numpy as np
-import subprocess
-import signal
 import psutil
-from threading import Thread
+
+from neural_compressor.profiling.parser.factory import ParserFactory
+from neural_compressor.profiling.profiler.factory import ProfilerFactory
 from .adaptor import FRAMEWORKS
-from .objective import MultiObjective
-from .conf.config import BenchmarkConf
-from .utils import logger
-from .utils import OPTIONS
-from .utils.utility import GLOBAL_STATE, MODE
-from .conf.dotdict import deep_get, deep_set
-from .model import BaseModel
-from .model import Model as NCModel
-from .model.model import get_model_fwk_name
-from .conf.pythonic_config import Config
-from .utils import logger
-from .conf.pythonic_config import Config
 from .config import BenchmarkConfig
+from .config import options
+from .data import check_dataloader
+from .model import BaseModel, Model
+from .objective import MultiObjective
+from .profiling.parser.parser import ProfilingParser
+from .profiling.profiler.profiler import Profiler
+from .utils import alias_param, logger, OPTIONS
+from .utils.neural_insights_utils import register_neural_insights_workload, \
+    update_neural_insights_workload
+from .utils.utility import GLOBAL_STATE, MODE, print_table, dump_table
 from .utils.utility import Statistics
 
 
 def set_env_var(env_var, value, overwrite_existing=False):
     """Set the specified environment variable.
 
     Only set new env in two cases:
@@ -54,26 +56,25 @@
 
 def set_all_env_var(conf, overwrite_existing=False):
     """Set all the environment variables with the configuration dict.
 
     Neural Compressor only uses physical cores
     """
     cpu_counts = psutil.cpu_count(logical=False)
-    if not conf:
-        conf = {}
-        conf['num_of_instance'] = 1
-        conf['cores_per_instance'] = cpu_counts
-    if 'cores_per_instance' in conf:
-        assert conf['cores_per_instance'] * conf['num_of_instance'] <= cpu_counts,\
+    assert isinstance(conf, BenchmarkConfig), \
+        'input has to be a Config object'
+
+    if conf.cores_per_instance is not None:
+        assert conf.cores_per_instance * conf.num_of_instance <= cpu_counts, \
             'num_of_instance * cores_per_instance should <= cpu physical cores'
     else:
-        assert conf['num_of_instance'] <= cpu_counts, 'num_of_instance should <= cpu counts'
-        conf['cores_per_instance'] = int(cpu_counts / conf['num_of_instance'])
-
-    for var, value in conf.items():
+        assert conf.num_of_instance <= cpu_counts, \
+            'num_of_instance should <= cpu counts'
+        conf.cores_per_instance = int(cpu_counts / conf.num_of_instance)
+    for var, value in dict(conf).items():
         set_env_var(var.upper(), value, overwrite_existing)
 
 
 def get_architecture():
     """Get the architecture name of the system."""
     p1 = subprocess.Popen("lscpu", stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
     p2 = subprocess.Popen(["grep", "Architecture"], stdin=p1.stdout, stdout=subprocess.PIPE)
@@ -136,426 +137,409 @@
         socket_core = sockets[idx] + ":" + x
         if socket_core not in existing_socket_core_list:
             res.append(int(threads[idx]))
             existing_socket_core_list.append(socket_core)
     return res
 
 
-class _Benchmark(object):
-    """Benchmark class can be used to evaluate the model performance.
+def run_instance(model, conf, b_dataloader=None, b_func=None):
+    """Run the instance with the configuration.
 
-    With the objective setting, user can get the data of what they configured in yaml.
+    Args:
+        model (object):           The model to be benchmarked.
+        conf (BenchmarkConfig): The configuration for benchmark containing accuracy goal,
+                                  tuning objective and preferred calibration & quantization
+                                  tuning space etc.
+        b_dataloader:             The dataloader for frameworks.
+        b_func:                   Customized benchmark function. If user passes the dataloader,
+                                  then b_func is not needed.
+    """
+    results = {}
+    if b_func is None:
+        GLOBAL_STATE.STATE = MODE.BENCHMARK
+        framework_specific_info = {'device': conf.device,
+                                   'approach': None,
+                                   'random_seed': options.random_seed,
+                                   'backend': conf.backend if conf.backend is not None else 'default',
+                                   'format': 'default'}
+        framework = conf.framework.lower()
+        if 'tensorflow' in framework:
+            framework_specific_info.update({"inputs": conf.inputs, \
+                                            "outputs": conf.outputs, \
+                                            "recipes": {}, \
+                                            'workspace_path': options.workspace})
+        if framework == 'keras':
+            framework_specific_info.update({'workspace_path': options.workspace})
+        if framework == 'mxnet':
+            framework_specific_info.update({"b_dataloader": b_dataloader})
+        if 'onnx' in framework:
+            framework_specific_info.update(
+                                 {'workspace_path': options.workspace, \
+                                 'graph_optimization': OPTIONS[framework].graph_optimization})
+        if framework == 'pytorch_ipex' or framework == 'pytorch' or framework == 'pytorch_fx':
+            framework_specific_info.update({"workspace_path": options.workspace,
+                                            "q_dataloader": None})
+
+        assert isinstance(model, BaseModel), 'need set neural_compressor Model for quantization....'
+
+        adaptor = FRAMEWORKS[framework](framework_specific_info)
+
+        assert b_dataloader is not None, "dataloader should not be None"
+
+        from neural_compressor.utils.create_obj_from_config import create_eval_func
+        b_func = create_eval_func(conf.framework,
+                                  b_dataloader,
+                                  adaptor,
+                                  None,
+                                  iteration=conf.iteration)
+
+        objectives = MultiObjective(["performance"],
+                                    {'relative': 0.1},
+                                    is_measure=True)
+
+        val = objectives.evaluate(b_func, model)
+        # measurer contain info not only performance(eg, memory, model_size)
+        # also measurer have result list among steps
+        acc, _ = val
+        batch_size = b_dataloader.batch_size
+        warmup = conf.warmup
+        if len(objectives.objectives[0].result_list()) < warmup:
+            if len(objectives.objectives[0].result_list()) > 1 and warmup != 0:
+                warmup = 1
+            else:
+                warmup = 0
+
+        result_list = objectives.objectives[0].result_list()[warmup:]
+        latency = np.array(result_list).mean() / batch_size
+        results["performance"] = acc, batch_size, result_list
+
+        logger.info("\nbenchmark result:")
+        for i, res in enumerate(result_list):
+            logger.debug("Iteration {} result {}:".format(i, res))
+        logger.info("Batch size = {}".format(batch_size))
+        logger.info("Latency: {:.3f} ms".format(latency * 1000))
+        logger.info("Throughput: {:.3f} images/sec".format(1. / latency))
+        return results
+    else:
+        b_func(model.model)
+
+
+def generate_prefix(core_list):
+    """Generate the command prefix with numactl.
 
     Args:
-        conf (obj): The config.BenchmarkConfig class containing accuracy goal, tuning objective etc.
+        core_list: a list of core indexes bound with specific instances
     """
+    if sys.platform in ['linux'] and os.system('numactl --show >/dev/null 2>&1') == 0:
+        return 'OMP_NUM_THREADS={} numactl --localalloc --physcpubind={}'.format(\
+            len(core_list), ','.join(core_list.astype(str)))
+    elif sys.platform in ['win32']:  # pragma: no cover
+        # (TODO) should we move the hw_info from ux?
+        from neural_compressor.utils.utility import get_number_of_sockets
+        num_of_socket = int(get_number_of_sockets())
+        cores_per_instance = int(os.environ.get('CORES_PER_INSTANCE'))
+        cores_per_socket = int(psutil.cpu_count(logical=False)) / num_of_socket
+        socket_id = int(core_list[0] // cores_per_socket)
+        # cores per socket should integral multiple of cores per instance, else not bind core
+        if cores_per_socket % cores_per_instance == 0:
+            from functools import reduce
+            hex_core = hex(reduce(lambda x, y : x | y, [1 << p for p in core_list]))
+            return 'start /b /WAIT /node {} /affinity {} CMD /c'.format(socket_id, hex_core)
+    else:
+        return ''
 
-    def __init__(self, conf):
-        """Init a Benchmark object."""
-        self.framework = None
-        self._model = None
-        self._b_dataloader = None
-        self._b_func = None
-        self._results = {}
-        assert isinstance(conf, BenchmarkConfig), \
-            "The config object should be config.BenchmarkConfig, not {}".format(type(conf))
-        conf = Config(quantization=None, benchmark=conf, pruning=None, distillation=None, nas=None)
-        self.conf = BenchmarkConf()
-        self.conf.map_pyconfig_to_cfg(conf)
-        if self.conf.usr_cfg.model.framework != 'NA':
-            self.framework = self.conf.usr_cfg.model.framework.lower()
-
-    def __call__(self, raw_cmd=None):
-        """Directly call a Benchmark object.
-
-        Args:
-            raw_cmd: raw command used for benchmark
-        """
-        cfg = self.conf.usr_cfg
-        assert cfg.evaluation is not None, 'benchmark evaluation filed should not be None...'
-        assert sys.platform in ['linux', 'win32'], 'only support platform windows and linux...'
-        set_all_env_var(deep_get(cfg, 'evaluation.performance.configs'))
-        # disable multi-instance for running bechmark on GPU device
-        if cfg.device == 'gpu':
-            set_env_var('NC_ENV_CONF', True, overwrite_existing=True)
 
-        logger.info("Start to run Benchmark.")
-        if os.environ.get('NC_ENV_CONF') == 'True':
-            return self.run_instance()
-        if raw_cmd is None:
-            raw_cmd = sys.executable + ' ' + ' '.join(sys.argv)
-        self.config_instance(raw_cmd)
-        self.summary_benchmark()
-        return None
+def call_one(cmd, log_file):
+    """Execute one command for one instance in one thread and dump the log (for Windows)."""
+    proc = subprocess.Popen(cmd, stdin=subprocess.PIPE,
+                            stdout=subprocess.PIPE,
+                            stderr=subprocess.STDOUT,
+                            shell=True) # nosec
+    with open(log_file, "w", 1, encoding="utf-8") as log_file:
+        log_file.write(f"[ COMMAND ] {cmd} \n")
+        for line in proc.stdout:
+            decoded_line = line.decode("utf-8", errors="ignore").strip()
+            logger.info(decoded_line)   # redirect to terminal
+            log_file.write(decoded_line + "\n")
 
-    fit = __call__
 
-    def summary_benchmark(self):
-        """Get the summary of the benchmark."""
-        if sys.platform in ['linux']:
-            num_of_instance = int(os.environ.get('NUM_OF_INSTANCE'))
-            cores_per_instance = int(os.environ.get('CORES_PER_INSTANCE'))
-            latency_l = []
-            throughput_l = []
-            for i in range(0, num_of_instance):
-                log = '{}_{}_{}.log'.format(num_of_instance, cores_per_instance, i)
-                with open(log, "r") as f:
-                    for line in f:
-                        latency = re.search(r"[L,l]atency:\s+(\d+(\.\d+)?)", line)
-                        latency_l.append(float(latency.group(1))) if latency and latency.group(1) else None
-                        throughput = re.search(r"[T,t]hroughput:\s+(\d+(\.\d+)?)", line)
-                        throughput_l.append(float(throughput.group(1))) if throughput and throughput.group(1) else None
-            if throughput_l and latency_l:
-                assert len(latency_l)==len(throughput_l)==num_of_instance, \
-                    "Multiple instance benchmark failed with some instance!"
-
-                output_data = [
-                    ["Latency average [second/sample]", "{:.6f}".format((sum(latency_l)/len(latency_l))/1000)],
-                    ["Throughput sum [samples/second]", "{:.3f}".format(sum(throughput_l))]
-                ]
-                logger.info("********************************************")
-                Statistics(
-                    output_data,
-                    header='Multiple Instance Benchmark Summary',
-                    field_names=["Items", "Result"]).print_stat()
-        else:
-            # (TODO) should add summary after win32 benchmark has log
-            pass
+def config_instance(raw_cmd):
+    """Configure the multi-instance commands and trigger benchmark with sub process.
 
-    def call_one(self, cmd, log_file):
-        """Execute one command for one instance in one thread and dump the log (for Windows)."""
-        proc = subprocess.Popen(cmd, stdin=subprocess.PIPE,
-                                stdout=subprocess.PIPE,
-                                stderr=subprocess.STDOUT,
-                                shell=True) # nosec
-        with open(log_file, "w", 1, encoding="utf-8") as log_file:
-            log_file.write(f"[ COMMAND ] {cmd} \n")
-            for line in proc.stdout:
-                decoded_line = line.decode("utf-8", errors="ignore").strip()
-                logger.info(decoded_line)   # redirect to terminal
-                log_file.write(decoded_line + "\n")
-
-    def config_instance(self, raw_cmd):
-        """Configure the multi-instance commands and trigger benchmark with sub process.
-
-        Args:
-            raw_cmd: raw command used for benchmark
-        """
-        multi_instance_cmd = ''
+    Args:
+        raw_cmd: raw command used for benchmark
+    """
+    multi_instance_cmd = ''
+    num_of_instance = int(os.environ.get('NUM_OF_INSTANCE'))
+    cores_per_instance = int(os.environ.get('CORES_PER_INSTANCE'))
+
+    logger.info("num of instance: {}".format(num_of_instance))
+    logger.info("cores per instance: {}".format(cores_per_instance))
+
+    if (sys.platform in ['linux'] and get_architecture() == 'aarch64' and int(get_threads_per_core()) > 1):
+        raise OSError('Currently no support on ARM with hyperthreads')
+    elif sys.platform in ['linux']:
+        bounded_threads = get_bounded_threads(get_core_ids(), get_threads(), get_physical_ids())
+
+    for i in range(0, num_of_instance):
+        if sys.platform in ['linux'] and get_architecture() == 'x86_64':
+            core_list_idx = np.arange(0, cores_per_instance) + i * cores_per_instance
+            core_list = np.array(bounded_threads)[core_list_idx]
+        else:
+            core_list = np.arange(0, cores_per_instance) + i * cores_per_instance
+        # bind cores only allowed in linux/mac os with numactl enabled
+        prefix = generate_prefix(core_list)
+        instance_cmd = '{} {}'.format(prefix, raw_cmd)
+        if sys.platform in ['linux']:
+            instance_log = '{}_{}_{}.log'.format(num_of_instance, cores_per_instance, i)
+            multi_instance_cmd += '{} 2>&1|tee {} & \\\n'.format(
+                instance_cmd, instance_log)
+        else:  # pragma: no cover
+            multi_instance_cmd += '{} \n'.format(instance_cmd)
+
+    multi_instance_cmd += 'wait' if sys.platform in ['linux'] else ''
+    logger.info("Running command is\n{}".format(multi_instance_cmd))
+    # each instance will execute single instance
+    set_env_var('NC_ENV_CONF', True, overwrite_existing=True)
+    if sys.platform in ['linux']:
+        p = subprocess.Popen(multi_instance_cmd, preexec_fn=os.setsid, shell=True) # nosec
+    elif sys.platform in ['win32']:  # pragma: no cover
+        cmd_list = multi_instance_cmd.split("\n")[:-1]
+        threads = []
+        for idx, cmd in enumerate(cmd_list):
+            # wrap each execution of windows bat file in one thread
+            # write the log to the log file of the corresponding instance
+            logger.info('Will dump to {}_{}_{}.log'.format(num_of_instance, cores_per_instance, idx))
+            threads.append(Thread(target=call_one, args=(cmd,
+                '{}_{}_{}.log'.format(num_of_instance, cores_per_instance, idx))))
+        for command_thread in threads:
+            command_thread.start()
+            logger.info("Worker threads start")
+        # Wait for all of them to finish
+        for command_thread in threads:
+            command_thread.join()
+            logger.info("Worker threads join")
+        return
+    try:
+        p.communicate()
+    except KeyboardInterrupt:
+        os.killpg(os.getpgid(p.pid), signal.SIGKILL)
+
+
+def summary_benchmark():
+    """Get the summary of the benchmark."""
+    if sys.platform in ['linux']:
         num_of_instance = int(os.environ.get('NUM_OF_INSTANCE'))
         cores_per_instance = int(os.environ.get('CORES_PER_INSTANCE'))
+        latency_l = []
+        throughput_l = []
+        for i in range(0, num_of_instance):
+            log = '{}_{}_{}.log'.format(num_of_instance, cores_per_instance, i)
+            with open(log, "r") as f:
+                for line in f:
+                    latency = re.search(r"[L,l]atency:\s+(\d+(\.\d+)?)", line)
+                    latency_l.append(float(latency.group(1))) if latency and latency.group(1) else None
+                    throughput = re.search(r"[T,t]hroughput:\s+(\d+(\.\d+)?)", line)
+                    throughput_l.append(float(throughput.group(1))) if throughput and throughput.group(1) else None
+        if throughput_l and latency_l:
+            assert len(latency_l)==len(throughput_l)==num_of_instance, \
+                "Multiple instance benchmark failed with some instance!"
+
+            output_data = [
+                ["Latency average [second/sample]", "{:.6f}".format((sum(latency_l)/len(latency_l))/1000)],
+                ["Throughput sum [samples/second]", "{:.3f}".format(sum(throughput_l))]
+            ]
+            logger.info("********************************************")
+            Statistics(
+                output_data,
+                header='Multiple Instance Benchmark Summary',
+                field_names=["Items", "Result"]).print_stat()
+    else:
+        # (TODO) should add summary after win32 benchmark has log
+        pass
 
-        logger.info("num of instance: {}".format(num_of_instance))
-        logger.info("cores per instance: {}".format(cores_per_instance))
 
-        if(sys.platform in ['linux'] and get_architecture() == 'aarch64' and int(get_threads_per_core()) > 1):
-            raise OSError('Currently no support on ARM with hyperthreads')
-        elif sys.platform in ['linux']:
-            bounded_threads = get_bounded_threads(get_core_ids(), get_threads(), get_physical_ids())
+def profile(model, conf, b_dataloader) -> None:
+    """Execute profiling for benchmark configuration.
 
-        for i in range(0, num_of_instance):
-            if sys.platform in ['linux'] and get_architecture() == 'x86_64':
-                core_list_idx = np.arange(0, cores_per_instance) + i * cores_per_instance
-                core_list = np.array(bounded_threads)[core_list_idx]
-            else:
-                core_list = np.arange(0, cores_per_instance) + i * cores_per_instance
-            # bind cores only allowed in linux/mac os with numactl enabled
-            prefix = self.generate_prefix(core_list)
-            instance_cmd = '{} {}'.format(prefix, raw_cmd)
-            if sys.platform in ['linux']:
-                instance_log = '{}_{}_{}.log'.format(num_of_instance, cores_per_instance, i)
-                multi_instance_cmd += '{} 2>&1|tee {} & \\\n'.format(
-                    instance_cmd, instance_log)
-            else:  # pragma: no cover
-                multi_instance_cmd += '{} \n'.format(instance_cmd)
-
-        multi_instance_cmd += 'wait' if sys.platform in ['linux'] else ''
-        logger.info("Running command is\n{}".format(multi_instance_cmd))
-        # each instance will execute single instance
-        set_env_var('NC_ENV_CONF', True, overwrite_existing=True)
-        if sys.platform in ['linux']:
-            p = subprocess.Popen(multi_instance_cmd, preexec_fn=os.setsid, shell=True) # nosec
-        elif sys.platform in ['win32']:  # pragma: no cover
-            cmd_list = multi_instance_cmd.split("\n")[:-1]
-            threads = []
-            for idx, cmd in enumerate(cmd_list):
-                # wrap each execution of windows bat file in one thread
-                # write the log to the log file of the corresponding instance
-                logger.info('Will dump to {}_{}_{}.log'.format(num_of_instance, cores_per_instance, idx))
-                threads.append(Thread(target=self.call_one, args=(cmd,
-                    '{}_{}_{}.log'.format(num_of_instance, cores_per_instance, idx))))
-            for command_thread in threads:
-                command_thread.start()
-                logger.info("Worker threads start")
-            # Wait for all of them to finish
-            for command_thread in threads:
-                command_thread.join()
-                logger.info("Worker threads join")
-            return
-        try:
-            p.communicate()
-        except KeyboardInterrupt:
-            os.killpg(os.getpgid(p.pid), signal.SIGKILL)
-
-    def generate_prefix(self, core_list):
-        """Generate the command prefix with numactl.
-
-        Args:
-            core_list: a list of core indexes bound with specific instances
-        """
-        if sys.platform in ['linux'] and os.system('numactl --show >/dev/null 2>&1') == 0:
-            return 'OMP_NUM_THREADS={} numactl --localalloc --physcpubind={}'.format(\
-                len(core_list), ','.join(core_list.astype(str)))
-        elif sys.platform in ['win32']:  # pragma: no cover
-            # (TODO) should we move the hw_info from ux?
-            from neural_compressor.ux.utils.hw_info import get_number_of_sockets
-            num_of_socket = int(get_number_of_sockets())
-            cores_per_instance = int(os.environ.get('CORES_PER_INSTANCE'))
-            cores_per_socket = int(psutil.cpu_count(logical=False)) / num_of_socket
-            socket_id = int(core_list[0] // cores_per_socket)
-            # cores per socket should integral multiple of cores per instance, else not bind core
-            if cores_per_socket % cores_per_instance == 0:
-                from functools import reduce
-                hex_core = hex(reduce(lambda x, y : x | y, [1 << p for p in core_list]))
-                return 'start /b /WAIT /node {} /affinity {} CMD /c'.format(socket_id, hex_core)
-        else:
-            return ''
+    Args:
+        model: The model to be profiled.
+        conf: The configuration for benchmark containing accuracy goal,
+              tuning objective and preferred calibration & quantization
+              tuning space etc.
+        b_dataloader: The dataloader for frameworks.
+
+    Returns:
+        None
+    """
+    intra_num_of_threads = 1
+    inter_num_of_threads = 1
+    num_warmup = 10
+
+    intra_num_of_threads_conf = conf.intra_num_of_threads
+    if intra_num_of_threads_conf is not None:
+        intra_num_of_threads = intra_num_of_threads_conf
+    else:
+        logger.warning(
+            f"Could not find intra_num_of_threads value in config. Using: {intra_num_of_threads}",
+        )
+
+    inter_num_of_threads_conf = conf.inter_num_of_threads
+    if inter_num_of_threads_conf is not None:
+        inter_num_of_threads = inter_num_of_threads_conf
+    else:
+        logger.warning(
+            f"Could not find inter_num_of_threads value in config. Using: {inter_num_of_threads}",
+        )
+
+    num_warmup_conf = conf.warmup
+    if num_warmup_conf is not None:
+        num_warmup = num_warmup_conf
+    else:
+        logger.warning(
+            f"Could not get find num_warmup value in config. Using: {num_warmup}",
+        )
+
+    profiling_log = os.path.abspath(
+        os.path.join(
+            options.workspace,
+            "diagnosis.log",
+        ),
+    )
+    profiler: Profiler = ProfilerFactory.get_profiler(
+        model=model,
+        dataloader=b_dataloader,
+        log_file=profiling_log,
+    )
+    profiler.profile_model(
+        intra_num_of_threads=intra_num_of_threads,
+        inter_num_of_threads=inter_num_of_threads,
+        num_warmup=num_warmup,
+    )
+    parser: ProfilingParser = ParserFactory.get_parser(
+        model=model,
+        logs=[profiling_log],
+    )
+    parsed_results = parser.process()
+    print_table(
+        title="Profiling",
+        column_mapping={
+            "Node name": "node_name",
+            "Total execution time [us]": "total_execution_time",
+            "Accelerator execution time [us]": "accelerator_execution_time",
+            "CPU execution time [us]": "cpu_execution_time",
+            "OP run": "op_run",
+            "OP defined": "op_defined",
+        },
+        table_entries=parsed_results,
+    )
+
+    profiling_table_file = os.path.join(
+        options.workspace,
+        "profiling_table.csv",
+    )
+
+    dump_table(
+        filepath=profiling_table_file,
+        column_mapping={
+            "Node name": "node_name",
+            "Total execution time [us]": "total_execution_time",
+            "Accelerator execution time [us]": "accelerator_execution_time",
+            "CPU execution time [us]": "cpu_execution_time",
+            "OP run": "op_run",
+            "OP defined": "op_defined",
+        },
+        table_entries=parsed_results,
+        file_type="csv",
+    )
+
+    profiling_data_file = os.path.join(
+        options.workspace,
+        "profiling_data.json",
+    )
+    with open(profiling_data_file, "w") as profiling_json:
+        json.dump(parsed_results, profiling_json, indent=4)
 
-    def run_instance(self):
-        """Run the instance with the configuration.
 
-        Args:
-            runs benchmarking with numactl on specific cores and instances set
-                by user config and returns model performance
-        """
-        if self._b_func is None:
-            cfg = self.conf.usr_cfg
-            GLOBAL_STATE.STATE = MODE.BENCHMARK
-            framework_specific_info = {'device': cfg.device, \
-                                       'approach': cfg.quantization.approach, \
-                                       'random_seed': cfg.tuning.random_seed,
-                                       'backend': cfg.model.get('backend', 'default'),
-                                       'format': cfg.model.get('quant_format', 'default')}
-            framework = cfg.model.framework.lower()
-            if 'tensorflow' in framework:
-                framework_specific_info.update({"inputs": cfg.model.inputs, \
-                                                "outputs": cfg.model.outputs, \
-                                                "recipes": cfg.model.recipes, \
-                                                'workspace_path': cfg.tuning.workspace.path})
-            if framework == 'keras':
-                framework_specific_info.update({'workspace_path': cfg.tuning.workspace.path})
-            if framework == 'mxnet':
-                framework_specific_info.update({"b_dataloader": self._b_dataloader})
-            if 'onnx' in framework.lower():
-                framework_specific_info.update(
-                                     {'workspace_path': cfg.tuning.workspace.path, \
-                                     'graph_optimization': OPTIONS[framework].graph_optimization})
-            if framework == 'pytorch_ipex' or framework == 'pytorch' or framework == 'pytorch_fx':
-                framework_specific_info.update({"workspace_path": cfg.tuning.workspace.path,
-                                                "q_dataloader": None})
-
-            assert isinstance(self._model, BaseModel), 'need set neural_compressor Model for quantization....'
-
-            adaptor = FRAMEWORKS[framework](framework_specific_info)
-
-            if deep_get(cfg, 'evaluation.performance.iteration') == -1 and 'dummy_v2' in \
-                deep_get(cfg, 'evaluation.performance.dataloader.dataset', {}):
-                deep_set(cfg, 'evaluation.performance.iteration', 10)
-
-            iteration = -1 if deep_get(cfg, 'evaluation.performance.iteration') is None \
-                else deep_get(cfg, 'evaluation.performance.iteration')
-
-            b_postprocess_cfg = deep_get(cfg, 'evaluation.performance.postprocess')
-
-            assert self._b_dataloader is not None, "dataloader should not be None"
-
-            from neural_compressor.utils.create_obj_from_config import create_eval_func
-            self._b_func = create_eval_func(self.framework, \
-                                    self._b_dataloader, \
-                                    adaptor, \
-                                    None, \
-                                    b_postprocess_cfg,
-                                    iteration=iteration)
-
-            self.objectives = MultiObjective(["performance"],
-                                             {'relative': 0.1},
-                                             is_measure=True)
-
-            val = self.objectives.evaluate(self._b_func, self._model)
-            # measurer contain info not only performance(eg, memory, model_size)
-            # also measurer have result list among steps
-            acc, _ = val
-            batch_size = self._b_dataloader.batch_size
-            warmup = deep_get(cfg, "evaluation.performance.warmup")
-            if len(self.objectives.objectives[0].result_list()) < warmup:
-                if len(self.objectives.objectives[0].result_list()) > 1 and warmup != 0:
-                    warmup = 1
-                else:
-                    warmup = 0
-
-            result_list = self.objectives.objectives[0].result_list()[warmup:]
-            latency = np.array(result_list).mean() / batch_size
-            self._results["performance"] = acc, batch_size, result_list
-
-            logger.info("\nbenchmark result:")
-            for i, res in enumerate(result_list):
-                logger.debug("Iteration {} result {}:".format(i, res))
-            logger.info("Batch size = {}".format(batch_size))
-            logger.info("Latency: {:.3f} ms".format(latency * 1000))
-            logger.info("Throughput: {:.3f} images/sec".format(1. / latency))
-        else:
-            self._b_func(self._model.model)
+def benchmark_with_raw_cmd(raw_cmd, conf=None):
+    """Benchmark the model performance with the raw commend.
+
+    Args:
+        raw_cmd (string):           The commend to be benchmarked.
+        conf (BenchmarkConfig): The configuration for benchmark containing accuracy goal,
+                                  tuning objective and preferred calibration & quantization
+                                  tuning space etc.
+
+    Example::
+
+        # Run benchmark according to config
+        from neural_compressor.benchmark import fit_with_raw_cmd
+
+        conf = BenchmarkConfig(iteration=100, cores_per_instance=4, num_of_instance=7)
+        fit_with_raw_cmd("test.py", conf)
+    """
+    if conf is not None:
+        if conf.backend == "ipex":
+            import intel_extension_for_pytorch
+        assert sys.platform in ['linux', 'win32'], 'only support platform windows and linux...'
+        # disable multi-instance for running bechmark on GPU device
+        set_all_env_var(conf)
+
+    config_instance(raw_cmd)
+    summary_benchmark()
 
-    @property
-    def results(self):
-        """Get the results of benchmarking."""
-        return self._results
-
-    @property
-    def b_dataloader(self):
-        """Get the dataloader for the benchmarking."""
-        return self._b_dataloader
-
-    @b_dataloader.setter
-    def b_dataloader(self, dataloader):
-        """Set dataloader for benchmarking.
-
-        It is iterable and the batched data should consist of a tuple like (input, label) or yield (input, _).
-        When b_dataloader is set, users can configure postprocess(optional) and metric
-        in yaml file or set postprocess and metric cls for evaluation,
-        or just get performance without a label in dataloader and configure postprocess/metric.
-
-        Args:
-            dataloader(generator): users are supported to set a user-defined dataloader
-                                    which meet the requirements that can yield a tuple of
-                                    (input, label)/(input, _) batched data.
-                                    Another good practice is to use
-                                    neural_compressor.data.DataLoader
-                                    to initialize a neural_compressor dataloader object.
-                                    Notice neural_compressor.data.DataLoader
-                                    is just a wrapper of the information needed to
-                                    build a dataloader, it can't yield
-                                    batched data and only in this setter method
-                                    a 'real' eval_dataloader will be created,
-                                    the reason is we have to know the framework info
-                                    and only after the Quantization object is created then
-                                    framework information can be known.
-                                    Future we will support creating iterable dataloader
-                                    from neural_compressor.data.DataLoader
-        """
-        assert hasattr(dataloader, '__iter__') and \
-                    hasattr(dataloader, 'batch_size'), \
-                    'dataloader must implement __iter__ method and batch_size attribute'
-        self._b_dataloader = dataloader
-
-    @property
-    def b_func(self):
-        """Not support getting b_func."""
-        assert False, 'Should not try to get the value of `b_func` attribute.'
-        return None
-
-    @b_func.setter
-    def b_func(self, user_b_func):
-        """Eval function for benchmark.
-
-        Args:
-            user_b_func: This function takes "model" as input parameter
-                         and executes the entire training process with self
-                         contained training hyper-parameters. If train_func is set,
-                         an evaluation process must be triggered and the user should
-                         set eval_dataloader with metric configured or directly eval_func
-                         to make an evaluation of the model executed.
-        """
-        self._b_func = user_b_func
-
-    @property
-    def model(self):
-        """Get the model."""
-        return self._model
-
-    @model.setter
-    def model(self, user_model):
-        """Set the user model and dispatch to the framework-specific internal model object.
-
-        Args:
-           user_model: users are supported to set model from the original framework model format
-                       (eg, tensorflow frozen_pb or path to a saved model),
-                       but not recommended. A best practice is to set from an initialized
-                       neural_compressor.model.Model.
-                       If tensorflow model is used, the model's inputs/outputs will be
-                       auto inferenced, but sometimes auto inferenced
-                       inputs/outputs will not meet your requests, so it is better to
-                       set them manually in config yaml file.
-                       Another corner case is the slim model of tensorflow,
-                       be careful of the name of the model configured in the yaml file,
-                       make sure the name is in the supported slim model list.
-        """
-        cfg = self.conf.usr_cfg
-        if cfg.model.framework == 'NA':
-            assert not isinstance(user_model, BaseModel), \
-                "Please pass an original framework model but not neural compressor model!"
-            self.framework = get_model_fwk_name(user_model)
-            if self.framework == "tensorflow":
-                from .model.tensorflow_model import get_model_type
-                if get_model_type(user_model) == 'keras' and cfg.model.backend == 'itex':
-                    self.framework = 'keras'
-            if self.framework == "pytorch":
-                if cfg.model.backend == "default":
-                    self.framework = "pytorch_fx"
-                elif cfg.model.backend == "ipex":
-                    self.framework = "pytorch_ipex"
-                    import intel_extension_for_pytorch
-            cfg.model.framework = self.framework
-
-        if not isinstance(user_model, BaseModel):
-            logger.warning("Force convert framework model to neural_compressor model.")
-            if "tensorflow" in self.framework or self.framework == "keras":
-                self._model = NCModel(user_model, backend=self.framework, device=cfg.device)
-            else:
-                self._model = NCModel(user_model, backend=self.framework)
-        else:
-            # It is config of neural_compressor version < 2.0, no need in 2.0
-            if cfg.model.framework == "pytorch_ipex":
-                from neural_compressor.model.torch_model import IPEXModel
-                if not isinstance(user_model, IPEXModel):
-                    self._model = NCModel(user_model.model, framework=cfg.model.framework)
-                    return
-            self._model = user_model
-
-        if 'tensorflow' in self.framework:
-            self._model.name = cfg.model.name
-            self._model.output_tensor_names = cfg.model.outputs
-            self._model.input_tensor_names = cfg.model.inputs
-            self._model.workspace_path = cfg.tuning.workspace.path
-
-    def __repr__(self):
-        """Get the object representation in string format."""
-        return 'Benchmark'
 
-def fit(model, config=None, b_dataloader=None, b_func=None):
+@alias_param("conf", param_alias='config')
+def fit(model, conf, b_dataloader=None, b_func=None):
     """Benchmark the model performance with the configure.
 
     Args:
         model (object):           The model to be benchmarked.
-        config (BenchmarkConfig): The configuration for benchmark containing accuracy goal,
+        conf (BenchmarkConfig): The configuration for benchmark containing accuracy goal,
                                   tuning objective and preferred calibration & quantization
                                   tuning space etc.
         b_dataloader:             The dataloader for frameworks.
         b_func:                   Customized benchmark function. If user passes the dataloader,
                                   then b_func is not needed.
 
     Example::
 
         # Run benchmark according to config
         from neural_compressor.benchmark import fit
 
         conf = BenchmarkConfig(iteration=100, cores_per_instance=4, num_of_instance=7)
-        fit(model='./int8.pb', config=conf, b_dataloader=eval_dataloader)
+        fit(model='./int8.pb', conf=conf, b_dataloader=eval_dataloader)
     """
-    benchmarker = _Benchmark(config)
-    benchmarker.model = model
-    if b_func is not None:
-        benchmarker.b_func = b_func
+    if conf.backend == "ipex":
+        import intel_extension_for_pytorch
+
+    wrapped_model = Model(model, conf=conf)
+
     if b_dataloader is not None:
-        benchmarker.b_dataloader = b_dataloader
-    benchmarker()
-    return benchmarker.results
+        check_dataloader(b_dataloader)
+
+    assert sys.platform in ['linux', 'win32'], 'only support platform windows and linux...'
+    # disable multi-instance for running benchmark on GPU device
+    set_all_env_var(conf)
+    if conf.device == 'gpu':
+        set_env_var('NC_ENV_CONF', True, overwrite_existing=True)
+
+    if conf.diagnosis and os.environ.get('NC_ENV_CONF', None) in [None, 'False']:
+        logger.info("Start to run Profiling")
+        ni_workload_id = register_neural_insights_workload(
+            workload_location=os.path.abspath(os.path.abspath(options.workspace)),
+            model=wrapped_model,
+            workload_mode="benchmark",
+        )
+        try:
+            update_neural_insights_workload(ni_workload_id, "wip")
+            profile(wrapped_model, conf, b_dataloader)
+            update_neural_insights_workload(ni_workload_id, "success")
+        except Exception as e:
+            logger.error(e)
+            update_neural_insights_workload(ni_workload_id, "failure")
+
+    logger.info("Start to run Benchmark.")
+    if os.environ.get('NC_ENV_CONF') == 'True':
+        return run_instance(model=wrapped_model, conf=conf, b_dataloader=b_dataloader, b_func=b_func)
+    raw_cmd = sys.executable + ' ' + ' '.join(sys.argv)
+    benchmark_with_raw_cmd(raw_cmd)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/compression/__init__.py` & `neural_compressor-2.2/neural_compressor/compression/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -11,10 +11,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from .callbacks import QuantizationAwareTrainingCallbacks, DistillationCallbacks, PruningCallbacks
-from ..experimental.compression import prepare_pruning
-from .. import WeightPruningConfig
+from .callbacks import QuantizationAwareTrainingCallbacks, DistillationCallbacks, PruningCallbacks
```

### Comparing `neural_compressor-2.1.1/neural_compressor/compression/distillation/__init__.py` & `neural_compressor-2.2/neural_compressor/compression/distillation/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/compression/distillation/criterions.py` & `neural_compressor-2.2/neural_compressor/compression/distillation/criterions.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/compression/pruner/__init__.py` & `neural_compressor-2.2/neural_compressor/contrib/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,19 @@
-"""prune init."""
-# !/usr/bin/env python
+#!/usr/bin/env python
 # -*- coding: utf-8 -*-
 #
-# Copyright (c) 2022 Intel Corporation
+# Copyright (c) 2021 Intel Corporation
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #   http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
-# limitations under the License.
+# limitations under the License.
+
+"""Built-in strategy for multiple framework backends."""
+from .strategy import *
```

### Comparing `neural_compressor-2.1.1/neural_compressor/compression/pruner/patterns.py` & `neural_compressor-2.2/neural_compressor/compression/pruner/patterns.py`

 * *Files 13% similar despite different names*

```diff
@@ -12,22 +12,23 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from .utils import torch
+import numpy as np
 from .utils import logger
 from collections import namedtuple
-
+from ...utils.utility import LazyImport
+torch = LazyImport('torch')
+tf = LazyImport('tensorflow')
 
 PATTERNS = {}
 
-
 def register_pattern(name):
     """Class decorator used to register a Pattern subclass to the registry.
 
     Decorator function used before a Pattern subclasses.
     Make sure that this Pattern class can be registered in PATTERNS.
 
     Args:
@@ -41,15 +42,15 @@
         """Register patterns."""
         PATTERNS[name] = pattern
         return pattern
 
     return register
 
 
-def get_pattern(config, modules):
+def get_pattern(config, modules, framework='pytorch'):
     """Get registered pattern class.
 
     Get a Pattern object from PATTERNS.
 
     Args:
         config: A config dict object that contains the pattern information.
         modules: Torch neural network modules to be pruned with the pattern.
@@ -59,112 +60,349 @@
 
     Raises:
         AssertionError: Currently only support patterns which have been registered in PATTERNS.
     """
     name = config.pattern
     name = name.split('_')[-1]
     if "x" in name:
-        return PATTERNS["NxM"](config, modules)
+        return PATTERNS["NxM"](config, modules, framework)
     if ":" in name:
         return PATTERNS["N:M"](config, modules)
+    if "mha" in name:
+        return PATTERNS["MHA"](config, modules)
     assert False, f"currently only support {PATTERNS.keys()}"
 
 
 SparsityInfo = namedtuple("SparsityInfo", ['zero_cnt', 'total_cnt', 'sparsity_ratio'])
 
+class ProgressivePatternUtils(object):
+    @staticmethod
+    def _reshape_orig_to_2dims(data):
+        """Process layers that are not two-dimensional(e.g conv layer).
+
+        Args:
+            data: Input.
+            
+        Returns:
+            Reshaped data.
+        """
+        if len(data.shape) == 4:  ##TODO need to verify whether it's ok for transposed conv
+            data = data.permute(0, 2, 3, 1)  ##cout,k,k,cin
+            data = data.reshape(data.shape[0], -1)
+        return data
+
+    @staticmethod
+    def _reshape_2dims_to_orig(data, orig_shape):
+        """Recover layers that are not two-dimensional(e.g conv layer).
+
+        Args:
+            data: Input.
+            
+        Returns:
+            Reshaped data.
+        """
+        if len(orig_shape) == 4:
+            data = data.reshape(orig_shape[0], orig_shape[2], orig_shape[3], orig_shape[1])
+            data = data.permute(0, 3, 1, 2)
+        return data
+
+    # some util functions which can be used.
+    @staticmethod
+    def count_new_masked_cnts(new_added_masks):
+        """Count the number of elements to be masked.
+        
+        Args:
+            new_added_masks: A dict {"layer_name": Tensor} that stores the added masks.
+
+        Returns:
+            The number of masked weights.
+        """
+        # count how many elements are to masked,
+        new_masked_cnts = 0
+        for key in new_added_masks.keys():
+            new_masked_cnts += torch.nonzero(1 - new_added_masks[key]).size()[0]
+        return new_masked_cnts
+
+    @staticmethod
+    def update_new_added_masks(pre_masks, cur_masks):
+        """Obtain the new set-to-zero masks during a pruning procedure.
+
+        Pre_masks, cur_masks should have identical keys bacause they represent the same model.
+
+        Args:
+            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
+            cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
+        
+        Returns:
+            A dict {"layer_name": Tensor} that stores the added masks.
+        """
+        # obtain the new set-to-zero mask during a pruning procedure.
+        # pre_masks, cur_masks should have identical keys bacause they stands for one model.
+        new_added_masks = {}
+        for key in pre_masks.keys():
+            pre_mask = pre_masks[key]
+            cur_mask = cur_masks[key]
+            zero = torch.tensor([0.]).to(pre_mask.device)
+            one = torch.tensor([1.]).to(cur_mask.device)
+            new_added_masks[key] = torch.where(pre_mask == cur_mask, one, zero)
+        return new_added_masks
+
+    @staticmethod
+    def update_progressive_masks_global_scores(pre_masks, cur_masks, scores, progressive_step, progressive_configs):
+        """Generate the progressive masks.
+        
+        Args:
+            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
+            cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
+            scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
+            progressive_step: An integer representing the number of current step in progressive pruning.
+            progressive_configs: A dict that stores configurations of progressive pruning.
+        
+        Returns:
+            A dict{"layer_name": Tensor} that stores the masks generated in progressive pruning.
+        """
+        # three types: score-global (nxm and n:m), score-local(nxm and n:m), linear (only nxm)
+        # score-local is a special type of score global therefore can be implemented with only one function
+        progressive_steps = progressive_configs["progressive_steps"]
+        progressive_masks = {}
+        global_new_added_score_list = []
+        new_added_masks = ProgressivePatternUtils.update_new_added_masks(pre_masks, cur_masks)
+        new_added_masks_cnts = ProgressivePatternUtils.count_new_masked_cnts(new_added_masks)
+        kth_masked_position = (new_added_masks_cnts * progressive_step) // progressive_steps
+        for key in scores.keys():
+            # block_size = self.block_size[key]
+            # mask_num_each_block = progressive_step * int((block_size[0] * block_size[1]) // progressive_steps)
+            new_added_filter = 1 - new_added_masks[key]
+            new_added_cnts = torch.nonzero(new_added_filter).size()[0]
+            score = scores[key]
+
+            score_masked = (score * new_added_filter).abs()
+            score_masked_row, _ = torch.sort(score_masked.flatten(), descending=True)
+            score_masked_row = score_masked_row[:new_added_cnts]
+            global_new_added_score_list.append(score_masked_row)
+
+        global_new_added_scores = torch.cat(global_new_added_score_list, dim=0)
+        if global_new_added_scores.size()[0] == 0:
+            # an empty tensor, at target sparsity is 0 situation
+            return pre_masks
+        threshold, _ = torch.kthvalue(global_new_added_scores, kth_masked_position, dim=0)
+        for key in scores.keys():
+            new_added_mask = new_added_masks[key]
+            score = scores[key]
+            new_added_filter = 1 - new_added_mask
+            score_masked = (score * new_added_filter).abs()
+            zero = torch.tensor([0.]).to(score.device)
+            one = torch.tensor([1.]).to(score.device)
+            progressive_mask = (new_added_mask + torch.where(score_masked <= threshold, zero, one)) * pre_masks[key]
+            progressive_masks[key] = progressive_mask
+        return progressive_masks
+    
+    @staticmethod
+    def update_progressive_masks_local_scores(pre_masks, cur_masks, scores, progressive_step, progressive_configs):
+        """Generate the progressive masks.
+        
+        Args:
+            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
+            cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
+            scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
+            progressive_step: An integer representing the number of current step in progressive pruning.
+            progressive_configs: A dict that stores configurations of progressive pruning.
+        
+        Returns:
+            A dict{"layer_name": Tensor} that stores the masks generated in progressive pruning.
+        """
+        # local is a speicial type of global, therefore we can call global to implement this
+        progressive_steps = progressive_configs["progressive_steps"]
+        progressive_masks = {}
+        for key in scores.keys():
+            # for local use
+            pre_masks_for_this = {key: pre_masks[key]}
+            cur_masks_for_this = {key: cur_masks[key]}
+            scores_for_this = {key: scores[key]}
+            progressive_masks_for_this = ProgressivePatternUtils.update_progressive_masks_global_scores(
+                pre_masks_for_this,
+                cur_masks_for_this,
+                scores_for_this,
+                progressive_step,
+                progressive_configs
+            )
+            progressive_masks.update(progressive_masks_for_this)
+        return progressive_masks
+
+    @staticmethod
+    def update_progressive_masks_scores_order(pre_masks, cur_masks, scores, progressive_step, progressive_configs):
+        """Generate the progressive masks.
+        
+        Args:
+            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
+            cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
+            scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
+            progressive_step: An integer representing the number of current step in progressive pruning.
+            progressive_configs: A dict that stores configurations of progressive pruning.
+        
+        Returns:
+            A dict{"layer_name": Tensor} that stores the masks generated in progressive pruning.
+        """
+        if progressive_configs['use_global']:
+            return ProgressivePatternUtils.update_progressive_masks_global_scores(pre_masks, cur_masks, scores, \
+                    progressive_step, progressive_configs)
+        else:
+            return ProgressivePatternUtils.update_progressive_masks_local_scores(pre_masks, cur_masks, scores, \
+                    progressive_step, progressive_configs)  
+    
+    @staticmethod
+    def update_progressive_masks_linear_order(
+        pre_masks, 
+        cur_masks, 
+        scores, 
+        progressive_step, 
+        progressive_configs: dict, 
+        block_sizes: dict
+    ):
+        """Generate the progressive masks.
+        
+        Args:
+            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
+            cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
+            scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
+            progressive_step: An integer representing the number of current step in progressive pruning.
+            progressive_configs: A dict that stores configurations of progressive pruning.
+            block_size: Dict{"layer_name": List or Tuple} that stores the block sizes, only for NxM patterns.
+        
+        Returns:
+            A dict{"layer_name": Tensor} that stores the masks generated in progressive pruning.
+        """
+        progressive_steps = progressive_configs["progressive_steps"]
+        progressive_masks = {}
+        new_added_masks = ProgressivePatternUtils.update_new_added_masks(pre_masks, cur_masks)
+        for key in pre_masks.keys():
+            block_size = block_sizes[key]
+            new_added_mask = new_added_masks[key]
+            # conv
+            new_added_mask = ProgressivePatternUtils._reshape_orig_to_2dims(new_added_mask)
+            shape = new_added_mask.shape
+            # progressive masks are generated in the direction of block's large dim.
+            if block_size[0] >= block_size[1]:
+                # NxM (N>=M), output channel pruning
+                new_shape = [shape[0] // block_size[0], progressive_steps, block_size[0] // progressive_steps,
+                             shape[1] // block_size[1], block_size[1]]
+                new_added_mask_reshape = new_added_mask.reshape(new_shape)
+                new_added_mask_reshape[:, progressive_step:, :, :, :] = 1.0
+            else:
+                # NxM (N<M), input channel pruning
+                new_shape = [shape[0] // block_size[0], block_size[0], shape[1] // block_size[1],
+                             progressive_steps, block_size[1] // progressive_steps]
+                new_added_mask_reshape = new_added_mask.reshape(new_shape)
+                new_added_mask_reshape[:, :, :, progressive_step:, :] = 1.0
+            new_added_mask = new_added_mask_reshape.reshape(shape)
+            new_added_mask = ProgressivePatternUtils._reshape_2dims_to_orig(new_added_mask, pre_masks[key].shape)
+            progressive_masks[key] = pre_masks[key] * new_added_mask
+        return progressive_masks      
 
 class BasePattern:
     """Pruning Pattern.
 
     It defines the basic pruning unit and how this unit will be pruned during pruning, e.g. 4x1, 2:4.
-
+    
     Args:
         config: A config dict object that contains the pattern information.
         modules: Torch neural network modules to be pruned with the pattern.
 
     Attributes:
-        pattern: A config dict object that includes information of the pattern.
+        pattern: A config dict object that includes information of the pattern.    
         is_global:  A bool determining whether the pruning takes global pruning option.
                     Global pruning means that pruning scores by a pruning criterion are evaluated in all layers.
-                    Local pruning, by contrast, means that pruning scores by the pruning criterion are evaluated
+                    Local pruning, by contrast, means that pruning scores by the pruning criterion are evaluated 
                         in every layer individually.
         keep_mask_layers:A dict that includes the layers whose mask will not be updated.
         invalid_layers: The layers whose shapes don't fit the pattern.
         modules: Torch neural network modules to be pruned with the pattern.
         config: A config dict object that contains all the information including the pattern's.
         max_sparsity_ratio_per_op: A float representing the maximum sparsity that one layer could reach.
         min_sparsity_ratio_per_op: A float representing the minimum sparsity that one layer could reach.
         target_sparsity: A float representing the sparsity ratio of the modules after pruning.
     """
 
-    def __init__(self, config, modules):
+    def __init__(self, config, modules, framework='pytorch'):
         """Initialize the basic pruning unit of a pattern."""
         self.pattern = config.pattern
         self.is_global = config.pruning_scope == "global"
         self.keep_mask_layers = {}
         self.invalid_layers = []
         self.modules = modules
         self.config = config
+        self.framework = framework
         self.max_sparsity_ratio_per_op = self.config['max_sparsity_ratio_per_op']
         self.min_sparsity_ratio_per_op = self.config['min_sparsity_ratio_per_op']
         self.target_sparsity_ratio = self.config['target_sparsity']
+        self.block = bool('block' in self.config['pruning_type'] or 'free' in self.config['pruning_type'])
         # Not using deterministic_algorithms for all examples
-        torch.use_deterministic_algorithms(False)
+        if self.framework == 'pytorch':
+            torch.use_deterministic_algorithms(False)
 
     def reduce_tensor(self, data, dim):
         """Reduce the data along the given dimension.
-
+        
         Args:
             data: The input data.
             dim: The reduced axis.
 
         Returns:
             The reduced tensor.
         """
         name = self.config['criterion_reduce_type']
         if name == "mean":
-            return torch.mean(data, dim=dim)
+            if self.framework == 'pytorch':
+                return torch.mean(data, dim=dim)
+            elif self.framework == 'keras':
+                return tf.math.reduce_mean(data, dim)
         elif name == "sum":
-            return torch.sum(data, dim=dim)
+            if self.framework == 'pytorch':
+                return torch.sum(data, dim=dim)
+            elif self.framework == 'keras':
+                return tf.math.reduce_sum(data, dim)
         elif name == "max":
-            return torch.max(data, dim=dim)[0]
+            if self.framework == 'pytorch':
+                return torch.max(data, dim=dim)[0]
+            elif self.framework == 'keras':
+                return tf.math.reduce_max(data, dim)
         else:
             assert False, "currently only support mean, sum and max reduce type"
 
     def get_masks(self, scores, target_sparsity_ratio, pre_masks):
         """Generate the weight masks according to the weight score and the current target sparsity ratio.
 
         Args:
             scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
             target_sparsity_ratio: A float representing the sparsity of the modules after pruning.
             pre_masks: A dict{"layer_name": Tensor} that stores the masks generated at last pruning step.
 
         Returns:
-            A dict with the identical size as pre_masks and its 0/1 values are updated.
+            A dict with the identical size as pre_masks and its 0/1 values are updated. 
                 1 means unpruned and 0 means pruned.
         """
         if self.is_global:
             return self.get_masks_global(scores, target_sparsity_ratio, pre_masks)
         else:
             return self.get_masks_local(scores, target_sparsity_ratio, pre_masks)
 
     def get_masks_global(self, scores, target_sparsity_ratio, pre_masks):
         """Generate the weight masks for global pruning, please refer to function get_masks for more information."""
         raise NotImplementedError
 
     def get_masks_local(self, scores, target_sparsity_ratio, pre_masks):
         """Generate the weight masks for local pruning.
-
+        
         Args:
             scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
             target_sparsity_ratio: A float. After pruning, the sparsity of the modules will reach this value.
             pre_masks: A dict{"layer_name": Tensor}. The previous masks generated at the last pruning step.
 
         Returns:
-            A dict with the identical size as pre_masks and its 0/1 values are updated.
+            A dict with the identical size as pre_masks and its 0/1 values are updated. 
                 1 means unpruned and 0 means pruned.
         """
         masks = {}
         if isinstance(self, PatternNxM) and not isinstance(self.block_size, dict):
             self.block_size = self.get_block_size_dict(pre_masks)
         for key in scores.keys():
             score = {key: scores[key]}
@@ -179,30 +417,42 @@
         Args:
             score: A Tensor representing the pruning scores of each weight elements.
             exact_sparsity_ratio: A float representing the layer's final sparsity ratio.
 
         Returns:
             A Tensor with the identical size as score. a new mask.
         """
-        flattern_score = torch.flatten(score)
-        k = int(exact_sparsity_ratio * flattern_score.numel())
-        threshold, _ = torch.kthvalue(flattern_score, k)
-        if not k < 1:
-            zero = torch.tensor([0.]).to(score.device)
-            one = torch.tensor([1.]).to(score.device)
-            mask = torch.where(score <= threshold, zero, one)
-        else:
-            mask = torch.ones(score.shape, device=score.device)
+        if self.framework == 'pytorch':
+            flattern_score = torch.flatten(score)
+            k = int(exact_sparsity_ratio * flattern_score.numel())
+            threshold, _ = torch.kthvalue(flattern_score, k)
+            if not k < 1:
+                zero = torch.tensor([0.]).to(score.device)
+                one = torch.tensor([1.]).to(score.device)
+                mask = torch.where(score <= threshold, zero, one)
+            else:
+                mask = torch.ones(score.shape, device=score.device)
+        elif self.framework == 'keras':
+            flattern_score = tf.reshape(score, [-1]).numpy()
+            k = int(exact_sparsity_ratio * flattern_score.size)
+            threshold = np.partition(flattern_score, kth=k)[k]
+            if not k < 1:
+                zero = tf.convert_to_tensor([0.])
+                one = tf.convert_to_tensor([1.])
+                mask = tf.where(score <= threshold, zero, one)
+            else:
+                mask = tf.ones_like(score.shape)
+
         return mask
 
     def get_block_size_dict(self, data):
         """Get pattern size for each module.
-
+        
         This is mainly for per-channel pruning when each module has different pruning size.
-
+        
         Args:
             data: the input data.
 
         Returns:
             To be implemented in subclasses.
         """
         raise NotImplementedError
@@ -218,20 +468,42 @@
             A float representing the zero elements' ratio in pre_masks.
         """
         zero_cnt = 0 # This function might need to refactor in subclass.
         total_cnt = 0
         for key in pre_masks.keys():
             pre_mask = pre_masks[key]
             zero_cnt += torch.sum(pre_mask == 0.0).data.item()
-            total_cnt += pre_masks[key].numel()  ##FIXME
+            total_cnt += pre_mask.numel()  ##FIXME
+
         if return_dict:
             return {"sparsity_ratio": float(zero_cnt) / total_cnt, "zero_cnt": zero_cnt, "total_cnt": total_cnt}
         else:
             return float(zero_cnt) / total_cnt
 
+    def get_sparsity_ratio_progressive(self, pre_masks, return_dict=False):
+        """Calculate the sparsity ratio of each layer.
+        
+        Args:
+            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
+            return_dict: A bool determining whether to return more information like zero_cnt and total_cnt.
+        
+        Returns:
+            A float representing the zero elements' ratio in pre_masks.
+        """
+        zero_cnt = 0
+        total_cnt = 0
+        for key in pre_masks.keys():
+            if key in self.invalid_layers:
+                continue
+            # progressive masks are unstructured, therefore directly find zeros
+            zero_cnt += float(torch.sum(pre_masks[key] == 0).data.item())
+            total_cnt += float(pre_masks[key].numel())
+
+        return (zero_cnt / total_cnt)
+        
     def get_pattern_lock_masks(self, modules):
         """Obtain masks from original weight map according the pattern and weights' zero positions.
 
         Args:
             modules: a dict {'layer_name': Tensor} that stores weights.
 
         Returns:
@@ -240,75 +512,93 @@
         pattern_lock_masks = {}
         for key in modules.keys():
             weight = modules[key].weight
             shape = weight.shape
             mask = torch.ones(shape)
             mask[weight == 0] = 0.0
             pattern_lock_masks[key] = mask.to(weight.device)
+
         return pattern_lock_masks
 
     def check_layer_validity(self):
         """Check if a layer is valid for this block_size."""
         pass
 
     def get_reduced_masks_from_data(self, data, key):
         """Obtain the unpruned weights and reshape according to the block_size."""
         raise NotImplementedError
 
     def update_residual_cnt(self, masks, target_sparsity_ratio):
         """Update the number of parameters yet to be pruned.
-
+        
         Args:
             masks: the current pruning mask.
             target_sparsity_ratio: A float representing the final sparsity of the modules.
 
         Returns:
             An int representing the number of weights left to be pruned to reach the target sparsity ratio.
         """
         self.total_params_cnt = self.get_sparsity_ratio(masks, return_dict=True)["total_cnt"]
         to_prune_cnt = int(self.total_params_cnt * target_sparsity_ratio)
         for key in masks.keys():
             if self.keep_mask_layers.get(key, False):
                 zero_cnt = self.get_sparsity_ratio({key: masks[key]}, return_dict=True)["zero_cnt"]
                 to_prune_cnt -= zero_cnt
-
+                
         return to_prune_cnt
 
     def get_sparsity_ratio_each_layer(self, masks):
         """Calculate the sparsity ratio of each layer.
-
+        
         Args:
             masks: The current weight masks.
 
         Returns:
             infos: the sparsity information for each layer including sparsity_ratio, zero_point and total cnts.
             SparsityInfo: the sparsity information for the model.
         """
         infos = {}
         zero_cnts = 0
         total_cnts = 0
-        for key in masks.keys():
-            if key in self.invalid_layers:
-                continue
-            reduced_mask = self.get_reduced_masks_from_data(masks[key], key)
-            zero_cnt = (int(torch.sum(reduced_mask == 0.0).data.item()))
-            total_cnt = int(reduced_mask.numel())
-            sparsity_ratio = float(zero_cnt) / total_cnt
-            val = SparsityInfo(zero_cnt, total_cnt, sparsity_ratio)
-            infos[key] = val
-            zero_cnts += zero_cnt
-            total_cnts += total_cnt
+
+        if self.framework == 'pytorch':
+            for key in masks.keys():
+                if key in self.invalid_layers:
+                    continue
+                reduced_mask = masks[key] if self.block else self.get_reduced_masks_from_data(masks[key], key)
+                zero_cnt = (int(torch.sum(reduced_mask == 0.0).data.item()))
+                total_cnt = int(reduced_mask.numel())
+                sparsity_ratio = float(zero_cnt) / total_cnt
+                val = SparsityInfo(zero_cnt, total_cnt, sparsity_ratio)
+                infos[key] = val
+                zero_cnts += zero_cnt
+                total_cnts += total_cnt
+        elif self.framework == 'keras':
+            for key in masks.keys():
+                if key in self.invalid_layers:
+                    continue
+                if not isinstance(masks[key], np.ndarray):
+                    masks[key] = masks[key].numpy()
+                reduced_mask = masks[key] if self.block else self.get_reduced_masks_from_data(masks[key], key)
+                zero_cnt = int(np.sum(reduced_mask == 0.0))
+                total_cnt = int(reduced_mask.size)
+                sparsity_ratio = float(zero_cnt) / total_cnt
+                val = SparsityInfo(zero_cnt, total_cnt, sparsity_ratio)
+                infos[key] = val
+                zero_cnts += zero_cnt
+                total_cnts += total_cnt
+
         sparsity_ratio = float(zero_cnts) / total_cnts
         return infos, SparsityInfo(zero_cnts, total_cnts, sparsity_ratio)
 
     def adjust_ratio(self, masks: dict, layer_name: str, key_new_sparsity: SparsityInfo,
                      max_sparsity_ratio: float, min_sparsity_ratio: float, \
                      final_target_sparsity_ratio: float):
         """Adjust the sparsity of a layer based on threshold.
-
+        
         Args:
             masks: The weight masks.
             layer_name: The layer to be examined.
             key_new_sparsity: The proposed ratio for the layer.
             max_sparsity_ratio: A float representing the maximum sparsity that one layer could reach.
             min_sparsity_ratio: A float representing the minimum sparsity that one layer could reach.
             final_target_sparsity_ratio: The final target sparsity ratio.
@@ -369,30 +659,30 @@
             adjust_sparsity_ratio = new_sparsity_ratio
             return True, adjust_sparsity_ratio
 
 
 @register_pattern('NxM')
 class PatternNxM(BasePattern):
     """Pruning Pattern.
-
+    
     A Pattern class derived from BasePattern. In this pattern, the weights in a NxM block will be pruned or kept
     during one pruning step.
-
+    
     Args:
         config: A config dict object that contains the pattern information.
-
+        
     Attributes:
             block_size: A list of two integers representing the height and width of the block.
             Please note that the vertical direction of a Linear layer's weight refers to the output channel.
                 because PyTorch's tensor matmul has a hidden transpose operation.
     """
 
-    def __init__(self, config, modules):
+    def __init__(self, config, modules, framework='pytorch'):
         """Initialize the basic pruning unit of NXM pattern."""
-        super(PatternNxM, self).__init__(config, modules)
+        super(PatternNxM, self).__init__(config, modules, framework)
         pattern = self.pattern.split('_')[-1]
         self.N = pattern.split('x')[0]
         self.M = pattern.split('x')[1]
         if self.N == "channel":  ##channel-wise pruning mode
             self.block_size = ["channel", int(self.M)]
         elif self.M == "channel":  ##channel-wise pruning mode
             self.block_size = [int(self.N), "channel"]
@@ -401,59 +691,69 @@
         self.total_params_cnt = -1
 
         self.block_size = self.get_block_size_dict()
         self.check_layer_validity()
 
     def get_block_size_dict(self):
         """Calulate the zero elements' ration in pre_masks.
-
+        
         Args:
             data: Dict{"layer_name": Tensor} that stores weights or scores.
-
+            
         Returns:
             A dict. Dict{"layer_name": [block_size_1, block_size_2]} containing block shapes of each layer.
                 In channel-wise pruning different layers can have different pruning patterns.
         """
-        data = self.modules
+        datas = self.modules
         block_sizes_dict = {}
-        if self.N == "channel" or self.M == "channel":
-            for key in data.keys():
-                if isinstance(data[key], torch.nn.Module):
-                    shape = data[key].weight.shape
-                else:
-                    shape = data[key].shape
-                if self.N == "channel":
-                    block_sizes_dict[key] = [shape[0], 1]
-                else:
-                    block_sizes_dict[key] = [1, shape[1]]
-            return block_sizes_dict
-        for key in data.keys():
+        for key in datas.keys():
             block_sizes_dict[key] = self.block_size
+            if not (self.N == "channel" or self.M == "channel"):
+                continue
+            if self.framework == 'pytorch' and isinstance(datas[key], torch.nn.Module):
+                shape = datas[key].weight.shape
+            else:
+                shape = datas[key].shape
+            if self.N == "channel": # support "channelxM" format
+                block_sizes_dict[key] = [shape[0], self.block_size[1]]
+            if self.M == "channel":
+                block_sizes_dict[key] = [self.block_size[0], shape[1]]
+                
         return block_sizes_dict
 
     def check_layer_validity(self):
         """Check if a layer is valid for this block_size."""
         block_sizes = self.block_size
         datas = self.modules
-        for key in datas.keys():
-            data = datas[key].weight
-            data = self._reshape_orig_to_2dims(data)
-            shape = data.shape
-            block_size = block_sizes[key]
-            if shape[0] % block_size[0] != 0 or shape[1] % block_size[1] != 0:  ## only consider input channel
-                self.invalid_layers.append(key)
-                logger.warning(f"{key} shape {data.shape} cannot be divided by {self.pattern}")
+        if self.framework == 'pytorch':
+            for key in datas.keys():
+                data = datas[key].weight
+                data = self._reshape_orig_to_2dims(data)
+                shape = data.shape
+                block_size = block_sizes[key]
+                if shape[0] % block_size[0] != 0 or shape[1] % block_size[1] != 0:  ## only consider input channel
+                    self.invalid_layers.append(key)
+                    logger.warning(f"{key} shape {data.shape} cannot be divided by {self.pattern}")
+        elif self.framework == 'keras':
+            for key in datas.keys():
+                data = datas[key].get_weights()[0]
+                data = self._reshape_orig_to_2dims(data)
+                shape = data.shape
+                block_size = block_sizes[key]
+                if shape[0] % block_size[0] != 0 or shape[1] % block_size[1] != 0:  ## only consider input channel
+                    self.invalid_layers.append(key)
+                    logger.warning(f"{key} shape {data.shape} cannot be divided by {self.pattern}")
 
     def get_reduced_masks_from_data(self, data, key):
         """Obtain the unpruned weights and reshape according to the block_size.
-
+        
         Args:
             data: Input.
             key: The layer name.
-
+        
         Returns:
             The unpruned weights.
         """
         assert key not in self.invalid_layers
         block_size = self.block_size[key]
         data = self._reshape_orig_to_2dims(data)
         shape = data.shape
@@ -471,87 +771,83 @@
             return_dict: A bool determining whether to return more information like zero_cnt and total_cnt.
 
         Returns:
             A float representing the zero elements' ratio in pre_masks.
         """
         zero_cnt = 0
         total_cnt = 0
-        for key in pre_masks.keys():
-            if key in self.invalid_layers:
-                continue
-            reduced_mask = self.get_reduced_masks_from_data(pre_masks[key], key)
-            zero_cnt += (int(torch.sum(reduced_mask == 0.0).data.item()))
-            total_cnt += int(reduced_mask.numel())
+        if self.framework == 'pytorch':
+            for key in pre_masks.keys():
+                if key in self.invalid_layers:
+                    continue
+                reduced_mask = pre_masks[key] if self.block else self.get_reduced_masks_from_data(pre_masks[key], key)
+                zero_cnt += (int(torch.sum(reduced_mask == 0.0).data.item()))
+                total_cnt += int(reduced_mask.numel())
+        elif self.framework == 'keras':
+            for key in pre_masks.keys():
+                if key in self.invalid_layers:
+                    continue
+                if not isinstance(pre_masks[key], np.ndarray):
+                    pre_masks[key] = pre_masks[key].numpy()
+                reduced_mask = pre_masks[key] if self.block else self.get_reduced_masks_from_data(pre_masks[key], key)
+                zero_cnt += int(np.sum(reduced_mask == 0.0))
+                total_cnt += int(reduced_mask.size)
         if total_cnt == 0:
             sparsity_ratio = 0.0
         else:
             sparsity_ratio = float(zero_cnt) / total_cnt
         if return_dict:
             return {"sparsity_ratio": sparsity_ratio, "zero_cnt": zero_cnt, "total_cnt": total_cnt}
         else:
             return sparsity_ratio
 
-    def get_sparsity_ratio_progressive(self, pre_masks, return_dict=False):
-        """Calculate the sparsity ratio of each layer.
-
-        Args:
-            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
-            return_dict: A bool determining whether to return more information like zero_cnt and total_cnt.
-
-        Returns:
-            A float representing the zero elements' ratio in pre_masks.
-        """
-        zero_cnt = 0
-        total_cnt = 0
-        for key in pre_masks.keys():
-            if key in self.invalid_layers:
-                continue
-            # progressive masks are unstructured, therefore directly find zeros
-            zero_cnt += float(torch.sum(pre_masks[key] == 0).data.item())
-            total_cnt += float(pre_masks[key].numel())
-        return (zero_cnt / total_cnt)
-
     def _reshape_orig_to_2dims(self, data):
         """Process layers that are not two-dimensional(e.g conv layer).
 
         Args:
             data: Input.
-
+            
         Returns:
             Reshaped data.
         """
         ##TODO need to verify whether it's ok for transposed conv
         if len(data.shape) == 4:
-            data = data.permute(0, 2, 3, 1)  ##cout,k,k,cin
+            if isinstance(data, np.ndarray):
+                data = np.transpose(data, (0, 2, 3, 1))
+            else:
+                data = data.permute(0, 2, 3, 1)  ##cout,k,k,cin
             data = data.reshape(data.shape[0], -1)
         return data
 
     def _reshape_2dims_to_orig(self, data, orig_shape):
         """Recover layers that are not two-dimensional(e.g conv layer).
 
         Args:
             data: Input.
             orig_shape: Target shape.
-
+            
         Returns:
             Reshaped data.
         """
         if len(orig_shape) == 4:
             data = data.reshape(orig_shape[0], orig_shape[2], orig_shape[3],
                                 orig_shape[1])
-            data = data.permute(0, 3, 1, 2)
+            if isinstance(data, np.ndarray):
+                data = np.transpose(data, (0, 3, 1, 2))
+            else:
+                data = data.permute(0, 3, 1, 2)
         return data
 
     def reshape_orig_to_pattern(self, data, key):
-        """Reshape the data(s1,s2) to [s1/N,N,s2,s2/M].
+        """Reshape the data(s1,s2) to [s1/N,N,s2/M,M].
 
         Args:
             data: The input.
             key: The layer name.
-
+            
         Returns:
             Reshaped input tensor.
         """
         block_size = self.block_size[key]
         data = self._reshape_orig_to_2dims(data)
         shape = data.shape
         new_shape = [shape[0] // block_size[0], block_size[0], shape[1] // block_size[1],
@@ -562,26 +858,26 @@
     def reshape_reduced_to_orig(self, data, key, orig_shape):
         """Reshape the data [s1/N,s2/M] to [s1,s2], also permute dims for conv layer.
 
         Args:
             data: Input.
             key: The layer name.
             orig_shape: The original shape of the layer.
-
+            
         Returns:
             Data of its original shape.
         """
         block_size = self.block_size[key]
         data = data.repeat_interleave(block_size[0], dim=0).repeat_interleave(block_size[1], dim=-1)
         data = self._reshape_2dims_to_orig(data, orig_shape)
         return data
 
     def reduce_scores(self, scores):
         """Recalculate the pruning scores after reducing the data.
-
+        
         Args:
             scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
 
         Returns:
             The reduced pruning scores.
         """
         new_scores = {}
@@ -596,54 +892,92 @@
             ##sum or mean is quite different for per channel pruning
             current_score_sum = self.reduce_tensor(self.reduce_tensor(current_score, dim=-1), dim=1)
             new_scores[key] = current_score_sum
         return new_scores
 
     def get_mask_per_threshold(self, score, threshold, block_size):
         """Get the mask per threshold."""
-        zero = torch.tensor([0.]).to(score.device)
-        one = torch.tensor([1.]).to(score.device)
-        mask = torch.where(score <= threshold, zero, one)
-        mask = mask.repeat_interleave(block_size[0], dim=0).repeat_interleave(block_size[1], dim=-1)
+        if self.framework == 'pytorch':
+            zero = torch.tensor([0.]).to(score.device)
+            one = torch.tensor([1.]).to(score.device)
+            mask = torch.where(score <= threshold, zero, one)
+            if not self.block:
+                mask = mask.repeat_interleave(block_size[0], dim=0).repeat_interleave(block_size[1], dim=-1)
+        elif self.framework == 'keras':
+            zero = tf.convert_to_tensor([0.])
+            one = tf.convert_to_tensor([1.])
+            mask = tf.where(score <= threshold, zero, one)
+            if not self.block:
+                mask = tf.repeat(mask, repeats=block_size[0], axis=0)
+                mask = tf.repeat(mask, repeats=block_size[1], axis=-1)
+            mask = mask.numpy()
         return mask
 
     def get_masks_global(self, scores, cur_target_sparsity_ratio, pre_masks,
                          keep_exact_sparsity_ratio=True):
         """Generate masks for layers.
 
         Gather all layer's scores together and calculate a common threshold.
         This threshold will be applied to all layers.
-
+        
         Args:
             scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
             cur_target_sparsity_ratio: A float representing the model's sparsity after pruning.
             pre_masks: A dict{"layer_name": Tensor} that stores the masks generated at the last pruning step.
             max_sparsity_ratio_per_op: A float representing the maximum sparsity that one layer can reach.
             keep_pre_masks: A bool representing if the masks should remain unchanged.
+            
+        Returns:
+            A dict with the identical size as pre_masks and its 0/1 values are updated.
+                1 means unpruned and 0 means pruned.
+        """
+        if self.framework == 'pytorch':
+            return self.get_masks_global_pytorch(scores, cur_target_sparsity_ratio, pre_masks, \
+                                                                        keep_exact_sparsity_ratio)
+        elif self.framework == 'keras':
+            return self.get_masks_global_tf(scores, cur_target_sparsity_ratio, pre_masks, keep_exact_sparsity_ratio)
 
+    def get_masks_global_pytorch(self, scores, cur_target_sparsity_ratio, pre_masks,
+                         keep_exact_sparsity_ratio=True):
+        """Generate masks for layers.
+
+        Gather all layer's scores together and calculate a common threshold.
+        This threshold will be applied to all layers.
+        
+        Args:
+            scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
+            cur_target_sparsity_ratio: A float representing the model's sparsity after pruning.
+            pre_masks: A dict{"layer_name": Tensor} that stores the masks generated at the last pruning step.
+            max_sparsity_ratio_per_op: A float representing the maximum sparsity that one layer can reach.
+            keep_pre_masks: A bool representing if the masks should remain unchanged.
+            
         Returns:
             A dict with the identical size as pre_masks and its 0/1 values are updated.
                 1 means unpruned and 0 means pruned.
         """
         ##keep the masks if the layer exceed max sparsity ratio
 
         masks = pre_masks
-
         k_blockwise = self.update_residual_cnt(masks, cur_target_sparsity_ratio)
         if k_blockwise <= 0:
             return masks
-        new_scores = self.reduce_scores(scores)
-        global_scores = torch.cat([torch.flatten(v) for v in new_scores.values()])
+        new_scores = scores if self.block else self.reduce_scores(scores)
+        not_exceed_layers = []
         residual_k = k_blockwise
-        not_exceed_layers = [key for key in new_scores.keys()]
         if self.min_sparsity_ratio_per_op > 0:
             sparsity_infos_perlayer, _ = self.get_sparsity_ratio_each_layer(masks)
 
         while True:
+            new_not_exceed_layers = [key for key in new_scores.keys() if not self.keep_mask_layers.get(key, False)]
+            if not_exceed_layers == new_not_exceed_layers or len(new_not_exceed_layers) == 0:
+                break
+            not_exceed_layers = new_not_exceed_layers
+            global_scores = torch.cat([torch.flatten(new_scores[key]) for key in not_exceed_layers])
             threshold, _ = torch.kthvalue(global_scores, residual_k)
+            
             for key in not_exceed_layers:
                 block_size = self.block_size[key]
                 score = new_scores[key]
                 mask = self.get_mask_per_threshold(score, threshold, block_size)
                 info = self.get_sparsity_ratio({key: mask}, return_dict=True)
                 zero_cnt = info["zero_cnt"]
                 total_cnt = info["total_cnt"]
@@ -653,307 +987,231 @@
                                                               self.max_sparsity_ratio_per_op,
                                                               self.min_sparsity_ratio_per_op,
                                                               self.target_sparsity_ratio)
                 if need_adjust:
                     # uptade status
                     self.keep_mask_layers[key] = True
                     masks[key] = self.get_single_mask_per_target_ratio(new_scores[key], adjust_ratio)
-                    masks[key] = masks[key].repeat_interleave(block_size[0], 0).repeat_interleave(block_size[1], -1)
+                    if not self.block:
+                        masks[key] = masks[key].repeat_interleave(block_size[0], 0).repeat_interleave(block_size[1], -1)
                     if keep_exact_sparsity_ratio:
                         zero_cnt = self.get_sparsity_ratio({key: masks[key]}, return_dict=True)["zero_cnt"]
                         residual_k -= zero_cnt
                 else:
                     masks[key] = mask
             if not keep_exact_sparsity_ratio:
                 break
+
+        for key in masks.keys():
+            if key in self.invalid_layers:
+                continue
+            if len(scores[key].shape) == 4:  ## need to permute
+                mask = masks[key]
+                orig_shape = scores[key].shape
+                mask = self._reshape_2dims_to_orig(mask, orig_shape)
+                masks[key] = mask
+            layer_ratio = torch.sum(masks[key] == 0.0).data.item() / masks[key].numel()
+            logger.info(f'{key} sparsity is {layer_ratio}')
+        return masks
+
+    def get_masks_global_tf(self, scores, cur_target_sparsity_ratio, pre_masks,
+                         keep_exact_sparsity_ratio=True):
+        """Generate masks for layers.
+
+        Gather all layer's scores together and calculate a common threshold.
+        This threshold will be applied to all layers.
+        
+        Args:
+            scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
+            cur_target_sparsity_ratio: A float representing the model's sparsity after pruning.
+            pre_masks: A dict{"layer_name": Tensor} that stores the masks generated at the last pruning step.
+            max_sparsity_ratio_per_op: A float representing the maximum sparsity that one layer can reach.
+            keep_pre_masks: A bool representing if the masks should remain unchanged.
+            
+        Returns:
+            A dict with the identical size as pre_masks and its 0/1 values are updated.
+                1 means unpruned and 0 means pruned.
+        """
+        ##keep the masks if the layer exceed max sparsity ratio
+
+        masks = pre_masks
+        k_blockwise = self.update_residual_cnt(masks, cur_target_sparsity_ratio)
+        if k_blockwise <= 0:
+            return masks
+        new_scores = scores if self.block else self.reduce_scores(scores)
+        not_exceed_layers = []
+        residual_k = k_blockwise
+        if self.min_sparsity_ratio_per_op > 0:
+            sparsity_infos_perlayer, _ = self.get_sparsity_ratio_each_layer(masks)
+
+        while True:
             new_not_exceed_layers = [key for key in new_scores.keys() if not self.keep_mask_layers.get(key, False)]
             if not_exceed_layers == new_not_exceed_layers or len(new_not_exceed_layers) == 0:
                 break
             not_exceed_layers = new_not_exceed_layers
-            global_scores = torch.cat([torch.flatten(new_scores[key]) for key in not_exceed_layers])
+            global_scores = np.concatenate([tf.reshape(new_scores[key], [-1]).numpy() for key in not_exceed_layers])
+            threshold = np.partition(global_scores, kth=residual_k)[residual_k]
+            
+            for key in not_exceed_layers:
+                block_size = self.block_size[key]
+                score = new_scores[key]
+                mask = self.get_mask_per_threshold(score, threshold, block_size)
+                info = self.get_sparsity_ratio({key: mask}, return_dict=True)
+                zero_cnt = info["zero_cnt"]
+                total_cnt = info["total_cnt"]
+                current_sparsity_ratio = float(zero_cnt) / total_cnt
+                key_new_sparsity = SparsityInfo(zero_cnt, total_cnt, current_sparsity_ratio)
+                need_adjust, adjust_ratio = self.adjust_ratio(masks, key, key_new_sparsity,
+                                                              self.max_sparsity_ratio_per_op,
+                                                              self.min_sparsity_ratio_per_op,
+                                                              self.target_sparsity_ratio)
+                if need_adjust:
+                    # uptade status
+                    self.keep_mask_layers[key] = True
+                    masks[key] = self.get_single_mask_per_target_ratio(new_scores[key], adjust_ratio)
+                    if not self.block:
+                        masks[key] = tf.repeat(masks[key], repeats=block_size[0], axis=0)
+                        masks[key] = tf.repeat(masks[key], repeats=block_size[1], axis=-1)
+                    if keep_exact_sparsity_ratio:
+                        zero_cnt = self.get_sparsity_ratio({key: masks[key]}, return_dict=True)["zero_cnt"]
+                        residual_k -= zero_cnt
+                else:
+                    masks[key] = mask
+            if not keep_exact_sparsity_ratio:
+                break
 
         for key in masks.keys():
             if key in self.invalid_layers:
                 continue
             if len(scores[key].shape) == 4:  ## need to permute
                 mask = masks[key]
                 orig_shape = scores[key].shape
                 mask = self._reshape_2dims_to_orig(mask, orig_shape)
                 masks[key] = mask
-            layer_ratio = torch.sum(masks[key] == 0.0).data.item() / masks[key].numel()
+            layer_ratio = np.sum(masks[key] == 0.0) / masks[key].size
             logger.info(f'{key} sparsity is {layer_ratio}')
         return masks
 
     def get_pattern_lock_masks(self, modules):
         """Obtain masks from original weight map by masking the zero-valued weights.
-
+        
         Args:
             modules: A dict{"layer_name": Tensor} that stores weights.
-
+            
         Returns:
             A dict with the identical size as modules, containing pattern lock masks.
         """
         pattern_lock_masks = {}
         for key in modules.keys():
             weight = modules[key].weight
             ori_shape = weight.shape
             if key in self.invalid_layers:
                 mask = torch.ones(weight.shape, device=weight.device)
                 pattern_lock_masks[key] = mask
                 continue
             reduced_mask = self.get_reduced_masks_from_data(weight, key)
             mask = self.reshape_reduced_to_orig(reduced_mask, key, ori_shape)
             pattern_lock_masks[key] = mask
-        return pattern_lock_masks
-
-    # ---------------progressive related--------------------
-    def count_new_masked_cnts(self, new_added_masks):
-        """Count the number of elements to be masked.
 
+        return pattern_lock_masks
+    
+    def register_block_masks(self, modules):
+        """Register the block mask parameters and get the mask gradients.
+        
         Args:
-            new_added_masks: A dict {"layer_name": Tensor} that stores the added masks.
-
+            modules: A dict{"layer_name": Tensor} that stores weights.
+            
         Returns:
-            The number of masked weights.
+            A dict containing block masks.
         """
-        # count how many elements are to masked,
-        new_masked_cnts = 0
-        for key in new_added_masks.keys():
-            new_masked_cnts += torch.nonzero(1 - new_added_masks[key]).size()[0]
-        return new_masked_cnts
-
-    def update_new_added_masks(self, pre_masks, cur_masks):
-        """Obtain the new set-to-zero masks during a pruning procedure.
-
-        Pre_masks, cur_masks should have identical keys bacause they represent the same model.
-
-        Args:
-            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
-            cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
+        masks = {}
+        for key in modules.keys():
+            if key in self.invalid_layers:
+                continue # No corresponding block mask, skip.
+            module = modules[key]
+            weight = module.weight
+            if type(module).__name__ not in ["Linear"]:
+                logger.warning(f"Currently only support Linear block mask pruning," \
+                                f"{type(module).__name__} won't be pruned.")
+                continue
+            block_mask = torch.nn.Parameter(self.get_reduced_masks_from_data(weight, key).to(dtype=weight.dtype))
+            module.register_parameter("block_mask", block_mask)
+            masks[key] = modules[key].block_mask.data
 
-        Returns:
-            A dict {"layer_name": Tensor} that stores the added masks.
-        """
-        # obtain the new set-to-zero mask during a pruning procedure.
-        # pre_masks, cur_masks should have identical keys bacause they stands for one model.
-        new_added_masks = {}
-        for key in pre_masks.keys():
-            pre_mask = pre_masks[key]
-            cur_mask = cur_masks[key]
-            zero = torch.tensor([0.]).to(pre_mask.device)
-            one = torch.tensor([1.]).to(cur_mask.device)
-            new_added_masks[key] = torch.where(pre_mask == cur_mask, one, zero)
-        return new_added_masks
+        return masks
+    
+    def remove_block_masks(self):
+        """Remove the block mask parameters."""
+        for key in self.modules.keys():
+            if hasattr(self.modules[key], 'block_mask'):
+                delattr(self.modules[key], 'block_mask')
+    
+    def mask_block_weights(self, masks):
+        """Achieve weight pruning by multiplying the reshaped weights and block masks."""
+        for key in masks.keys():
+            if key in self.invalid_layers:
+                continue
+            module = self.modules[key]
+            block_size = self.block_size[key]
+            org_shape = module.weight.shape
+            mask = masks[key].data.repeat_interleave(\
+                    block_size[0], dim=0).repeat_interleave(block_size[1], dim=-1).to(module.weight.device)
+            reshaped_weight = self._reshape_orig_to_2dims(module.weight.data) * mask
+            module.weight.data = self._reshape_2dims_to_orig(reshaped_weight, org_shape)
 
     def update_progressive_masks(self, pre_masks, cur_masks, scores, progressive_step, progressive_configs):
         """Generate the progressive masks.
-
+        
         Args:
             pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
             cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
             scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
             progressive_step: An integer representing the number of current step in progressive pruning.
             progressive_configs: A dict that stores configurations of progressive pruning.
-
+        
         Returns:
             A dict{"layer_name": Tensor} that stores the masks generated in progressive pruning.
         """
-        use_global = progressive_configs["use_global"]
-        if use_global:
-            return self.update_progressive_masks_global(pre_masks, cur_masks, scores, \
+        score_or_linear = progressive_configs['progressive_type'] # "scores" or "linear"
+        if score_or_linear == "scores":
+            return ProgressivePatternUtils.update_progressive_masks_scores_order(pre_masks, cur_masks, scores, \
                                                         progressive_step, progressive_configs)
-        else:
-            return self.update_progressive_masks_local(pre_masks, cur_masks, scores, \
-                                                       progressive_step, progressive_configs)
-
-    def update_progressive_masks_linear(self, pre_masks, cur_masks, progressive_step, progressive_configs):
-        """Generate the progressive masks along the block's larger dimension.
-
-        Args:
-            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
-            cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
-            progressive_step: An integer representing the number of current step in progressive pruning.
-            progressive_configs: A dict that stores configurations of progressive pruning.
-
-        Returns:
-            A dict{"layer_name": Tensor} that stores the masks generated in progressive pruning.
-        """
-        progressive_steps = progressive_configs["progressive_steps"]
-        progressive_masks = {}
-        new_added_masks = self.update_new_added_masks(pre_masks, cur_masks)
-        for key in pre_masks.keys():
-            block_size = self.block_size[key]
-            new_added_mask = new_added_masks[key]
-            # conv
-            new_added_mask = self._reshape_orig_to_2dims(new_added_mask)
-            shape = new_added_mask.shape
-            # progressive masks are generated in the direction of block's large dim.
-            if block_size[0] >= block_size[1]:
-                # NxM (N>=M), output channel pruning
-                new_shape = [shape[0] // block_size[0], progressive_steps, block_size[0] // progressive_steps,
-                             shape[1] // block_size[1], block_size[1]]
-                new_added_mask_reshape = new_added_mask.reshape(new_shape)
-                new_added_mask_reshape[:, progressive_step:, :, :, :] = 1.0
-            else:
-                # NxM (N<M), input channel pruning
-                new_shape = [shape[0] // block_size[0], block_size[0], shape[1] // block_size[1],
-                             progressive_steps, block_size[1] // progressive_steps]
-                new_added_mask_reshape = new_added_mask.reshape(new_shape)
-                new_added_mask_reshape[:, :, :, progressive_step:, :] = 1.0
-            new_added_mask = new_added_mask_reshape.reshape(shape)
-            new_added_mask = self._reshape_2dims_to_orig(new_added_mask, pre_masks[key].shape)
-            progressive_masks[key] = pre_masks[key] * new_added_mask
-        return progressive_masks
-
-    def update_progressive_masks_scores(self, pre_masks, cur_masks, scores, progressive_step, progressive_configs):
-        """Generate the progressive masks based on scores.
-
-        Args:
-            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
-            cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
-            scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
-            progressive_step: An integer representing the number of current step in progressive pruning.
-            progressive_configs: A dict that stores configurations of progressive pruning.
-
-        Returns:
-            A dict{"layer_name": Tensor} that stores the masks generated in progressive pruning.
-        """
-        progressive_steps = progressive_configs["progressive_steps"]
-        progressive_masks = {}
-        new_added_masks = self.update_new_added_masks(pre_masks, cur_masks)
-        for key in scores.keys():
-            block_size = self.block_size[key]
-            mask_num_each_block = progressive_step * int((block_size[0] * block_size[1]) / progressive_steps)
-            new_added_filter = 1 - new_added_masks[key]
-            score = scores[key]
-            score_masked = (score * new_added_filter).abs()
-            score_masked = self._reshape_orig_to_2dims(score_masked)
-
-            # similar to n:m type
-            # generate progressive masks from scores
-            shape = score_masked.shape
-            new_shape = [shape[0] // block_size[0], block_size[0], shape[1] // block_size[1], block_size[1]]
-            score_masked_new = score_masked.clone()
-            score_masked_new_shape = score_masked_new.reshape(new_shape)
-            score_masked_new_shape = score_masked_new_shape.permute(0, 2, 1, 3)
-            score_masked_new_flatten = torch.flatten(score_masked_new_shape, start_dim=2, end_dim=3)
-            threshold, _ = torch.kthvalue(score_masked_new_flatten, mask_num_each_block, dim=2)
-            threshold = threshold.unsqueeze(-1)
-            threshold = threshold.repeat(1, 1, (block_size[0] * block_size[1]))
-            threshold = threshold.reshape([threshold.shape[0], threshold.shape[1], block_size[0], block_size[1]])
-            threshold = threshold.permute(0, 2, 1, 3)
-            threshold = threshold.reshape((shape[0], shape[1]))
-            one = torch.tensor([1.]).to(score.device)
-            zero = torch.tensor([0.]).to(score.device)
-            mask = torch.where(score_masked <= threshold, zero, one)
-
-            mask = self._reshape_2dims_to_orig(mask, pre_masks[key].shape)
-            progressive_mask = pre_masks[key] * (mask + new_added_masks[key])
-            progressive_masks[key] = torch.where(progressive_mask == 0, zero, one)  # binary
-        return progressive_masks
-
-    def update_progressive_masks_local(self, pre_masks, cur_masks, scores, progressive_step, progressive_configs):
-        """Generate progressive masks in a local pruning domain.
-
-        Args:
-            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
-            cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
-            scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
-            progressive_step: An integer representing the number of current step in progressive pruning.
-            progressive_configs: A dict that stores configurations of progressive pruning.
-
-        Returns:
-            A dict{"layer_name": Tensor} that stores the masks generated in progressive pruning.
-        """
-        progressive_type = progressive_configs["progressive_type"]
-        if progressive_type == "linear":
-            progressive_masks = self.update_progressive_masks_linear(pre_masks, cur_masks, \
-                                                                     progressive_step, progressive_configs)
-        elif progressive_type == "scores":
-            progressive_masks = self.update_progressive_masks_scores(pre_masks, cur_masks, scores, \
-                                                                     progressive_step, progressive_configs)
+        elif score_or_linear == "linear":
+            return ProgressivePatternUtils.update_progressive_masks_linear_order(pre_masks, cur_masks, scores, \
+                                                        progressive_step, progressive_configs, self.block_size)
         else:
             raise NotImplementedError
-        return progressive_masks
-
-    def update_progressive_masks_global(self, pre_masks, cur_masks, scores, progressive_step, progressive_configs):
-        """Gather all layer's scores to obtain a threshold that would be applied to all layers.
-
-        Args:
-            pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
-            cur_masks: Dict{"layer_name": Tensor} that stores the current masks.
-            scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
-            progressive_step: An integer representing the number of current step in progressive pruning.
-            progressive_configs: A dict that stores configurations of progressive pruning.
-
-        Returns:
-            A dict{"layer_name": Tensor} that stores the masks generated in progressive pruning.
-        """
-        progressive_steps = progressive_configs["progressive_steps"]
-        progressive_masks = {}
-        global_new_added_score_list = []
-        new_added_masks = self.update_new_added_masks(pre_masks, cur_masks)
-        new_added_masks_cnts = self.count_new_masked_cnts(new_added_masks)
-        kth_masked_position = (new_added_masks_cnts * progressive_step) // progressive_steps
-        for key in scores.keys():
-            block_size = self.block_size[key]
-            mask_num_each_block = progressive_step * int((block_size[0] * block_size[1]) // progressive_steps)
-            new_added_filter = 1 - new_added_masks[key]
-            new_added_cnts = torch.nonzero(new_added_filter).size()[0]
-            score = scores[key]
-
-            score_masked = (score * new_added_filter).abs()
-            score_masked_row, _ = torch.sort(score_masked.flatten(), descending=True)
-            score_masked_row = score_masked_row[:new_added_cnts]
-            global_new_added_score_list.append(score_masked_row)
-
-        global_new_added_scores = torch.cat(global_new_added_score_list, dim=0)
-        if global_new_added_scores.size()[0] == 0:
-            # an empty tensor, at target sparsity is 0 situation
-            return pre_masks
-        threshold, _ = torch.kthvalue(global_new_added_scores, kth_masked_position, dim=0)
-        for key in scores.keys():
-            new_added_mask = new_added_masks[key]
-            score = scores[key]
-            new_added_filter = 1 - new_added_mask
-            score_masked = (score * new_added_filter).abs()
-            zero = torch.tensor([0.]).to(score.device)
-            one = torch.tensor([1.]).to(score.device)
-            progressive_mask = (new_added_mask + torch.where(score_masked <= threshold, zero, one)) * pre_masks[key]
-            progressive_masks[key] = progressive_mask
-        return progressive_masks
-
 
 @register_pattern('N:M')
 class PatternNInM(BasePattern):
     """Pruning Pattern.
-
+    
     A Pattern class derived from Pattern. In this pattern, N out of every M continuous weights will be pruned.
     For more info of this pattern, please refer to :
     https://github.com/intel/neural-compressor/blob/master/docs/sparsity.md
-
+    
     Args:
         config: A config dict object that contains the pattern information.
-
+        
     Attributes:
         N: The number of elements to be pruned in a weight sequence.
         M: The size of the weight sequence.
     """
 
-    def __init__(self, config, modules):
+    def __init__(self, config, modules, framework='pytorch'):
         """Initialize the basic pruning unit of N:M pattern."""
-        super(PatternNInM, self).__init__(config, modules)
+        super(PatternNInM, self).__init__(config, modules, framework)
         pattern = self.pattern.split('_')[-1]
         self.N = int(pattern.split(':')[0])
         self.M = int(pattern.split(':')[1])  ##m is bigger
         self.check_layer_validity(self.modules, (self.N, self.M))
 
     def check_layer_validity(self, datas: dict, block_size: tuple):
         """Check if a layer is valid for this block_size.
-
+        
         Args:
             datas: A dict object containing the weights for all layers.
             block_size: A tuple representing the size of the pattern block.
         """
         self.invalid_layers = []
         for key in datas.keys():
             data = datas[key].weight
@@ -961,15 +1219,15 @@
             shape = data.shape
             if shape[1] % block_size[1] != 0:
                 self.invalid_layers.append(key)
                 logger.warning(f"{key} shape {shape} cannot be divided by {self.pattern}")
 
     def get_reduced_masks_from_data(self, data, key):
         """Obtain the unpruned weights and reshape according to the block_size.
-
+        
         Args:
             data: Input.
             key: The layer name.
 
         Returns:
             A tensor representing the unpruned weights.
         """
@@ -981,20 +1239,20 @@
         data = data.reshape(new_shape)
         nonzeros = torch.count_nonzero(data, dim=-1)
         reduced_mask = nonzeros > N
         return reduced_mask
 
     def get_least_ninm_mask_from_data(self, score):
         """Generate the least N scores in M.
-
+        
         Args:
             score: the pruning scores of weights.
 
         Returns:
-            A dict with the identical size as pre_masks and its 0/1 values are updated.
+            A dict with the identical size as pre_masks and its 0/1 values are updated. 
                 1 means unpruned and 0 means pruned.
         """
         current_score = score
         M = self.M
         N = self.N
         current_score = self._reshape_orig_to_2dims(current_score)
         shape = current_score.shape
@@ -1012,19 +1270,19 @@
         mask = torch.where(current_score <= threshold, zero, one)
         return mask
 
     def get_sparsity_ratio(self, pre_masks, return_dict=False):
         """Please note that the zero cnt and total cnt are all block_wise for supporting channel-wise pruning.
 
         The return sparsity ratio is elementwised.
-
+        
         Args:
             pre_masks: Dict{"layer_name": Tensor} that stores the masks generated after the last pruning step.
             return_dict: A bool determining whether to return more information like zero_cnt and total_cnt.
-
+            
         Returns:
             An elementwise sparisty ratio.
         """
         zero_cnt = 0
         total_cnt = 0
         for key in pre_masks.keys():
             if key in self.invalid_layers:
@@ -1042,70 +1300,70 @@
             return sparsity_ratio
 
     def _reshape_orig_to_2dims(self, data):
         """Process layers that are not two-dimensional(e.g conv layer).
 
         Args:
             data: Input.
-
+            
         Returns:
             Reshaped data.
         """
         if len(data.shape) == 4:  ##TODO need to verify whether it's ok for transposed conv
             data = data.permute(0, 2, 3, 1)  ##cout,k,k,cin
             data = data.reshape(data.shape[0], -1)
         return data
 
     def _reshape_2dims_to_orig(self, data, orig_shape):
         """Recover layers that are not two-dimensional(e.g conv layer).
 
         Args:
             data: Input.
-
+            
         Returns:
             Reshaped data.
         """
         if len(orig_shape) == 4:
             data = data.reshape(orig_shape[0], orig_shape[2], orig_shape[3], orig_shape[1])
             data = data.permute(0, 3, 1, 2)
         return data
 
     def reshape_orig_to_pattern(self, data, key):
         """Reshape the data based on the pruning pattern.
-
+        
         Args:
             data: Input.
             key: layer name.
-
+        
         Returns:
             Reshaped data.
         """
         data = self._reshape_orig_to_2dims(data)
         shape = data.shape
         new_shape = [shape[0], shape[1] // self.M, self.M]
         data = data.reshape(new_shape)
         return data
 
     def reshape_reduced_to_orig(self, data, key, orig_shape):
         """Reshape the reduced data to its original shape.
-
+        
         Args:
             data: Input.
             key: The layer name.
             orig_shape: The original shape of the layer.
-
+            
         Returns:
             Data of its original shape.
         """
         data = data.repeat_interleave(self.M, dim=-1)
         return self._reshape_2dims_to_orig(data, orig_shape)
 
     def reduce_scores(self, scores):
         """Calculate the pruning scores after reducing the data and obtain the least N scores in M.
-
+        
         Args:
             scores: Pruning scores of weights.
 
         Returns:
             Updated pruning scores and the least N scores in M.
         """
         ##to get the least N scores in M
@@ -1135,15 +1393,15 @@
         """Get the elementwise mask per threshold.
 
         Args:
             score: A tensor that stores the pruning scores of weights.
             threshold: A float used to determine whether to prune a weight.
             block_size: A list of two integers representing the height and width of the block.
             least_m_in_m_masks: A tensor representing the least N scores in M.
-
+            
         Returns:
             mask: The elementwise pruning mask.
         """
         zero = torch.tensor([0.]).to(score.device)
         one = torch.tensor([1.]).to(score.device)
         mask = torch.where(score <= threshold, zero, one)
         mask = mask.repeat_interleave(block_size[1], dim=-1)
@@ -1151,26 +1409,26 @@
         mask = (mask + least_ninm_mask)
         mask = torch.where(mask <= 0, zero, one)
         return mask
 
     def get_masks_global(self, scores, cur_target_sparsity_ratio, pre_masks,
                          keep_exact_sparsity_ratio=True):
         """Generate masks for layers.
-
+        
         Gather all layer's scores together and calculate a common threshold.
         This threshold will be applied for all layers.
-
+        
         Args:
             scores: A dict{"layer_name": Tensor} that stores the pruning scores of weights.
             target_sparsity_ratio: A float representing the model's final sparsity.
             pre_masks: A dict{"layer_name": Tensor} representing the masks generated after the last pruning step.
             max_sparsity_ratio_per_op: A float representing the maximum sparsity that one layer can reach.
-
+            
         Returns:
-            A dict with the identical size as pre_masks and its 0/1 values are updated.
+            A dict with the identical size as pre_masks and its 0/1 values are updated. 
                 1 means unpruned and 0 means pruned.
         """
         masks = pre_masks
 
         block_sparsity_ratio = cur_target_sparsity_ratio * self.M / self.N
         k_blockwise = self.update_residual_cnt(pre_masks, block_sparsity_ratio)
         if k_blockwise <= 0:
@@ -1227,18 +1485,18 @@
                 masks[key] = mask
             layer_ratio = torch.sum(masks[key] == 0.0).data.item() / masks[key].numel()
             logger.info(f'layer {key} sparsity_ratio is {layer_ratio}')
         return masks
 
     def get_pattern_lock_masks(self, modules):
         """Obtain masks from original weight map, by masking where weights' are zero.
-
+        
         Args:
             modules: A dict{"layer_name": Tensor} that stores weights.
-
+            
         Returns:
             A dict with the identical size as modules, containing pattern lock masks.
         """
         pattern_lock_masks = {}
         for key in modules.keys():
             weight = modules[key].weight
             orig_shape = weight.shape
@@ -1247,7 +1505,50 @@
                 pattern_lock_masks[key] = mask
                 continue
             mask = self.get_least_ninm_mask_from_data(weight)
             mask = self._reshape_2dims_to_orig(mask, orig_shape)
             pattern_lock_masks[key] = mask
         return pattern_lock_masks
 
+    def update_progressive_masks(self, pre_masks, cur_masks, scores, progressive_step, progressive_configs):
+        assert progressive_configs['progressive_type'] == "scores", f"N:M progressive pruning only supports 'scores'."
+        # we only have to handle global score or local score
+        return ProgressivePatternUtils.update_progressive_masks_scores_order(pre_masks, cur_masks, scores, \
+                progressive_step, progressive_configs)
+
+@register_pattern('MHA')
+class PatternMHA(BasePattern):
+    """Pruning Pattern.
+    
+    A Pattern class derived from BasePattern. In this pattern, we calculate head masks for a MHA module
+    For more info of this pattern, please refer to :
+    https://github.com/intel/neural-compressor/blob/master/docs/sparsity.md
+    
+    Args:
+        config: A config dict object that contains the pattern information.
+        
+    Attributes:
+        N: The number of elements to be pruned in a weight sequence.
+        M: The size of the weight sequence.
+    """
+
+    def __init__(self, config, modules = None, framework='pytorch'):
+        self.framework = framework
+        self.is_global = config.pruning_scope == "global"
+    
+    # only implement three method: get_masks, get_masks_local, get_masks_global
+        
+    def get_masks_global(self, scores, target_sparsity_ratio, pre_masks):
+        # gather all score items into one tensor
+        if target_sparsity_ratio <= .0: 
+            return pre_masks
+        flatten_score = torch.cat(list(scores.values())).flatten()
+        k = int(target_sparsity_ratio * flatten_score.numel())
+        if k <= 0:
+            return pre_masks
+        threshold, _ = torch.kthvalue(flatten_score, k)
+        head_masks = {}
+        zero = torch.tensor([0.]).to(threshold.device)
+        one = torch.tensor([1.]).to(threshold.device)
+        for mha_name, mha_score in scores.items():
+            head_masks[mha_name] = torch.where(mha_score <= threshold, zero, one).permute(1, 0)
+        return head_masks
```

### Comparing `neural_compressor-2.1.1/neural_compressor/compression/pruner/regs.py` & `neural_compressor-2.2/neural_compressor/compression/pruner/regs.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/compression/pruner/schedulers.py` & `neural_compressor-2.2/neural_compressor/compression/pruner/schedulers.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/compression/pruner/utils.py` & `neural_compressor-2.2/neural_compressor/compression/pruner/utils.py`

 * *Files 18% similar despite different names*

```diff
@@ -14,31 +14,36 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import re
 import yaml
+import numpy as np
+from ...config import WeightPruningConfig as WeightPruningConf
 
 try:
-    from ...config import WeightPruningConfig
+    from ...conf.pythonic_config import WeightPruningConfig
     from ...conf.config import PrunerV2
     from ...utils.utility import LazyImport
     from neural_compressor.conf.dotdict import DotDict
     from neural_compressor.utils import logger
     from neural_compressor.conf.config import Pruner
     LazyImport('torch.nn')
     torch = LazyImport('torch')
+    tf = LazyImport('tensorflow')
+    F = LazyImport('torch.nn.functional')
 except:
     import torch
+    import torch.nn.functional as F
     from .dot_dict import DotDict  ##TODO
     import logging
     logger = logging.getLogger(__name__)
     from .schema_check import PrunerV2
-    
+
     class WeightPruningConfig:
         """Similiar to torch optimizer's interface."""
 
         def __init__(self, pruning_configs=[{}],  ##empty dict will use global values
                      target_sparsity=0.9, pruning_type="snip_momentum", pattern="4x1", op_names=[],
                      excluded_op_names=[],
                      start_step=0, end_step=0, pruning_scope="global", pruning_frequency=1,
@@ -85,22 +90,35 @@
         blockwise_over_matmul_gemm_conv refers to all-zero blocks' ratio in pruning layers.
     """
     pattern_sparsity_cnt = 0
     element_sparsity_cnt = 0
     if hasattr(model, 'model'):
         model = model.model
     for pruner in pruners:
+        if "MultiheadAttentionPruner" in type(pruner).__name__:
+            logger.info("Calculate multihead-attention sparsity")
+            mha_total = .0
+            mha_sparse = .0
+            for k, v in pruner.head_masks.items():
+                mha_total += v.numel()
+                mha_sparse += v.numel() - torch.count_nonzero(v)
+            logger.info(f"MHA sparsity: {mha_sparse / mha_total}")
+            continue
         modules = pruner.modules
         sparsity_ratio = pruner.pattern.get_sparsity_ratio(pruner.masks)
         cnt = 0
         for key in modules.keys():
             cnt += modules[key].weight.numel()
         pattern_sparsity_cnt += int(cnt * sparsity_ratio)
         for key in pruner.masks.keys():
-            element_sparsity_cnt += torch.sum(pruner.masks[key] == 0).data.item()
+            block_num = 1 
+            if pruner.pattern.block:
+                block_size = pruner.pattern.block_size[key]
+                block_num = block_size[0] * block_size[1]
+            element_sparsity_cnt += torch.sum(pruner.masks[key] == 0).data.item() * block_num
 
     linear_conv_cnt = 0
     param_cnt = 0
     for name, module in model.named_modules():
         if type(module).__name__ in ["Linear"] or re.search(r'Conv.d', type(module).__name__) != None:
             linear_conv_cnt += module.weight.numel()
 
@@ -121,14 +139,70 @@
     logger.info(
         f"elementwise_over_matmul_gemm_conv:{elementwise_over_matmul_gemm_conv},"
         f" elementwise_over_all:{elementwise_over_all},"
         f"blockwise_over_matmul_gemm_conv:{blockwise_over_matmul_gemm_conv}")
 
     return elementwise_over_matmul_gemm_conv, elementwise_over_all, blockwise_over_matmul_gemm_conv
 
+def get_sparsity_ratio_tf(pruners, model):
+    """Calculate sparsity ratio of a module/layer.
+
+    Returns:
+        Three floats.
+        elementwise_over_matmul_gemm_conv refers to zero elements' ratio in pruning layers.
+        elementwise_over_all refers to zero elements' ratio in all layers in the model.
+        blockwise_over_matmul_gemm_conv refers to all-zero blocks' ratio in pruning layers.
+    """
+    pattern_sparsity_cnt = 0
+    element_sparsity_cnt = 0
+    if hasattr(model, 'model'):
+        model = model.model
+    for pruner in pruners:
+        modules = pruner.modules
+        sparsity_ratio = pruner.pattern.get_sparsity_ratio(pruner.masks)
+        cnt = 0
+        for key in modules.keys():
+            cnt += modules[key].get_weights()[0].size
+        pattern_sparsity_cnt += int(cnt * sparsity_ratio)
+        for key in pruner.masks.keys():
+            block_num = 1 
+            if pruner.pattern.block:
+                block_size = pruner.pattern.block_size[key]
+                block_num = block_size[0] * block_size[1]
+            element_sparsity_cnt += np.sum(pruner.masks[key] == 0) * block_num
+
+    linear_conv_cnt = 0
+    param_cnt = 0
+    for layer in model.layers:
+        if layer.__class__.__name__ in ["Dense"] or re.search(r'Conv.d', layer.__class__.__name__) != None:
+            linear_conv_cnt += layer.get_weights()[0].size
+
+    for layer in model.layers:
+        if bool(layer.weights):
+            weights = layer.get_weights()[0]   
+            param_cnt += weights.size
+    if linear_conv_cnt == 0:
+        blockwise_over_matmul_gemm_conv = 0
+        elementwise_over_matmul_gemm_conv = 0
+    else:
+        blockwise_over_matmul_gemm_conv = float(pattern_sparsity_cnt) / linear_conv_cnt
+        elementwise_over_matmul_gemm_conv = float(element_sparsity_cnt) / linear_conv_cnt
+    if param_cnt == 0:
+        elementwise_over_all = 0
+    else:
+        elementwise_over_all = float(
+            element_sparsity_cnt) / param_cnt
+
+    logger.info(
+        f"elementwise_over_matmul_gemm_conv:{elementwise_over_matmul_gemm_conv},"
+        f" elementwise_over_all:{elementwise_over_all},"
+        f"blockwise_over_matmul_gemm_conv:{blockwise_over_matmul_gemm_conv}")
+
+    return elementwise_over_matmul_gemm_conv, elementwise_over_all, blockwise_over_matmul_gemm_conv
+
 def check_config(prune_config):
     """Check if the configuration dict is valid for running Pruning object.
 
     Args:
         prune_config: A config dict object that contains Pruning parameters and configurations.
 
     Returns:
@@ -172,15 +246,15 @@
         except:
             assert False, "N or M can't convert to int"
         assert N > 0, "N should be greater than 0"
         assert M > N, "M should be greater than N"
         max_ratio = float(N) / M
         if prune_config['pruning_type']!="pattern_lock":
             assert prune_config['target_sparsity'] <= max_ratio, \
-                "in N:M pattern, the max sparsity is N/M={}".format(max_ratio)
+                   "in N:M pattern, the max sparsity is N/M={}".format(max_ratio)
         prune_config['max_sparsity_ratio_per_op'] = min(max_ratio, prune_config['max_sparsity_ratio_per_op'])
     if prune_config['reg_coeff'] != None:
         prune_config['reg_coeff'] = float(prune_config['reg_coeff'])
         assert prune_config['reg_coeff'] >= 0, "only support positive reg_type"
     assert prune_config["min_sparsity_ratio_per_op"] >= 0 and prune_config["min_sparsity_ratio_per_op"] <= \
            prune_config['max_sparsity_ratio_per_op'], \
         "min_sparsity_ratio_per_op should in[0, max_sparsity_ratio_per_op]"
@@ -307,53 +381,53 @@
         Args:
             template_config: A default config dict object that contains pruning parameters and configurations.
             usr_cfg_dict: A user config dict object that contains pruning parameters and configurations.
         """
         for user_key, user_value in usr_cfg_dict.pruner_config.items():
             if user_key not in template_config.keys():
                 logger.warning(f"{user_key} is not supported for config")
-    
+
     # multi pruners
     if isinstance(user_config, list):
         for obj in user_config:
             if isinstance(obj, dict):
                 check_key_validity_dict(template_config, obj)
             elif isinstance(obj, PrunerV2):
                 check_key_validity_prunerv2(template_config, obj)
-                
+
     # single pruner, weightconfig or yaml
     elif isinstance(user_config, dict):
         check_key_validity_dict(template_config, user_config)
     elif isinstance(user_config, PrunerV2):
         check_key_validity_prunerv2(template_config, user_config)
     return
 
 def process_and_check_config(val):
     """Process and check configurations.
-    
-    Args:  
+
+    Args:
         val: A dict that contains the layer-specific pruning configurations.
     """
     default_global_config = {'target_sparsity': 0.9, 'pruning_type': 'snip_momentum', 'pattern': '4x1', 'op_names': [],
                              'excluded_op_names': [],
                              'start_step': 0, 'end_step': 0, 'pruning_scope': 'global', 'pruning_frequency': 1,
                              'min_sparsity_ratio_per_op': 0.0, 'max_sparsity_ratio_per_op': 0.98,
-                             'sparsity_decay_type': 'exp',
+                             'sparsity_decay_type': 'exp', "criterion_type": "snip_momentum",
                              'pruning_op_types': ['Conv', 'Linear'],
                              }
     default_local_config = {'resume_from_pruned_checkpoint': False, 'reg_type': None,
                             'criterion_reduce_type': "mean", 'parameters': {"reg_coeff": 0.0}}
 
     params_default_config = {"reg_coeff": 0.0}
 
     default_config = {}
     default_config.update(default_global_config)
     default_config.update(default_local_config)
     default_config.update(params_default_config)
-    if isinstance(val, WeightPruningConfig):
+    if isinstance(val, WeightPruningConfig) or isinstance(val, WeightPruningConf):
         global_configs = val.weight_compression
         pruning_configs = val.pruning_configs
         check_key_validity(default_config, pruning_configs)
         check_key_validity(default_config, global_configs)
         return process_weight_config(global_configs, pruning_configs, default_config)
     else:
         val = val["pruning"]["approach"]["weight_compression_v2"]
@@ -386,28 +460,58 @@
             )
         except Exception as e:
             logger.error("{}.".format(e))
             raise RuntimeError(
                 "The yaml file format is not correct. Please refer to document."
             )
 
-    if isinstance(config, WeightPruningConfig):
+    if isinstance(config, WeightPruningConfig) or isinstance(config, WeightPruningConf):
         return process_and_check_config(config)
     else:
         assert False, f"not supported type {config}"
 
+def parse_last_linear(model):
+    """Locate the last linear layers of the model.
+    While pruning, the final linear often acts like classifier head, which might cause
+    accuracy drop.
+
+    Args:
+        model: The model to be pruned.
+    """
+    from .model_slim.pattern_analyzer import ClassifierHeadSearcher
+    searcher = ClassifierHeadSearcher(model)
+    layer = searcher.search(return_name=True)
+    return layer
+
+def parse_last_linear_tf(model):
+    """Locate the last linear layers of the model.
+    While pruning, the final linear often acts like classifier head, which might cause
+    accuracy drop.
+
+    Args:
+        model(tf.keras.Model): The model to be pruned.
+    """
+    from .model_slim.pattern_analyzer import ClassifierHeadSearcherTF
+    searcher = ClassifierHeadSearcherTF(model)
+    layer = searcher.search(return_name=True)
+    return layer
 
 def parse_to_prune(config, model):
     """Keep target pruned layers.
-    
+
     Args:
         config: A string representing the path to the configuration file.
         model: The model to be pruned.
     """
     modules = {}
+    # additional function: exclude last layer (often a classifier head and not suitable to be pruned)
+    classifier_head_name = parse_last_linear(model)
+    if classifier_head_name != None:
+        config["excluded_op_names"].append(classifier_head_name)
+    # locate target layers
     if config["op_names"] == None or config["op_names"] == []:
         config["op_names"] = [".*"]
     for raw in config["op_names"]:
         try:
             pattern = re.compile(raw)
         except:
             assert False, f"regular expression match does not support {raw}"
@@ -425,17 +529,51 @@
     new_modules = {}
     for name in modules.keys():
         if any([p.search(name) for p in patterns]):
             continue
         new_modules[name] = modules[name]
     return new_modules
 
+def parse_to_prune_tf(config, model):
+    """Keep target pruned layers.
+
+    Args:
+        config(string): A string representing the path to the configuration file.
+        model(tf.keras.Model): The model to be pruned.
+    """
+    modules = {}
+    # additional function: exclude last layer (often a classifier head and not suitable to be pruned)
+    classifier_head_name = parse_last_linear_tf(model)
+    if classifier_head_name != None:
+        config["excluded_op_names"].append(classifier_head_name)
+    # locate target layers
+    if config["op_names"] == None or config["op_names"] == []:
+        config["op_names"] = [".*"]
+
+    for layer in model.layers:
+        for layer_type in config["pruning_op_types"]:
+            if layer_type in layer.__class__.__name__ and bool(layer.weights):
+                modules[layer.name] = layer
+
+    ##remove not to prune layers
+    """Drop non-pruned layers."""
+    exclude_names = config["excluded_op_names"]
+    patterns = [re.compile(s) for s in exclude_names]
+    if len(patterns) <= 0:
+        return modules
+    new_modules = {}
+    for name in modules.keys():
+        if any([p.search(name) for p in patterns]):
+            continue
+        new_modules[name] = modules[name]
+    return new_modules
+
 def generate_pruner_config(info):
     """Generate pruner config object from prune information.
-    
+
     Args:
         info: A dotdict that saves prune information.
 
     Returns:
         pruner: A pruner config object.
     """
     return Pruner(initial_sparsity=0,
```

### Comparing `neural_compressor-2.1.1/neural_compressor/conf/__init__.py` & `neural_compressor-2.2/neural_compressor/conf/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/conf/config.py` & `neural_compressor-2.2/neural_compressor/conf/config.py`

 * *Files 0% similar despite different names*

```diff
@@ -13,25 +13,29 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import yaml
 from schema import Schema, And, Use, Optional, Or, Hook
+
 from ..adaptor import FRAMEWORKS
-from ..strategy import STRATEGIES
 from ..objective import OBJECTIVES
 from ..utils import logger
 from ..version import __version__
 import re
 import copy
 import itertools
 from collections import OrderedDict
 from .dotdict import DotDict, deep_set
 import os, datetime
+# TODO WA for avoid circular import
+# from ..experimental.strategy import EXP_STRATEGIES
+EXP_STRATEGIES = ['basic', 'auto_mixed_precision', 'bayesian', 'conservative',\
+    'exhaustive', 'hawq_v2', 'mse', 'mse_v2', 'random', 'sigopt', 'tpe', 'fake']
 
 def constructor_register(cls):
     yaml_key = "!{}".format(cls.__name__)
 
     def constructor(loader, node):
         instance = cls.__new__(cls)
         yield instance
@@ -273,15 +277,15 @@
         # compute_dtypeis only for PyTorch framework
         Optional('compute_dtype', default=['uint8']): And(
             list,
             lambda s: all(i in ['int8', 'uint8', 'fp32', 'bf16', 'None'] for i in s)),
         # placeholder are only for PyTorch framework
         Optional('algorithm'): And(
             list,
-            lambda s: all(i in ['minmax', 'kl', 'placeholder'] for i in s))
+            lambda s: all(i in ['minmax', 'kl', 'placeholder', 'percentile'] for i in s))
     }
 })
 
 graph_optimization_schema = Schema({
 
     Optional('precisions', default={'precisions': ['fp32']}): And(
         Or(str, list),
@@ -883,15 +887,15 @@
                     Or(str, list),
                     Use(input_to_list),
                     lambda s: all(i in ['int8', 'uint8', 'fp32', 'bf16', 'None'] for i in s)),
                 # placeholder are only for PyTorch framework
                 Optional('algorithm', default=None): And(
                     Or(str, list),
                     Use(input_to_list),
-                    lambda s: all(i in ['minmax', 'kl', 'placeholder'] for i in s)),
+                    lambda s: all(i in ['minmax', 'kl', 'placeholder', 'percentile'] for i in s)),
             }
         },
         Optional('optype_wise', default=None): {
             str: ops_schema
         },
         Optional('op_wise', default=None): {
             str: ops_schema
@@ -910,15 +914,15 @@
         'objective': 'performance',
         'exit_policy': {'timeout': 0, 'max_trials': 100, 'performance_only': False},
         'random_seed': 1978, 'tensorboard': False,
         'workspace': {'path': default_workspace},
         'diagnosis': False,
         }): {
         Optional('strategy', default={'name': 'basic'}): {
-            'name': And(str, lambda s: s in STRATEGIES),
+            'name': And(str, lambda s: s in EXP_STRATEGIES),
             Optional('sigopt_api_token'): str,
             Optional('sigopt_project_id'): str,
             Optional('sigopt_experiment_name', default='nc-tune'): str,
             Optional('accuracy_weight', default=1.0): float,
             Optional('latency_weight', default=1.0): float,
             Optional('confidence_batches', default=2): int,
             Optional('hawq_v2_loss', default=None): object,
@@ -947,29 +951,15 @@
         },
         Optional('random_seed', default=1978): int,
         Optional('tensorboard', default=False): And(bool, lambda s: s in [True, False]),
         Optional('workspace', default={'path': default_workspace}): {
             Optional('path', default=None): str,
             Optional('resume'): str
         },
-        Optional('diagnosis', default = {
-            'diagnosis_after_tuning': False,
-            'op_list': [],
-            'iteration_list': [1],
-            'inspect_type': 'activation',
-            'save_to_disk': True,
-            'save_path': './nc_workspace/inspect_saved/',
-        }):{
-            Optional('diagnosis_after_tuning', default=False): And(bool, lambda s: s in [True, False]),
-            Optional('op_list', default=[]): And(Or(str, list), Use(input_to_list)),
-            Optional('iteration_list', default=[1]): And(Or(int, list), Use(input_to_list_int)),
-            Optional('inspect_type', default='all'): And(str, lambda s : s in ['all', 'activation', 'weight']),
-            Optional('save_to_disk', default=True): And(bool, lambda s: s in [True, False]),
-            Optional('save_path', default='./nc_workspace/inspect_saved/'): str,
-        },
+        Optional('diagnosis', default=False): And(bool, lambda s: s in [True, False]),
     },
     Optional('evaluation'): {
         Hook('accuracy', handler=_valid_multi_metrics): object,
         Optional('accuracy'): {
             Hook('multi_metrics', handler=_valid_metric_length): object,
             Optional('multi_metrics', default=None): {
                 Optional('weight'): And(Or(str, list), Use(input_to_list_float)),
@@ -1084,14 +1074,15 @@
             Optional('iteration', default=-1): int,
             Optional('configs'): configs_schema,
             Optional('dataloader'): dataloader_schema,
             Optional('postprocess'): {
                 Optional('transform'): postprocess_schema
             }
         },
+        Optional('diagnosis', default=False): And(bool, lambda s: s in [True, False]),
     },
     Optional('pruning'): {
         Hook('train', handler=_valid_prune_epoch): object,
         Optional("train"): train_schema,
         Optional("approach"): approach_schema
     },
 
@@ -1410,19 +1401,25 @@
                     pythonic_config.quantization.accuracy_criterion.absolute,
                 'tuning.accuracy_criterion.higher_is_better':
                     pythonic_config.quantization.accuracy_criterion.higher_is_better,
                 'tuning.objective': pythonic_config.quantization.objective,
                 'tuning.exit_policy.timeout': pythonic_config.quantization.timeout,
                 'tuning.exit_policy.max_trials': pythonic_config.quantization.max_trials,
                 'tuning.exit_policy.performance_only': pythonic_config.quantization.performance_only,
-                'tuning.use_distributed_tuning': pythonic_config.quantization.use_distributed_tuning,
                 'use_bf16': pythonic_config.quantization.use_bf16,
                 'quantization.quant_level': pythonic_config.quantization.quant_level,
                 'reduce_range': pythonic_config.quantization.reduce_range
             })
+
+            if pythonic_config.quantization.diagnosis:
+                mapping.update({
+                    'tuning.diagnosis': True,
+                    'tuning.exit_policy.max_trials': 1,
+                })
+
             if pythonic_config.quantization.strategy_kwargs:
                 st_kwargs = pythonic_config.quantization.strategy_kwargs
                 for st_key in ['sigopt_api_token', 'sigopt_project_id', 'sigopt_experiment_name', \
                     'accuracy_weight', 'latency_weight', 'hawq_v2_loss', 'confidence_batches']:
                     if st_key in st_kwargs:
                         st_val =  st_kwargs[st_key]
                         mapping.update({'tuning.strategy.' + st_key: st_val})
@@ -1470,14 +1467,17 @@
                 'evaluation.accuracy.configs.num_of_instance':
                     pythonic_config.benchmark.num_of_instance,
                 'evaluation.accuracy.configs.inter_num_of_threads':
                     pythonic_config.benchmark.inter_num_of_threads,
                 'evaluation.accuracy.configs.intra_num_of_threads':
                     pythonic_config.benchmark.intra_num_of_threads,
             })
+            if pythonic_config.benchmark.diagnosis:
+                mapping.update({'evaluation.diagnosis': pythonic_config.benchmark.diagnosis})
+
             if "model.backend" not in mapping:
                 mapping.update({
                     'model.backend': pythonic_config.benchmark.backend,
                 })
             else:
                 if mapping['model.backend'] == 'default' and \
                         pythonic_config.benchmark.backend != 'default':
```

### Comparing `neural_compressor-2.1.1/neural_compressor/conf/dotdict.py` & `neural_compressor-2.2/neural_compressor/conf/dotdict.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/config.py` & `neural_compressor-2.2/neural_compressor/config.py`

 * *Files 21% similar despite different names*

```diff
@@ -10,19 +10,19 @@
 #   http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Configs for Neural Compressor."""
+"""Configs for Neural Compressor 2.x."""
 import datetime
 import logging
 from schema import Schema, And, Optional
-from .conf.dotdict import DotDict
+from .utils import alias_param
 
 logger = logging.getLogger("neural_compressor")
 default_workspace = './nc_workspace/{}/'.format(
     datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))
 
 QUANTMAPPING = {
     "auto": "post_training_auto_quant",
@@ -54,15 +54,15 @@
             list,
             lambda s: all(i in ['asym', 'sym'] for i in s)),
         Optional('dtype'): And(
             list,
             lambda s: all(i in ['int8', 'uint8', 'fp32', 'bf16', 'fp16', 'None'] for i in s)),
         Optional('algorithm'): And(
             list,
-            lambda s: all(i in ['minmax', 'kl', 'placeholder'] for i in s))}})
+            lambda s: all(i in ['minmax', 'kl', 'placeholder', 'percentile'] for i in s))}})
 
 
 def _check_value(name, src, supported_type, supported_value=[]):
     """Check if the given object is the given supported type and in the given supported value.
 
     Example::
 
@@ -87,14 +87,60 @@
             any([i not in supported_value for i in src]):
             assert False, ("{} is not in supported {}: {}. Skip setting it.".format(
                 src, name, str(supported_value)))
 
     return True
 
 
+class DotDict(dict):
+    """access yaml using attributes instead of using the dictionary notation.
+
+    Args:
+        value (dict): The dict object to access.
+
+    """
+
+    def __init__(self, value=None):
+        """Init a DotDict object."""
+        if value is None:
+            pass
+        elif isinstance(value, dict):
+            for key in value:
+                self.__setitem__(key, value[key])
+        else:
+            raise TypeError('expected dict')
+
+    def __getitem__(self, key):
+        """Get the key."""
+        value = self.get(key, None)
+        return value
+
+    def __setitem__(self, key, value):
+        """Set the value to the key."""
+        if isinstance(value, dict) and not isinstance(value, DotDict):
+            value = DotDict(value)
+        if isinstance(value, list) and len(value) == 1 and isinstance(
+                value[0], dict):
+            value = DotDict(value[0])
+        if isinstance(value, list) and len(value) > 1 and all(isinstance(
+                v, dict) for v in value):
+            value = DotDict({k: v for d in value for k, v in d.items()})
+        super(DotDict, self).__setitem__(key, value)
+
+    def __getstate__(self):
+        """Get the dict."""
+        return self.__dict__
+
+    def __setstate__(self, d):
+        """Set the dict."""
+        self.__dict__.update(d)
+
+    __setattr__, __getattr__ = __setitem__, __getitem__
+
+
 class Options:
     """Option Class for configs.
 
     This class is used for configuring global variables. The global variable options is created with this class.
     If you want to change global variables, you should use functions from utils.utility.py:
         set_random_seed(seed: int)
         set_workspace(workspace: str)
@@ -113,19 +159,20 @@
                           Default value is None.
         tensorboard(bool): This flag indicates whether to save the weights of the model and the inputs of each layer
                                for visual display.
                            Default value is False.
 
     Example::
 
-        from neural_compressor.utils.utility import set_random_seed, set_workspace, set_resume_from, set_tensorboard
+        from neural_compressor import set_random_seed, set_workspace, set_resume_from, set_tensorboard
         set_random_seed(2022)
         set_workspace("workspace_path")
         set_resume_from("workspace_path")
         set_tensorboard(True)
+
     """
     def __init__(self, random_seed=1978, workspace=default_workspace,
                  resume_from=None, tensorboard=False):
         """Init an Option object."""
         self.random_seed = random_seed
         self.workspace = workspace
         self.resume_from = resume_from
@@ -172,78 +219,103 @@
     @tensorboard.setter
     def tensorboard(self, tensorboard):
         """Set tensorboard."""
         if _check_value('tensorboard', tensorboard, bool):
             self._tensorboard = tensorboard
 
 
-options = Options()
-
-
 class BenchmarkConfig:
     """Config Class for Benchmark.
 
     Args:
         inputs (list, optional): A list of strings containing the inputs of model. Default is an empty list.
         outputs (list, optional): A list of strings containing the outputs of model. Default is an empty list.
         backend (str, optional): Backend name for model execution. Supported values include: 'default', 'itex',
                                 'ipex', 'onnxrt_trt_ep', 'onnxrt_cuda_ep'. Default value is 'default'.
         warmup (int, optional): The number of iterations to perform warmup before running performance tests.
                                 Default value is 5.
         iteration (int, optional): The number of iterations to run performance tests. Default is -1.
+        model_name (str, optional): The name of the model. Default value is empty.
         cores_per_instance (int, optional): The number of CPU cores to use per instance. Default value is None.
         num_of_instance (int, optional): The number of instances to use for performance testing.
-                                         Default value is None.
+                                         Default value is 1.
         inter_num_of_threads (int, optional): The number of threads to use for inter-thread operations.
                                               Default value is None.
         intra_num_of_threads (int, optional): The number of threads to use for intra-thread operations.
                                               Default value is None.
 
     Example::
 
         # Run benchmark according to config
-
         from neural_compressor.benchmark import fit
+
         conf = BenchmarkConfig(iteration=100, cores_per_instance=4, num_of_instance=7)
-        fit(model='./int8.pb', config=conf, b_dataloader=eval_dataloader)
+        fit(model='./int8.pb', conf=conf, b_dataloader=eval_dataloader)
     """
     def __init__(self,
                  inputs=[],
                  outputs=[],
                  backend='default',
+                 device='cpu',
                  warmup=5,
                  iteration=-1,
+                 model_name="",
                  cores_per_instance=None,
-                 num_of_instance=None,
+                 num_of_instance=1,
                  inter_num_of_threads=None,
-                 intra_num_of_threads=None):
+                 intra_num_of_threads=None,
+                 diagnosis=False):
         """Init a BenchmarkConfig object."""
         self.inputs = inputs
         self.outputs = outputs
         self.backend = backend
+        self.device=device
         self.warmup = warmup
         self.iteration = iteration
+        self.model_name = model_name
         self.cores_per_instance = cores_per_instance
         self.num_of_instance = num_of_instance
         self.inter_num_of_threads = inter_num_of_threads
         self.intra_num_of_threads = intra_num_of_threads
+        self.diagnosis = diagnosis
+        self._framework = None
+
+    def keys(self):
+        """Returns keys of the dict."""
+        return ('inputs', 'outputs', 'backend', 'device', 'warmup', 'iteration', \
+                'model_name', 'cores_per_instance', 'num_of_instance', 'framework', \
+                'inter_num_of_threads','intra_num_of_threads')
+
+    def __getitem__(self, item):
+        """Get the dict."""
+        return getattr(self, item)
 
     @property
     def backend(self):
         """Get backend."""
         return self._backend
 
     @backend.setter
     def backend(self, backend):
         """Set backend."""
         if _check_value('backend', backend, str, [
                 'default', 'itex', 'ipex', 'onnxrt_trt_ep', 'onnxrt_cuda_ep']):
             self._backend = backend
 
     @property
+    def device(self):
+        """Get device name."""
+        return self._device
+
+    @device.setter
+    def device(self, device):
+        if _check_value('device', device, str, ['cpu', 'gpu']):
+            self._device = device
+
+    @property
     def outputs(self):
         """Get outputs."""
         return self._outputs
 
     @outputs.setter
     def outputs(self, outputs):
         """Set outputs."""
@@ -288,26 +360,26 @@
         """Get cores_per_instance."""
         return self._cores_per_instance
 
     @cores_per_instance.setter
     def cores_per_instance(self, cores_per_instance):
         """Set cores_per_instance."""
         if cores_per_instance is None or _check_value('cores_per_instance', cores_per_instance,
-                                                     int):
+                                                      int):
             self._cores_per_instance = cores_per_instance
 
     @property
     def num_of_instance(self):
         """Get num_of_instance."""
         return self._num_of_instance
 
     @num_of_instance.setter
     def num_of_instance(self, num_of_instance):
         """Set num_of_instance."""
-        if num_of_instance is None or _check_value('num_of_instance', num_of_instance, int):
+        if _check_value('num_of_instance', num_of_instance, int):
             self._num_of_instance = num_of_instance
 
     @property
     def inter_num_of_threads(self):
         """Get inter_num_of_threads."""
         return self._inter_num_of_threads
 
@@ -323,17 +395,49 @@
         """Get intra_num_of_threads."""
         return self._intra_num_of_threads
 
     @intra_num_of_threads.setter
     def intra_num_of_threads(self, intra_num_of_threads):
         """Get intra_num_of_threads."""
         if intra_num_of_threads is None or _check_value('intra_num_of_threads',
-                                                       intra_num_of_threads, int):
+                                                        intra_num_of_threads, int):
             self._intra_num_of_threads = intra_num_of_threads
 
+    @property
+    def diagnosis(self):
+        """Get diagnosis property."""
+        return self._diagnosis
+
+    @diagnosis.setter
+    def diagnosis(self, diagnosis):
+        """Set diagnosis property."""
+        if _check_value('diagnosis', diagnosis, bool):
+            self._diagnosis = diagnosis
+
+    @property
+    def model_name(self):
+        """Get model name."""
+        return self._model_name
+
+    @model_name.setter
+    def model_name(self, model_name):
+        """Set model name."""
+        if _check_value("model_name", model_name, str):
+            self._model_name = model_name
+
+    @property
+    def framework(self):
+        """Set framework."""
+        return self._framework
+
+    @framework.setter
+    def framework(self, framework):
+        """Get framework."""
+        self._framework = framework
+
 
 class AccuracyCriterion:
     """Class of Accuracy Criterion.
 
     Args:
         higher_is_better(bool, optional): This flag indicates whether the metric higher is the better.
                                           Default value is True.
@@ -343,15 +447,15 @@
                                          Default value is 0.01.
 
     Example::
 
         from neural_compressor.config import AccuracyCriterion
 
         accuracy_criterion = AccuracyCriterion(
-            higher_is_better=True,  # optional. 
+            higher_is_better=True,  # optional.
             criterion='relative',  # optional. Available values are 'relative' and 'absolute'.
             tolerable_loss=0.01,  # optional.
         )
     """
     def __init__(self, higher_is_better=True, criterion='relative', tolerable_loss=0.01):
         """Init an AccuracyCriterion object."""
         self.higher_is_better = higher_is_better
@@ -417,29 +521,138 @@
         if _check_value('tolerable_loss', tolerable_loss, float):
             self._tolerable_loss = tolerable_loss
 
     def __str__(self):
         """Get criterion."""
         return self.criterion
 
+    def keys(self):
+        """Returns keys of the dict."""
+        return ('higher_is_better', 'criterion', 'tolerable_loss')
+
+    def __getitem__(self, item):
+        """Get the dict."""
+        return getattr(self, item)
+
 
 accuracy_criterion = AccuracyCriterion()
 
 
+class TuningCriterion:
+    """Class for Tuning Criterion.
+
+    Args:
+        strategy: Strategy name used in tuning. Please refer to docs/source/tuning_strategies.md.
+        strategy_kwargs: Parameters for strategy. Please refer to docs/source/tuning_strategies.md.
+        objective: String or dict. Objective with accuracy constraint guaranteed. String value supports
+                  'performance', 'modelsize', 'footprint'. Default value is 'performance'.
+                   Please refer to docs/source/objective.md.
+        timeout: Tuning timeout (seconds). Default value is 0 which means early stop.
+        max_trials: Max tune times. Default value is 100. Combine with timeout field to decide when to exit.
+
+    Example::
+        from neural_compressor.config import TuningCriterion
+
+        tuning_criterion=TuningCriterion(
+            timeout=0,
+            max_trials=100,
+            strategy="basic",
+            strategy_kwargs=None,
+        )
+    """
+    def __init__(self, strategy="basic", strategy_kwargs=None, timeout=0,
+                 max_trials=100, objective="performance"):
+        """Init a TuningCriterion object."""
+        self.strategy = strategy
+        self.timeout = timeout
+        self.max_trials = max_trials
+        self.objective = objective
+        self.strategy_kwargs = strategy_kwargs
+
+    @property
+    def max_trials(self):
+        """Get max_trials."""
+        return self._max_trials
+
+    @max_trials.setter
+    def max_trials(self, max_trials):
+        """Set max_trials."""
+        if _check_value('max_trials', max_trials, int):
+            self._max_trials = max_trials
+
+    @property
+    def timeout(self):
+        """Get timeout."""
+        return self._timeout
+
+    @timeout.setter
+    def timeout(self, timeout):
+        """Set timeout."""
+        if _check_value('timeout', timeout, int):
+            self._timeout = timeout
+
+    @property
+    def objective(self):
+        """Get objective."""
+        return self._objective
+
+    @objective.setter
+    def objective(self, objective):
+        if _check_value('objective', objective, str,
+            ['performance', 'accuracy', 'modelsize', 'footprint']):
+            self._objective = objective
+            return
+
+        if _check_value('objective', objective, dict):
+            if 'weight' in objective.keys() and isinstance(objective['weight'], list):
+                assert len(objective['objective']) == len(objective['weight'])
+            for k, v in objective.items():
+                _check_value('objective', k, str, ['objective', 'weight', 'higher_is_better'])
+                if k == 'objective':
+                    _check_value('objective', v, str, ['performance', 'accuracy', 'modelsize', 'footprint'])
+            self._objective = objective
+
+    @property
+    def strategy(self):
+        """Get strategy."""
+        return self._strategy
+
+    @strategy.setter
+    def strategy(self, strategy):
+        """Set strategy."""
+        if _check_value('strategy', strategy, str,
+            ['basic', 'mse', 'bayesian', 'random', 'exhaustive', 'sigopt', 'tpe', 'mse_v2', 'hawq_v2']):
+            self._strategy = strategy
+
+    @property
+    def strategy_kwargs(self):
+        """Get strategy_kwargs."""
+        return self._strategy_kwargs
+
+    @strategy_kwargs.setter
+    def strategy_kwargs(self, strategy_kwargs):
+        """Set strategy_kwargs."""
+        self._strategy_kwargs = strategy_kwargs
+
+
+tuning_criterion = TuningCriterion()
+
+
 class _BaseQuantizationConfig:
     """Basic class for quantization config. Inherited by PostTrainingQuantConfig and QuantizationAwareTrainingConfig.
 
     Args:
         inputs: Inputs of model, only required in tensorflow.
         outputs: Outputs of model, only required in tensorflow.
         backend: Backend for model execution. Support 'default', 'itex', 'ipex', 'onnxrt_trt_ep', 'onnxrt_cuda_ep'
         domain: Model domain. Support 'auto', 'cv', 'object_detection', 'nlp' and 'recommendation_system'.
                 Adaptor will use specific quantization settings for different domains automatically, and
                 explicitly specified quantization settings will override the automatic setting.
                 If users set domain as auto, automatic detection for domain will be executed.
+        model_name: The name of the model. Default value is empty.
         recipes: Recipes for quantiztaion, support list is as below.
                  'smooth_quant': whether do smooth quant
                  'smooth_quant_args': parameters for smooth_quant
                  'fast_bias_correction': whether do fast bias correction
                  'weight_correction': whether do weight correction
                  'gemm_to_matmul': whether convert gemm to matmul and add, only valid for onnx models
                  'graph_optimization_level': support 'DISABLE_ALL', 'ENABLE_BASIC', 'ENABLE_EXTENDED', 'ENABLE_ALL'
@@ -475,78 +688,62 @@
                                   "dtype": ["fp32"]
                               },
                               "weight": {
                                   "dtype": ["fp32"]
                               }
                           },
                       }
-        strategy: Strategy name used in tuning, Please refer to docs/source/tuning_strategies.md.
-        strategy_kwargs: Parameters for strategy, Please refer to docs/source/tuning_strategies.md.
-        objective: Objective with accuracy constraint guaranteed, support 'performance', 'modelsize', 'footprint'.
-                   Please refer to docs/source/objective.md.
-                   Default value is 'performance'.
-        timeout: Tuning timeout (seconds). default value is 0 which means early stop
-        max_trials: Max tune times. default value is 100. Combine with timeout field to decide when to exit
-        performance_only: Whether do evaluation
         reduce_range: Whether use 7 bit to quantization.
         example_inputs: Used to trace PyTorch model with torch.jit/torch.fx.
         excluded_precisions: Precisions to be excluded, Default value is empty list.
                              Neural compressor enable the mixed precision with fp32 + bf16 + int8 by default.
                              If you want to disable bf16 data type, you can specify excluded_precisions = ['bf16].
-        quant_level: Support auto, 0 and 1, 0 is conservative strategy, 1 is basic or user-specified 
+        quant_level: Support auto, 0 and 1, 0 is conservative strategy, 1 is basic or user-specified
                      strategy, auto (default) is the combination of 0 and 1.
         accuracy_criterion: Accuracy constraint settings.
-        use_distributed_tuning: Whether use distributed tuning or not.
     """
     def __init__(self,
                  inputs=[],
                  outputs=[],
                  backend="default",
                  domain="auto",
+                 model_name="",
                  recipes={},
                  quant_format="default",
                  device="cpu",
                  calibration_sampling_size=[100],
+                 example_inputs=None,
                  op_type_dict=None,
                  op_name_dict=None,
-                 strategy="basic",
-                 strategy_kwargs=None,
-                 objective="performance",
-                 timeout=0,
-                 max_trials=100,
-                 performance_only=False,
                  reduce_range=None,
-                 example_inputs=None,
                  excluded_precisions=[],
                  quant_level="auto",
                  accuracy_criterion=accuracy_criterion,
-                 use_distributed_tuning=False):
+                 tuning_criterion=tuning_criterion,
+                 diagnosis=False):
         """Initialize _BaseQuantizationConfig class."""
         self.inputs = inputs
         self.outputs = outputs
         self.backend = backend
         self.domain = domain
+        self.model_name = model_name
         self.recipes = recipes
         self.quant_format = quant_format
         self.device = device
         self.op_type_dict = op_type_dict
         self.op_name_dict = op_name_dict
-        self.strategy = strategy
-        self.strategy_kwargs = strategy_kwargs
-        self.objective = objective
-        self.timeout = timeout
-        self.max_trials = max_trials
-        self.performance_only = performance_only
         self.reduce_range = reduce_range
         self.excluded_precisions = excluded_precisions
         self.use_bf16 = "bf16" not in self.excluded_precisions
         self.accuracy_criterion = accuracy_criterion
+        self.tuning_criterion = tuning_criterion
         self.calibration_sampling_size = calibration_sampling_size
         self.quant_level = quant_level
-        self.use_distributed_tuning=use_distributed_tuning
+        self._framework = None
+        self.diagnosis = diagnosis
         self._example_inputs = example_inputs
 
     @property
     def domain(self):
         """Get domain."""
         return self._domain
 
@@ -554,14 +751,25 @@
     def domain(self, domain):
         """Set domain."""
         if _check_value("domain", domain, str,
             ["auto", "cv", "object_detection", "nlp", "recommendation_system"]):
             self._domain = domain
 
     @property
+    def model_name(self):
+        """Get model name."""
+        return self._model_name
+
+    @model_name.setter
+    def model_name(self, model_name):
+        """Set model name."""
+        if _check_value("model_name", model_name, str):
+            self._model_name = model_name
+
+    @property
     def recipes(self):
         """Get recipes."""
         return self._recipes
 
     @recipes.setter
     def recipes(self, recipes):
         """Set recipes."""
@@ -675,14 +883,25 @@
 
     @accuracy_criterion.setter
     def accuracy_criterion(self, accuracy_criterion):
         if _check_value("accuracy_criterion", accuracy_criterion, AccuracyCriterion):
             self._accuracy_criterion = accuracy_criterion
 
     @property
+    def tuning_criterion(self):
+        """Get tuning_criterion."""
+        return self._tuning_criterion
+
+    @tuning_criterion.setter
+    def tuning_criterion(self, tuning_criterion):
+        """Set tuning_criterion."""
+        if _check_value("tuning_criterion", tuning_criterion, TuningCriterion):
+            self._tuning_criterion = tuning_criterion
+
+    @property
     def excluded_precisions(self):
         return self._excluded_precisions
 
     @excluded_precisions.setter
     def excluded_precisions(self, excluded_precisions):
         if _check_value("excluded_precisions", excluded_precisions, str, ["bf16", "fp16"]):
             self._excluded_precisions = excluded_precisions
@@ -693,87 +912,23 @@
         return self._quant_level
 
     @quant_level.setter
     def quant_level(self, quant_level):
         self._quant_level = quant_level
 
     @property
-    def use_distributed_tuning(self):
-        return self._use_distributed_tuning
-
-    @use_distributed_tuning.setter
-    def use_distributed_tuning(self, use_distributed_tuning):
-        if _check_value('use_distributed_tuning', use_distributed_tuning, bool):
-            self._use_distributed_tuning = use_distributed_tuning
-
-    @property
     def reduce_range(self):
         return self._reduce_range
 
     @reduce_range.setter
     def reduce_range(self, reduce_range):
         if reduce_range is None or _check_value('reduce_range', reduce_range, bool):
             self._reduce_range = reduce_range
 
     @property
-    def performance_only(self):
-        return self._performance_only
-
-    @performance_only.setter
-    def performance_only(self, performance_only):
-        if _check_value('performance_only', performance_only, bool):
-            self._performance_only = performance_only
-
-    @property
-    def max_trials(self):
-        return self._max_trials
-
-    @max_trials.setter
-    def max_trials(self, max_trials):
-        if _check_value('max_trials', max_trials, int):
-            self._max_trials = max_trials
-
-    @property
-    def timeout(self):
-        return self._timeout
-
-    @timeout.setter
-    def timeout(self, timeout):
-        if _check_value('timeout', timeout, int):
-            self._timeout = timeout
-
-    @property
-    def objective(self):
-        return self._objective
-
-    @objective.setter
-    def objective(self, objective):
-        if _check_value('objective', objective, str,
-            ['performance', 'accuracy', 'modelsize', 'footprint']):
-            self._objective = objective
-
-    @property
-    def strategy(self):
-        return self._strategy
-
-    @strategy.setter
-    def strategy(self, strategy):
-        if _check_value('strategy', strategy, str,
-            ['basic', 'mse', 'bayesian', 'random', 'exhaustive', 'sigopt', 'tpe', 'mse_v2', 'hawq_v2']):
-            self._strategy = strategy
-
-    @property
-    def strategy_kwargs(self):
-        return self._strategy_kwargs
-
-    @strategy_kwargs.setter
-    def strategy_kwargs(self, strategy_kwargs):
-        self._strategy_kwargs = strategy_kwargs
-
-    @property
     def op_name_dict(self):
         return self._op_name_dict
 
     @op_name_dict.setter
     def op_name_dict(self, op_name_dict):
         if op_name_dict is None:
             self._op_name_dict = op_name_dict
@@ -856,120 +1011,32 @@
 
     @inputs.setter
     def inputs(self, inputs):
         if _check_value('inputs', inputs, str):
             self._inputs = inputs
 
     @property
+    def framework(self):
+        return self._framework
+
+    @framework.setter
+    def framework(self, framework):
+        self._framework = framework
+
+    @property
     def example_inputs(self):
         """Get strategy_kwargs."""
         return self._example_inputs
 
     @example_inputs.setter
     def example_inputs(self, example_inputs):
         """Set example_inputs."""
         self._example_inputs = example_inputs
 
 
-class TuningCriterion:
-    """Class for Tuning Criterion.
-
-    Args:
-        strategy(str, optional): Name of the tuning strategy. Please refer to docs/source/tuning_strategies.md.
-                                 Default is 'basic'.
-        strategy_kwargs(dict, optional): The strategy setting dictionary.
-                                         Please refer to docs/source/tuning_strategies.md.
-                                         Default value is None.
-        timeout(int, optional): Tuning timeout(seconds). When set to 0, early stopping is enabled.
-                                Default value is 0.
-        max_trials(int, optional): Max tuning times. Combined with the `timeout` field to decide when to exit tuning.
-                                   Default is 100.
-        objective(str, optinal): Objective with accuracy constraint guaranteed,
-                                     support 'performance', 'modelsize', 'footprint'.
-                                 Please refer to docs/source/objective.md.
-                                 Default value is 'performance'.
-
-    Example::
-
-        from neural_compressor.config import TuningCriterion
-
-        tuning_criterion=TuningCriterion(
-            strategy="basic",
-            strategy_kwargs=None,
-            timeout=0,
-            max_trials=100,
-        )
-    """
-    def __init__(self, strategy="basic", strategy_kwargs=None, timeout=0, max_trials=100, objective="performance"):
-        """Init a TuningCriterion object."""
-        self.strategy = strategy
-        self.timeout = timeout
-        self.max_trials = max_trials
-        self.objective = objective
-        self.strategy_kwargs = strategy_kwargs
-
-    @property
-    def max_trials(self):
-        """Get max_trials."""
-        return self._max_trials
-
-    @max_trials.setter
-    def max_trials(self, max_trials):
-        """Set max_trials."""
-        if _check_value('max_trials', max_trials, int):
-            self._max_trials = max_trials
-
-    @property
-    def timeout(self):
-        """Get timeout."""
-        return self._timeout
-
-    @timeout.setter
-    def timeout(self, timeout):
-        """Set timeout."""
-        if _check_value('timeout', timeout, int):
-            self._timeout = timeout
-
-    @property
-    def objective(self):
-        """Get objective."""
-        return self._objective
-
-    @objective.setter
-    def objective(self, objective):
-        """Set objective."""
-        if _check_value('objective', objective, str,
-            ['performance', 'accuracy', 'modelsize', 'footprint']):
-            self._objective = objective
-
-    @property
-    def strategy(self):
-        """Get strategy."""
-        return self._strategy
-
-    @strategy.setter
-    def strategy(self, strategy):
-        """Set strategy."""
-        if _check_value('strategy', strategy, str,
-            ['basic', 'mse', 'bayesian', 'random', 'exhaustive', 'sigopt', 'tpe', 'mse_v2', 'hawq_v2']):
-            self._strategy = strategy
-
-    @property
-    def strategy_kwargs(self):
-        """Get strategy_kwargs."""
-        return self._strategy_kwargs
-
-    @strategy_kwargs.setter
-    def strategy_kwargs(self, strategy_kwargs):
-        """Set strategy_kwargs."""
-        self._strategy_kwargs = strategy_kwargs
-
-tuning_criterion = TuningCriterion()
-
-
 class PostTrainingQuantConfig(_BaseQuantizationConfig):
     """Config Class for Post Training Quantization.
 
     Args:
         device: Support 'cpu' and 'gpu'.
         backend: Backend for model execution. Support 'default', 'itex', 'ipex', 'onnxrt_trt_ep', 'onnxrt_cuda_ep'
         domain: Model domain. Support 'auto', 'cv', 'object_detection', 'nlp' and 'recommendation_system'.
@@ -990,15 +1057,15 @@
                  'add_qdq_pair_to_weight': whether add QDQ pair for weights, only vaild for onnxrt_trt_ep
                  'optypes_to_exclude_output_quant': don't quantize output of specified optypes
                  'dedicated_qdq_pair': whether dedicate QDQ pair, only vaild for onnxrt_trt_ep
         quant_format: Support 'default', 'QDQ' and 'QOperator', only required in ONNXRuntime.
         inputs: Inputs of model, only required in tensorflow.
         outputs: Outputs of model, only required in tensorflow.
         approach: Post-Training Quantization method. Neural compressor support 'static', 'dynamic' and 'auto' method.
-                  Default value is 'auto'.
+                  Default value is 'static'.
                   For strategy 'basic', 'auto' method means neural compressor will quantize all OPs support PTQ static
                       or PTQ dynamic. For OPs supporting both PTQ static and PTQ dynamic,
                       PTQ static will be tried first, and PTQ dynamic will be tried when none of the OP type wise
                       tuning configs meet the accuracy loss criteria.
                   For strategy 'bayesian', 'mse', 'mse_v2' and 'HAWQ_V2', 'exhaustive', and 'random',
                       'auto' means neural compressor will quantize all OPs support PTQ static or PTQ dynamic.
                       if OPs supporting both PTQ static and PTQ dynamic, PTQ static will be tried, else PTQ dynamic
@@ -1030,23 +1097,24 @@
                               }
                           },
                       }
         reduce_range: Whether use 7 bit to quantization.
         excluded_precisions: Precisions to be excluded, Default value is empty list.
                              Neural compressor enable the mixed precision with fp32 + bf16 + int8 by default.
                              If you want to disable bf16 data type, you can specify excluded_precisions = ['bf16].
-        quant_level: Support auto, 0 and 1, 0 is conservative strategy, 1 is basic or user-specified 
+        quant_level: Support auto, 0 and 1, 0 is conservative strategy, 1 is basic or user-specified
                      strategy, auto (default) is the combination of 0 and 1.
         tuning_criterion: Instance of TuningCriterion class. In this class you can set strategy, strategy_kwargs,
                               timeout, max_trials and objective.
                           Please refer to docstring of TuningCriterion class.
         accuracy_criterion: Instance of AccuracyCriterion class. In this class you can set higher_is_better,
                                 criterion and tolerable_loss.
                             Please refer to docstring of AccuracyCriterion class.
-        use_distributed_tuning: Whether use distributed tuning or not.
+        diagnosis(bool): This flag indicates whether to do diagnosis.
+                           Default value is False.
 
     Example::
 
         from neural_compressor.config PostTrainingQuantConfig, TuningCriterion
 
         conf = PostTrainingQuantConfig(
             quant_level="auto",
@@ -1065,65 +1133,66 @@
                  inputs=[],
                  outputs=[],
                  approach="static",
                  calibration_sampling_size=[100],
                  op_type_dict=None,
                  op_name_dict=None,
                  reduce_range=None,
+                 example_inputs=None,
                  excluded_precisions=[],
                  quant_level="auto",
-                 tuning_criterion=tuning_criterion,
                  accuracy_criterion=accuracy_criterion,
-                 use_distributed_tuning=False,
-    ):
+                 tuning_criterion=tuning_criterion,
+                 diagnosis=False):
         """Init a PostTrainingQuantConfig object."""
-        self.tuning_criterion = tuning_criterion
         super().__init__(inputs=inputs,
                          outputs=outputs,
                          device=device,
                          backend=backend,
                          domain=domain,
                          recipes=recipes,
                          quant_format=quant_format,
                          calibration_sampling_size=calibration_sampling_size,
                          op_type_dict=op_type_dict,
                          op_name_dict=op_name_dict,
-                         strategy=tuning_criterion.strategy,
-                         strategy_kwargs=tuning_criterion.strategy_kwargs,
-                         objective=tuning_criterion.objective,
-                         timeout=tuning_criterion.timeout,
-                         max_trials=tuning_criterion.max_trials,
                          reduce_range=reduce_range,
+                         example_inputs=example_inputs,
                          excluded_precisions=excluded_precisions,
                          quant_level=quant_level,
                          accuracy_criterion=accuracy_criterion,
-                         use_distributed_tuning=use_distributed_tuning)
+                         tuning_criterion=tuning_criterion,
+                         diagnosis=diagnosis)
         self.approach = approach
+        self.diagnosis = diagnosis
 
     @property
     def approach(self):
         """Get approach."""
         return self._approach
 
     @approach.setter
     def approach(self, approach):
         """Set approach."""
+        if 'static' in approach:
+            approach = 'static'
+        if 'dynamic' in approach:
+            approach = 'dynamic'
         if _check_value("approach", approach, str, ["static", "dynamic", "auto"]):
             self._approach = QUANTMAPPING[approach]
 
     @property
-    def tuning_criterion(self):
-        """Get tuning_criterion."""
-        return self._tuning_criterion
-
-    @tuning_criterion.setter
-    def tuning_criterion(self, tuning_criterion):
-        """Set tuning_criterion."""
-        if _check_value("tuning_criterion", tuning_criterion, TuningCriterion):
-            self._tuning_criterion = tuning_criterion
+    def diagnosis(self):
+        """Get diagnosis."""
+        return self._diagnosis
+
+    @diagnosis.setter
+    def diagnosis(self, diagnosis):
+        """Set diagnosis."""
+        if _check_value('diagnosis', diagnosis, bool):
+            self._diagnosis = diagnosis
 
 
 class QuantizationAwareTrainingConfig(_BaseQuantizationConfig):
     """Config Class for Quantization Aware Training.
 
     Args:
         device: Support 'cpu' and 'gpu'.
@@ -1153,18 +1222,19 @@
                               },
                               "weight": {
                                   "dtype": ["fp32"]
                               }
                           },
                       }
         reduce_range: Whether use 7 bit to quantization.
+        model_name: The name of the model. Default value is empty.
         excluded_precisions: Precisions to be excluded, Default value is empty list.
                              Neural compressor enable the mixed precision with fp32 + bf16 + int8 by default.
                              If you want to disable bf16 data type, you can specify excluded_precisions = ['bf16].
-        quant_level: Support auto, 0 and 1, 0 is conservative strategy, 1 is basic or user-specified 
+        quant_level: Support auto, 0 and 1, 0 is conservative strategy, 1 is basic or user-specified
                      strategy, auto (default) is the combination of 0 and 1.
         tuning_criterion: Instance of TuningCriterion class. In this class you can set strategy, strategy_kwargs,
                               timeout, max_trials and objective.
                           Please refer to docstring of TuningCriterion class.
                           This parameter only required by Quantization Aware Training with tuning.
         accuracy_criterion: Instance of AccuracyCriterion class. In this class you can set higher_is_better,
                                 criterion and tolerable_loss.
@@ -1186,55 +1256,66 @@
                  device="cpu",
                  backend="default",
                  inputs=[],
                  outputs=[],
                  op_type_dict=None,
                  op_name_dict=None,
                  reduce_range=None,
+                 model_name="",
+                 quant_format="default",
                  excluded_precisions=[],
                  quant_level="auto",
-                 tuning_criterion=tuning_criterion,
-                 accuracy_criterion=accuracy_criterion):
+                 accuracy_criterion=accuracy_criterion,
+                 tuning_criterion=tuning_criterion):
         """Init a QuantizationAwareTrainingConfig object."""
         super().__init__(inputs=inputs,
                          outputs=outputs,
                          device=device,
                          backend=backend,
                          op_type_dict=op_type_dict,
                          op_name_dict=op_name_dict,
-                         strategy=tuning_criterion.strategy,
-                         strategy_kwargs=tuning_criterion.strategy_kwargs,
-                         objective=tuning_criterion.objective,
-                         timeout=tuning_criterion.timeout,
-                         max_trials=tuning_criterion.max_trials,
                          reduce_range=reduce_range,
+                         model_name=model_name,
+                         quant_format=quant_format,
                          excluded_precisions=excluded_precisions,
+                         quant_level=quant_level,
                          accuracy_criterion=accuracy_criterion,
-                         quant_level=quant_level)
+                         tuning_criterion=tuning_criterion)
         self._approach = 'quant_aware_training'
+        self._framework = None
 
     @property
     def approach(self):
         """Get approach."""
         return self._approach
 
+    @property
+    def framework(self):
+        """Get framework."""
+        return self._framework
+
+    @framework.setter
+    def framework(self, framework):
+        """Set framework."""
+        self._framework = framework
+
 
 class WeightPruningConfig:
     """Config Class for Pruning. Define a single or a sequence of pruning configs.
-    
+
     Args:
         pruning_configs (list of dicts, optional): Local pruning configs only valid to linked layers.
             Parameters defined out of pruning_configs are valid for all layers.
             By defining dicts in pruning_config, users can set different pruning strategies for corresponding layers.
             Defaults to [{}].
         target_sparsity (float, optional): Sparsity ratio the model can reach after pruning.
             Supports a float between 0 and 1.
             Default to 0.90.
-        pruning_type (str, optional): A string define the criteria for pruning. 
-            Supports "magnitude", "snip", "snip_momentum", 
+        pruning_type (str, optional): A string define the criteria for pruning.
+            Supports "magnitude", "snip", "snip_momentum",
                      "magnitude_progressive", "snip_progressive", "snip_momentum_progressive", "pattern_lock"
             Default to "snip_momentum", which is the most feasible pruning criteria under most situations.
         pattern (str, optional): Sparsity's structure (or unstructure) types.
             Supports "NxM" (e.g "4x1", "8x1"), "channelx1" & "1xchannel"(channel-wise), "N:M" (e.g "2:4").
             Default to "4x1", which can be directly processed by our kernels in ITREX.
         op_names (list of str, optional): Layers contains some specific names to be included for pruning.
             Defaults to [].
@@ -1242,35 +1323,34 @@
             Defaults to [].
         start_step (int, optional): The step to start pruning.
             Supports an integer.
             Default to 0.
         end_step: (int, optional): The step to end pruning.
             Supports an integer.
             Default to 0.
-        pruning_scope (str, optional): Determine layers' scores should be gather together to sort 
-            Supports "global" and "local". 
+        pruning_scope (str, optional): Determine layers' scores should be gather together to sort
+            Supports "global" and "local".
             Default: "global", since this leads to less accuracy loss.
         pruning_frequency: the frequency of pruning operation.
             Supports an integer.
             Default to 1.
         min_sparsity_ratio_per_op (float, optional): Minimum restriction for every layer's sparsity.
             Supports a float between 0 and 1.
-            Default to 0.0.  
+            Default to 0.0.
         max_sparsity_ratio_per_op (float, optional): Maximum restriction for every layer's sparsity.
             Supports a float between 0 and 1.
             Default to 0.98.
         sparsity_decay_type (str, optional): how to schedule the sparsity increasing methods.
             Supports "exp", "cube", "cube", "linear".
             Default to "exp".
         pruning_op_types (list of str): Operator types currently support for pruning.
             Supports ['Conv', 'Linear'].
             Default to ['Conv', 'Linear'].
 
-    Example::
-
+    Example:
         from neural_compressor.config import WeightPruningConfig
         local_configs = [
             {
                 "pruning_scope": "local",
                 "target_sparsity": 0.6,
                 "op_names": ["query", "key", "value"],
                 "pattern": "channelx1",
@@ -1288,20 +1368,21 @@
         prune = Pruning(config)
         prune.update_config(start_step=1, end_step=10)
         prune.model = self.model
     """
 
     def __init__(self, pruning_configs=[{}],  ##empty dict will use global values
                  target_sparsity=0.9, pruning_type="snip_momentum", pattern="4x1", op_names=[],
-                 excluded_op_names=[],
+                 excluded_op_names=[], backend=None,
                  start_step=0, end_step=0, pruning_scope="global", pruning_frequency=1,
                  min_sparsity_ratio_per_op=0.0, max_sparsity_ratio_per_op=0.98,
                  sparsity_decay_type="exp", pruning_op_types=['Conv', 'Linear'],
                  **kwargs):
         """Init a WeightPruningConfig object."""
+        self.backend = backend
         self.pruning_configs = pruning_configs
         self._weight_compression = DotDict({
             'target_sparsity': target_sparsity,
             'pruning_type': pruning_type,
             'pattern': pattern,
             'op_names': op_names,
             'excluded_op_names': excluded_op_names,  ##global only
@@ -1332,15 +1413,15 @@
 
     Args:
         temperature (float, optional): Hyperparameters that control the entropy
             of probability distributions. Defaults to 1.0.
         loss_types (list[str], optional): loss types, should be a list of length 2.
             First item is the loss type for student model output and groundtruth label,
             second item is the loss type for student model output and teacher model output.
-            Supported tpyes for first item are "CE", "MSE". 
+            Supported tpyes for first item are "CE", "MSE".
             Supported tpyes for second item are "CE", "MSE", "KL".
             Defaults to ['CE', 'CE'].
         loss_weights (list[float], optional): loss weights, should be a list of length 2 and sum to 1.0.
             First item is the weight multipled to the loss of student model output and groundtruth label,
             second item is the weight multipled to the loss of student model output and teacher model output.
             Defaults to [0.5, 0.5].
 
@@ -1376,15 +1457,15 @@
             and the teacher_layer_name are the layer names of the student and the teacher models,
             e.g. 'bert.layer1.attention'. The student_layer_output_process and teacher_layer_output_process
             are output process method to get the desired output from the layer specified in the layer
             name, its value can be either a function or a string, in function case, the function
             takes output of the specified layer as input, in string case, when output of the
             specified layer is a dict, this string will serve as key to get corresponding value,
             when output of the specified layer is a list or tuple, the string should be numeric and
-            will serve as the index to get corresponding value. 
+            will serve as the index to get corresponding value.
             When output process is not needed, the item in layer_mappings can be abbreviated to
             [(student_layer_name, ), (teacher_layer_name, )], if student_layer_name and teacher_layer_name
             are the same, it can be abbreviated further to [(layer_name, )].
             Some examples of the item in layer_mappings are listed below:
               [('student_model.layer1.attention', '1'), ('teacher_model.layer1.attention', '1')]
               [('student_model.layer1.output', ), ('teacher_model.layer1.output', )].
               [('model.layer1.output', )].
@@ -1482,45 +1563,44 @@
                 'add_origin_loss': add_origin_loss,
             }
         })
 
 
 criterion = KnowledgeDistillationLossConfig()
 
-
 class DistillationConfig:
     """Config of distillation.
 
     Args:
         teacher_model (Callable): Teacher model for distillation. Defaults to None.
         features (optional): Teacher features for distillation, features and teacher_model are alternative.
                              Defaults to None.
         criterion (Callable, optional): Distillation loss configure.
         optimizer (dictionary, optional): Optimizer configure.
 
     Example::
 
         from neural_compressor.training import prepare_compression
-        from neural_compressor.config import DistillationConfig, SelfKnowledgeDistillationLossConfig
+        from neural_compressor.config import DistillationConfig, KnowledgeDistillationLossConfig
 
-        distil_loss = SelfKnowledgeDistillationLossConfig()
+        distil_loss = KnowledgeDistillationLossConfig()
         conf = DistillationConfig(teacher_model=model, criterion=distil_loss)
         criterion = nn.CrossEntropyLoss()
         optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)
         compression_manager = prepare_compression(model, conf)
         model = compression_manager.model
     """
     def __init__(self,
                  teacher_model=None,
                  criterion=criterion,
                  optimizer={'SGD': {
                      'learning_rate': 0.0001
                  }}):
         """Init a DistillationConfig object."""
-        self.criterion = criterion.config
+        self.criterion = criterion
         self.optimizer = optimizer
         self.teacher_model = teacher_model
 
     @property
     def criterion(self):
         """Get criterion."""
         return self._criterion
@@ -1547,85 +1627,269 @@
 
     @teacher_model.setter
     def teacher_model(self, teacher_model):
         """Set teacher_model."""
         self._teacher_model = teacher_model
 
 
-class MixedPrecisionConfig(PostTrainingQuantConfig):
+class MixedPrecisionConfig(object):
     """Config Class for MixedPrecision.
-    
+
     Args:
         device (str, optional): Device for execution.
                                 Support 'cpu' and 'gpu', default is 'cpu'.
         backend (str, optional): Backend for model execution.
                                  Support 'default', 'itex', 'ipex', 'onnxrt_trt_ep', 'onnxrt_cuda_ep',
-                                 default is 'default'.
-        precision (str, optional): Target precision for mix precision conversion.
+                                 default is 'default', 'ipex' doesn't support tune.
+        precisions ([str, list], optional): Target precision for mix precision conversion.
                                    Support 'bf16' and 'fp16', default is 'bf16'.
+        model_name (str, optional): The name of the model. Default value is empty.
         inputs (list, optional): Inputs of model, default is [].
         outputs (list, optional): Outputs of model, default is [].
         tuning_criterion (TuningCriterion object, optional): Accuracy tuning settings,
                                                              it won't work if there is no accuracy tuning process.
         accuracy_criterion (AccuracyCriterion object, optional): Accuracy constraint settings,
                                                                  it won't work if there is no accuracy tuning process.
         excluded_precisions (list, optional): Precisions to be excluded during mix precision conversion, default is [].
+        op_type_dict (dict, optional): Tuning constraints on optype-wise  for advance user to reduce tuning space.
+                      User can specify the quantization config by op type:
+                      example:
+                      {
+                          'Conv': {
+                              'weight': {
+                                  'dtype': ['fp32']
+                              },
+                              'activation': {
+                                  'dtype': ['fp32']
+                              }
+                          }
+                      }
+        op_name_dict (dict, optional): Tuning constraints on op-wise for advance user to reduce tuning space.
+                      User can specify the quantization config by op name:
+                      example:
+                      {
+                          "layer1.0.conv1": {
+                              "activation": {
+                                  "dtype": ["fp32"]
+                              },
+                              "weight": {
+                                  "dtype": ["fp32"]
+                              }
+                          },
+                      }
+        example_inputs (tensor|list|tuple|dict, optional): Example inputs used for tracing model. Defaults to None.
 
-    Example::
-
+    Example:
         from neural_compressor import mix_precision
         from neural_compressor.config import MixedPrecisionConfig
 
         conf = MixedPrecisionConfig()
-        converted_model = mix_precision.fit(model, config=conf)
+        converted_model = mix_precision.fit(model, conf=conf)
     """
+    @alias_param("precisions", param_alias="precision")
     def __init__(self,
                  device="cpu",
                  backend="default",
-                 precision="bf16",
+                 precisions="bf16",
+                 model_name="",
                  inputs=[],
                  outputs=[],
                  tuning_criterion=tuning_criterion,
                  accuracy_criterion=accuracy_criterion,
-                 excluded_precisions=[]):
+                 excluded_precisions=[],
+                 op_name_dict={},
+                 op_type_dict={},
+                 example_inputs=None):
         """Init a MixedPrecisionConfig object."""
-        super().__init__(inputs=inputs,
-                         outputs=outputs,
-                         device=device,
-                         backend=backend,
-                         tuning_criterion=tuning_criterion,
-                         accuracy_criterion=accuracy_criterion,
-                         excluded_precisions=excluded_precisions,
-        )
-        self.precision = precision
+        self.inputs = inputs
+        self.outputs = outputs
+        self.backend = backend
+        self.device = device
+        self.excluded_precisions = excluded_precisions
+        self.accuracy_criterion = accuracy_criterion
+        self.tuning_criterion = tuning_criterion
+        self.precisions = precisions
+        self.use_bf16 = "bf16" in self.precisions
+        self.model_name = model_name
+        self._framework = None
+        self.op_name_dict = op_name_dict
+        self.op_type_dict = op_type_dict
+        self.example_inputs = example_inputs
 
     @property
-    def precision(self):
+    def precisions(self):
         """Get precision."""
-        return self._precision
+        return self._precisions
 
-    @precision.setter
-    def precision(self, precision):
+    @precisions.setter
+    def precisions(self, precision):
         """Set precision."""
         if isinstance(precision, str):
             assert precision in ["fp16", "bf16"], "Only support 'fp16' and 'bf16' for mix precision."
-            self._precision = [precision]
+            self._precisions = [precision]
         elif isinstance(precision, list):
             assert all([i in ["fp16", "bf16"] for i in precision]), "Only " \
                 "support 'fp16' and 'bf16' for mix precision."
-            self._precision = precision
+            self._precisions = precision
+
+    @property
+    def model_name(self):
+        """Get model name."""
+        return self._model_name
+
+    @model_name.setter
+    def model_name(self, model_name):
+        """Set model name."""
+        if _check_value("model_name", model_name, str):
+            self._model_name = model_name
+
+    @property
+    def accuracy_criterion(self):
+        """Get the accuracy criterion."""
+        return self._accuracy_criterion
+
+    @accuracy_criterion.setter
+    def accuracy_criterion(self, accuracy_criterion):
+        """Set the accuracy criterion."""
+        if _check_value("accuracy_criterion", accuracy_criterion, AccuracyCriterion):
+            self._accuracy_criterion = accuracy_criterion
+
+    @property
+    def tuning_criterion(self):
+        """Get tuning_criterion."""
+        return self._tuning_criterion
+
+    @tuning_criterion.setter
+    def tuning_criterion(self, tuning_criterion):
+        """Set tuning_criterion."""
+        if _check_value("tuning_criterion", tuning_criterion, TuningCriterion):
+            self._tuning_criterion = tuning_criterion
+
+    @property
+    def device(self):
+        """Get device."""
+        return self._device
+
+    @device.setter
+    def device(self, device):
+        """Set device."""
+        if _check_value('device', device, str, ['cpu', 'gpu']):
+            self._device = device
+
+    @property
+    def backend(self):
+        """Get backend."""
+        return self._backend
+
+    @backend.setter
+    def backend(self, backend):
+        """Set backend."""
+        if _check_value('backend', backend, str, [
+                'default', 'itex', 'ipex', 'onnxrt_trt_ep', 'onnxrt_cuda_ep']):
+            self._backend = backend
+
+    @property
+    def outputs(self):
+        """Get outputs."""
+        return self._outputs
+
+    @outputs.setter
+    def outputs(self, outputs):
+        """Set outputs."""
+        if _check_value('outputs', outputs, str):
+            self._outputs = outputs
+
+    @property
+    def inputs(self):
+        """Get inputs."""
+        return self._inputs
+
+    @inputs.setter
+    def inputs(self, inputs):
+        """Set inputs."""
+        if _check_value('inputs', inputs, str):
+            self._inputs = inputs
+
+    @property
+    def framework(self):
+        """Get framework."""
+        return self._framework
+
+    @framework.setter
+    def framework(self, framework):
+        """Set framework."""
+        self._framework = framework
+
+    @property
+    def excluded_precisions(self):
+        """Get excluded precisions."""
+        return self._excluded_precisions
+
+    @excluded_precisions.setter
+    def excluded_precisions(self, excluded_precisions):
+        """Set excluded precisions."""
+        if _check_value("excluded_precisions", excluded_precisions, str, ["bf16", "fp16"]):
+            self._excluded_precisions = excluded_precisions
+            self._use_bf16 = "bf16" not in excluded_precisions
+
+    @property
+    def op_name_dict(self):
+        """Get op name dict."""
+        return self._op_name_dict
+
+    @op_name_dict.setter
+    def op_name_dict(self, op_name_dict):
+        """Set op name dict."""
+        if op_name_dict is None:
+            self._op_name_dict = op_name_dict
+        elif isinstance(op_name_dict, dict):
+            for k, v in op_name_dict.items():
+                ops_schema.validate(v)
+            self._op_name_dict = op_name_dict
+        else:
+            assert False, ("Type of op_name_dict should be dict but not {}, ".format(
+                type(op_name_dict)))
+
+    @property
+    def op_type_dict(self):
+        """Get op type dict."""
+        return self._op_type_dict
+
+    @op_type_dict.setter
+    def op_type_dict(self, op_type_dict):
+        """Set op type dict."""
+        if op_type_dict is None:
+            self._op_type_dict = op_type_dict
+        elif isinstance(op_type_dict, dict):
+            for k, v in op_type_dict.items():
+                ops_schema.validate(v)
+            self._op_type_dict = op_type_dict
+        else:
+            assert False, ("Type of op_type_dict should be dict but not {}".format(
+                type(op_type_dict)))
+
+    @property
+    def example_inputs(self):
+        """Get strategy_kwargs."""
+        return self._example_inputs
+
+    @example_inputs.setter
+    def example_inputs(self, example_inputs):
+        """Set example_inputs."""
+        self._example_inputs = example_inputs
+
 
 class ExportConfig:
     """Common Base Config for Export.
 
     Args:
-        dtype (str, optional): The data type of the exported model, select from ["fp32", "int8"]. 
+        dtype (str, optional): The data type of the exported model, select from ["fp32", "int8"].
                                Defaults to "int8".
         opset_version (int, optional): The ONNX opset version used for export. Defaults to 14.
-        quant_format (str, optional): The quantization format of the exported int8 onnx model, 
+        quant_format (str, optional): The quantization format of the exported int8 onnx model,
                                       select from ["QDQ", "QLinear"]. Defaults to "QDQ".
         example_inputs (tensor|list|tuple|dict, optional): Example inputs used for tracing model. Defaults to None.
         input_names (list, optional): A list of model input names. Defaults to None.
         output_names (list, optional): A list of model output names. Defaults to None.
         dynamic_axes (dict, optional): A dictionary of dynamic axes information. Defaults to None.
     """
     def __init__(
@@ -1713,45 +1977,37 @@
         return self._dynamic_axes
 
     @dynamic_axes.setter
     def dynamic_axes(self, dynamic_axes):
         """Set dynamic_axes."""
         self._dynamic_axes = dynamic_axes
 
-class ONNXQlinear2QDQConfig:
-    """Config Class for ONNXQlinear2QDQ.
-    
-    Example::
 
-        from neural_compressor.config import ONNXQlinear2QDQConfig
-        from neural_compressor.model import Model
-        
-        conf = ONNXQlinear2QDQConfig()
-        model = Model(model)
-        model.export('new_model.onnx', conf)
-    """
+class ONNXQlinear2QDQConfig:
+    """Config Class for ONNXQlinear2QDQ."""
     def __init__(self):
         """Init an ONNXQlinear2QDQConfig object."""
         pass
 
+
 class Torch2ONNXConfig(ExportConfig):
     """Config Class for Torch2ONNX.
 
     Args:
-        dtype (str, optional): The data type of the exported model, select from ["fp32", "int8"]. 
+        dtype (str, optional): The data type of the exported model, select from ["fp32", "int8"].
                                Defaults to "int8".
         opset_version (int, optional): The ONNX opset version used for export. Defaults to 14.
-        quant_format (str, optional): The quantization format of the exported int8 onnx model, 
+        quant_format (str, optional): The quantization format of the exported int8 onnx model,
                                       select from ["QDQ", "QLinear"]. Defaults to "QDQ".
         example_inputs (tensor|list|tuple|dict, required): Example inputs used for tracing model. Defaults to None.
         input_names (list, optional): A list of model input names. Defaults to None.
         output_names (list, optional): A list of model output names. Defaults to None.
         dynamic_axes (dict, optional): A dictionary of dynamic axes information. Defaults to None.
-        recipe (str, optional): A string to select recipes used for Linear -> Matmul + Add, select from 
-                                ["QDQ_OP_FP32_BIAS", "QDQ_OP_INT32_BIAS", "QDQ_OP_FP32_BIAS_QDQ"]. 
+        recipe (str, optional): A string to select recipes used for Linear -> Matmul + Add, select from
+                                ["QDQ_OP_FP32_BIAS", "QDQ_OP_INT32_BIAS", "QDQ_OP_FP32_BIAS_QDQ"].
                                 Defaults to 'QDQ_OP_FP32_BIAS'.
 
     Example:
         # resnet50
         from neural_compressor.config import Torch2ONNXConfig
         int8_onnx_config = Torch2ONNXConfig(
             dtype="int8",
@@ -1770,28 +2026,26 @@
        dtype="int8",
        opset_version=14,
        quant_format="QDQ",
        example_inputs=None,
        input_names=None,
        output_names=None,
        dynamic_axes=None,
-       recipe='QDQ_OP_FP32_BIAS',
        **kwargs,
     ):
         """Init a Torch2ONNXConfig object."""
         super().__init__(
             dtype=dtype,
             opset_version=opset_version,
             quant_format=quant_format,
             example_inputs=example_inputs,
             input_names=input_names,
             output_names=output_names,
             dynamic_axes=dynamic_axes,
         )
-        self.recipe = recipe
         self.kwargs = kwargs
 
 
 class TF2ONNXConfig(ExportConfig):
     """Config Class for TF2ONNX.
 
     Args:
@@ -1832,7 +2086,222 @@
             quant_format=quant_format,
             example_inputs=example_inputs,
             input_names=input_names,
             output_names=output_names,
             dynamic_axes=dynamic_axes,
         )
         self.kwargs = kwargs
+
+
+class NASConfig:
+    """Config class for NAS approaches."""
+    def __init__(self, approach=None, search_space=None, search_algorithm=None,
+                 metrics=[], higher_is_better=[], max_trials=3, seed=42, dynas=None):
+        """Init a NASConfig object."""
+        self._approach = approach
+        self._search = DotDict({
+            'search_space': search_space,
+            'search_algorithm': search_algorithm,
+            'metrics': metrics,
+            'higher_is_better': higher_is_better,
+            'max_trials': max_trials,
+            'seed': seed
+        })
+        self.dynas = None
+        if approach == 'dynas' and dynas:
+            self.dynas = dynas.config
+
+    @property
+    def approach(self):
+        """Get approach."""
+        return self._approach
+
+    @approach.setter
+    def approach(self, approach):
+        """Set approach."""
+        self._approach = approach
+
+    @property
+    def search(self):
+        """Get the setting dict for search."""
+        return self._search
+
+    @search.setter
+    def search(self, search):
+        """Set the setting dict for search."""
+        self._search = search
+
+
+class MXNet:
+    """Base config class for MXNet."""
+    def __init__(self, precisions=None):
+        """Init an MXNet object."""
+        self._precisions = precisions
+
+    @property
+    def precisions(self):
+        """Get precision."""
+        return self._precisions
+
+    @precisions.setter
+    def precisions(self, precisions):
+        """Set precision."""
+        if not isinstance(precisions, list):
+            precisions = [precisions]
+        for pr in precisions:
+            _check_value('precisions', pr, str, ['int8', 'uint8', 'fp32', 'bf16', 'fp16'])
+        self._precisions = precisions
+
+
+class ONNX(MXNet):
+    """Config class for ONNX."""
+    def __init__(self, graph_optimization_level=None, precisions=None):
+        """Init an ONNX object."""
+        super().__init__(precisions)
+        self._graph_optimization_level = graph_optimization_level
+
+    @property
+    def graph_optimization_level(self):
+        """Get graph optimization level."""
+        return self._graph_optimization_level
+
+    @graph_optimization_level.setter
+    def graph_optimization_level(self, graph_optimization_level):
+        """Set graph optimization level."""
+        if _check_value('graph_optimization_level', graph_optimization_level, str,
+            ['DISABLE_ALL', 'ENABLE_BASIC', 'ENABLE_EXTENDED', 'ENABLE_ALL']):
+            self._graph_optimization_level = graph_optimization_level
+
+
+class TensorFlow(MXNet):
+    """Config class for TensorFlow."""
+    def __init__(self, precisions=None):
+        """Init a TensorFlow object."""
+        super().__init__(precisions)
+
+
+class Keras(MXNet):
+    """Config class for Keras."""
+    def __init__(self, precisions=None):
+        """Init a Keras object."""
+        super().__init__(precisions)
+
+
+class PyTorch(MXNet):
+    """Config class for PyTorch."""
+    def __init__(self, precisions=None):
+        """Init a PyTorch object."""
+        super().__init__(precisions)
+
+
+quantization = PostTrainingQuantConfig()
+benchmark = BenchmarkConfig()
+options = Options()
+mixed_precision = MixedPrecisionConfig()
+pruning = WeightPruningConfig()
+distillation = DistillationConfig(teacher_model=None)
+nas = NASConfig()
+onnxruntime_config = ONNX()
+tensorflow_config = TensorFlow()
+keras_config = Keras()
+pytorch_config = PyTorch()
+mxnet_config = MXNet()
+
+
+class _Config:
+    """Main config class."""
+    def __init__(self,
+                 quantization=quantization,
+                 benchmark=benchmark,
+                 mixed_precision=mixed_precision,
+                 pruning=pruning,
+                 distillation=distillation,
+                 nas=nas,
+                 onnxruntime=onnxruntime_config,
+                 tensorflow=tensorflow_config,
+                 pytorch=pytorch_config,
+                 mxnet=mxnet_config,
+                 keras=keras_config,
+                 diagnosis=None):
+        """Init a config object."""
+        self._quantization = quantization
+        self._benchmark = benchmark
+        self._mixed_precision = mixed_precision
+        self._onnxruntime = onnxruntime
+        self._pruning = pruning
+        self._distillation = distillation
+        self._nas = nas
+        self._tensorflow = tensorflow
+        self._pytorch = pytorch
+        self._mxnet = mxnet
+        self._keras = keras
+        if diagnosis is None:
+            diagnosis = False
+            if (quantization is not None and quantization.diagnosis) or \
+                    (benchmark is not None and benchmark.diagnosis):
+                diagnosis = True
+        if diagnosis:
+            tuning_criterion.max_trials = 1
+        self._diagnosis = diagnosis
+
+    @property
+    def distillation(self):
+        """Get the distillation object."""
+        return self._distillation
+
+    @property
+    def nas(self):
+        """Get the nas object."""
+        return self._nas
+
+    @property
+    def tensorflow(self):
+        """Get the tensorflow object."""
+        return self._tensorflow
+
+    @property
+    def keras(self):
+        """Get the keras object."""
+        return self._keras
+
+    @property
+    def pytorch(self):
+        """Get the pytorch object."""
+        return self._pytorch
+
+    @property
+    def mxnet(self):
+        """Get the mxnet object."""
+        return self._mxnet
+
+    @property
+    def pruning(self):
+        """Get the pruning object."""
+        return self._pruning
+
+    @property
+    def quantization(self):
+        """Get the quantization object."""
+        return self._quantization
+
+    @property
+    def benchmark(self):
+        """Get the benchmark object."""
+        return self._benchmark
+
+    @property
+    def mixed_precision(self):
+        """Get the mixed_precision object."""
+        return self._mixed_precision
+
+    @property
+    def onnxruntime(self):
+        """Get the onnxruntime object."""
+        return self._onnxruntime
+
+
+    @property
+    def diagnosis(self):
+        """Get the diagnosis value."""
+        return self._diagnosis
+
+config = _Config()
```

### Comparing `neural_compressor-2.1.1/neural_compressor/contrib/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/contrib/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/contrib/strategy/__init__.py` & `neural_compressor-2.2/neural_compressor/contrib/strategy/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/contrib/strategy/sigopt.py` & `neural_compressor-2.2/neural_compressor/experimental/contrib/strategy/sigopt.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 #
 # Copyright (c) 2021 Intel Corporation
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -16,18 +15,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """The SigOpt Tuning Strategy provides support for the quantization process."""
 import copy
 from neural_compressor.utils import logger
 from neural_compressor.utils.utility import LazyImport
-from neural_compressor.strategy.strategy import strategy_registry, TuneStrategy
+from neural_compressor.experimental.strategy.strategy import strategy_registry, TuneStrategy
 from collections import OrderedDict
-from neural_compressor.strategy.utils.tuning_sampler import OpWiseTuningSampler
-from neural_compressor.strategy.utils.tuning_structs import OpTuningConfig
+from neural_compressor.experimental.strategy.utils.tuning_sampler import OpWiseTuningSampler
+from neural_compressor.experimental.strategy.utils.tuning_structs import OpTuningConfig
 
 sigopt = LazyImport('sigopt')
 
 @strategy_registry
 class SigOptTuneStrategy(TuneStrategy):
     """The tuning strategy using SigOpt HPO search in tuning space.
```

### Comparing `neural_compressor-2.1.1/neural_compressor/contrib/strategy/tpe.py` & `neural_compressor-2.2/neural_compressor/experimental/contrib/strategy/tpe.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,18 +19,18 @@
 import copy
 import os
 from pathlib import Path
 from functools import partial
 import numpy as np
 from neural_compressor.utils import logger
 from neural_compressor.utils.utility import LazyImport
-from neural_compressor.strategy.strategy import strategy_registry, TuneStrategy
+from neural_compressor.experimental.strategy.strategy import strategy_registry, TuneStrategy
 from collections import OrderedDict
-from neural_compressor.strategy.utils.tuning_sampler import OpWiseTuningSampler
-from neural_compressor.strategy.utils.tuning_structs import OpTuningConfig
+from neural_compressor.experimental.strategy.utils.tuning_sampler import OpWiseTuningSampler
+from neural_compressor.experimental.strategy.utils.tuning_structs import OpTuningConfig
 
 hyperopt = LazyImport('hyperopt')
 
 try:
     import pandas as pd
 except ImportError:
     pd = None
```

### Comparing `neural_compressor-2.1.1/neural_compressor/data/__init__.py` & `neural_compressor-2.2/neural_compressor/data/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,24 +18,26 @@
 """Built-in dataloaders, datasets, transforms, filters for multiple framework backends."""
 
 
 import neural_compressor.data.datasets
 import neural_compressor.data.transforms
 from .datasets import Datasets, Dataset, IterableDataset, dataset_registry, TensorflowImageRecord, COCORecordDataset
 from .dataloaders import DATALOADERS, DataLoader
+from .dataloaders.dataloader import check_dataloader
 from .dataloaders.default_dataloader import DefaultDataLoader
 from .transforms import TRANSFORMS, BaseTransform, ComposeTransform, transform_registry, Postprocess
 from .transforms import LabelShift, BilinearImagenetTransform, TensorflowResizeCropImagenetTransform
 from .transforms import TFSquadV1PostTransform, TFSquadV1ModelZooPostTransform
 from .transforms import TensorflowResizeWithRatio, ResizeTFTransform, RescaleTFTransform, NormalizeTFTransform
 from .transforms import ParseDecodeCocoTransform
 
 from .filters import FILTERS, Filter, filter_registry, LabelBalanceCOCORecordFilter
 
 __all__ = [
+    "check_dataloader",
     "DataLoader",
     "DATALOADERS",
     "DefaultDataLoader",
     "Datasets",
     "Dataset",
     "IterableDataset",
     "COCORecordDataset",
```

### Comparing `neural_compressor-2.1.1/neural_compressor/data/dataloaders/__init__.py` & `neural_compressor-2.2/neural_compressor/data/dataloaders/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/dataloaders/base_dataloader.py` & `neural_compressor-2.2/neural_compressor/data/dataloaders/base_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/dataloaders/dataloader.py` & `neural_compressor-2.2/neural_compressor/data/dataloaders/dataloader.py`

 * *Files 3% similar despite different names*

```diff
@@ -76,14 +76,15 @@
                                       sampler=sampler,
                                       batch_sampler=batch_sampler,
                                       num_workers=num_workers,
                                       pin_memory=pin_memory,
                                       shuffle=shuffle,
                                       distributed=distributed)
 
+
 def _generate_common_dataloader(dataloader, framework, distributed=False):
     """Generate common dataloader.
 
     Args:
         dataloader (generator): A dataloader which can yield tuple of (input, label)/(input, _) 
             batched data.
         framework (str): The string of supported framework.
@@ -108,7 +109,15 @@
             last_batch=dataloader.last_batch,
             sampler=dataloader.sampler,
             batch_sampler=dataloader.batch_sampler,
             num_workers=dataloader.num_workers,
             pin_memory=dataloader.pin_memory,
             shuffle=dataloader.shuffle,
             distributed=bool(dataloader.distributed or distributed))
+
+
+def check_dataloader(dataloader):
+    """Check if the dataloader meets requirement of neural_compressor."""
+    assert hasattr(dataloader, '__iter__') and \
+        hasattr(dataloader, 'batch_size'), \
+        'dataloader must implement __iter__ method and batch_size attribute'
+    return True
```

### Comparing `neural_compressor-2.1.1/neural_compressor/data/dataloaders/default_dataloader.py` & `neural_compressor-2.2/neural_compressor/data/dataloaders/default_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/dataloaders/fetcher.py` & `neural_compressor-2.2/neural_compressor/data/dataloaders/fetcher.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/dataloaders/mxnet_dataloader.py` & `neural_compressor-2.2/neural_compressor/data/dataloaders/mxnet_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/dataloaders/onnxrt_dataloader.py` & `neural_compressor-2.2/neural_compressor/data/dataloaders/onnxrt_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/dataloaders/pytorch_dataloader.py` & `neural_compressor-2.2/neural_compressor/data/dataloaders/pytorch_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/dataloaders/sampler.py` & `neural_compressor-2.2/neural_compressor/data/dataloaders/sampler.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/dataloaders/tensorflow_dataloader.py` & `neural_compressor-2.2/neural_compressor/data/dataloaders/tensorflow_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/datasets/__init__.py` & `neural_compressor-2.2/neural_compressor/data/datasets/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/datasets/bert_dataset.py` & `neural_compressor-2.2/neural_compressor/data/datasets/bert_dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/datasets/coco_dataset.py` & `neural_compressor-2.2/neural_compressor/data/datasets/coco_dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/datasets/dataset.py` & `neural_compressor-2.2/neural_compressor/data/datasets/dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/datasets/dummy_dataset.py` & `neural_compressor-2.2/neural_compressor/data/datasets/dummy_dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/datasets/dummy_dataset_v2.py` & `neural_compressor-2.2/neural_compressor/data/datasets/dummy_dataset_v2.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/datasets/imagenet_dataset.py` & `neural_compressor-2.2/neural_compressor/data/datasets/imagenet_dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/datasets/style_transfer_dataset.py` & `neural_compressor-2.2/neural_compressor/data/datasets/style_transfer_dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/filters/__init__.py` & `neural_compressor-2.2/neural_compressor/data/filters/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/filters/coco_filter.py` & `neural_compressor-2.2/neural_compressor/data/filters/coco_filter.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/filters/filter.py` & `neural_compressor-2.2/neural_compressor/data/filters/filter.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/transforms/__init__.py` & `neural_compressor-2.2/neural_compressor/data/transforms/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/transforms/coco_transform.py` & `neural_compressor-2.2/neural_compressor/data/transforms/coco_transform.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/transforms/imagenet_transform.py` & `neural_compressor-2.2/neural_compressor/data/transforms/imagenet_transform.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/transforms/postprocess.py` & `neural_compressor-2.2/neural_compressor/data/transforms/postprocess.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/transforms/tokenization.py` & `neural_compressor-2.2/neural_compressor/data/transforms/tokenization.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/data/transforms/transform.py` & `neural_compressor-2.2/neural_compressor/experimental/data/transforms/transform.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -24,11 +24,12 @@
 from .benchmark import Benchmark
 from .graph_optimization import Graph_Optimization, GraphOptimization
 from .mixed_precision import MixedPrecision
 from .model_conversion import ModelConversion
 from .distillation import Distillation
 from .nas import NAS
 from . import export
+from .contrib import *
 
 __all__ = ['Component', 'Quantization', 'Pruning', 'Benchmark', 'Graph_Optimization', \
            'GraphOptimization', 'ModelConversion', 'Distillation', 'NAS', 'MixedPrecision', \
            'export']
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/benchmark.py` & `neural_compressor-2.2/neural_compressor/experimental/benchmark.py`

 * *Files 0% similar despite different names*

```diff
@@ -294,15 +294,15 @@
             core_list: a list of core indexes bound with specific instances
         """
         if sys.platform in ['linux'] and os.system('numactl --show >/dev/null 2>&1') == 0:
             return 'OMP_NUM_THREADS={} numactl --localalloc --physcpubind={}'.format(\
                 len(core_list), ','.join(core_list.astype(str)))
         elif sys.platform in ['win32']:  # pragma: no cover
             # (TODO) should we move the hw_info from ux?
-            from neural_compressor.ux.utils.hw_info import get_number_of_sockets
+            from neural_compressor.utils.utility import get_number_of_sockets
             num_of_socket = int(get_number_of_sockets())
             cores_per_instance = int(os.environ.get('CORES_PER_INSTANCE'))
             cores_per_socket = int(psutil.cpu_count(logical=False)) / num_of_socket
             socket_id = int(core_list[0] // cores_per_socket)
             # cores per socket should integral multiple of cores per instance, else not bind core
             if cores_per_socket % cores_per_instance == 0:
                 from functools import reduce
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/common/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/common/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/common/criterion.py` & `neural_compressor-2.2/neural_compressor/experimental/common/criterion.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/common/dataloader.py` & `neural_compressor-2.2/neural_compressor/experimental/common/dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/common/metric.py` & `neural_compressor-2.2/neural_compressor/experimental/common/metric.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/common/model.py` & `neural_compressor-2.2/neural_compressor/experimental/common/model.py`

 * *Files 6% similar despite different names*

```diff
@@ -43,15 +43,18 @@
             framework = get_model_fwk_name(root)
 
         if 'tensorflow' in framework:
             if 'modelType' in kwargs:
                 model_type = kwargs['modelType']
             else:
                 model_type = get_model_type(root)
-            model = MODELS['tensorflow'](model_type, root, **kwargs)
+            if model_type =='AutoTrackable': # pragma: no cover
+                model = MODELS['tensorflow']("keras", root, **kwargs)
+            else:
+                model = MODELS['tensorflow'](model_type, root, **kwargs)
         elif framework == 'keras':
             model = MODELS['keras'](root, **kwargs)
         elif framework == 'pytorch':
             if BACKEND != "default":
                 framework = BACKEND
             model = MODELS[framework](root, **kwargs)
         else:
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/common/optimizer.py` & `neural_compressor-2.2/neural_compressor/experimental/common/optimizer.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/common/postprocess.py` & `neural_compressor-2.2/neural_compressor/experimental/common/postprocess.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/common/torch_utils.py` & `neural_compressor-2.2/neural_compressor/experimental/common/torch_utils.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/component.py` & `neural_compressor-2.2/neural_compressor/experimental/component.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/compression/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/compression/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/compression/pruning.py` & `neural_compressor-2.2/neural_compressor/experimental/compression/pruning.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 # limitations under the License.
 
 from neural_compressor.compression.pruner.utils import process_config, parse_to_prune
 from neural_compressor.compression.pruner.pruners import get_pruner
 from neural_compressor.compression.pruner.utils import logger, torch
 
 
-def _generate_pruners(config, model):
+def _generate_pruners(config, model: torch.nn.Module):
     """Generate pruners.
 
     :param config: WeightPruningConfig
     :param model: The torch module to be pruned
     :return: A list of pruner
     """
     assert isinstance(model, torch.nn.Module)
@@ -39,15 +39,15 @@
         pruners.append(get_pruner(info, modules))
         info['modules'] = [key for key in modules.keys()]
         info['len_of_modules'] = len(info['modules'])
         logger.info(info)
     return pruners
 
 
-def _register_on_step_begin(model):
+def _register_on_step_begin(model: torch.nn.Module):
     """Mount on_step_begin to the model.
 
     :param model:The torch module to be pruned
     :return: hook handle
     """
 
     def hook(module, input):
@@ -57,15 +57,15 @@
     hook_handle = model.register_forward_pre_hook(hook)
     return hook_handle
 
 
 def _rewrite_optimizer_step(opt: torch.optim.Optimizer):
     """Mount on_before/after_optimizer_step to optimizer.
 
-    :param opt: user optimizer
+    :param opt: user optimizer: should be a torch.optim.Optimizer object
     :return: the modified optimizer
     """
 
     def new_step(self, closure=None):
         if hasattr(self, "pruners"):  ## in case user save the whole optimzer
             for pruner in self.pruners:
                 pruner.on_before_optimizer_step()
@@ -141,20 +141,20 @@
         return
     if len(params) != 0:
         torch.orig_save(obj, f, params)
     else:
         torch.orig_save(obj, f)
 
 
-def prepare_pruning(config, model: torch.nn.Module, opt: torch.optim):
+def prepare_pruning(config, model: torch.nn.Module, opt: torch.optim.Optimizer):
     """Wrapper the model and optimizer to support all the pruning functionality.
 
     :param config: WeightPruningConfig
-    :param model: The user's model
-    :param opt: The user's optimizer
+    :param model: The user's model, a torch.nn.Module object
+    :param opt: The user's optimizer, a torch.optim object
     :return: The modified model and optimizer
     """
     import torch
     torch.orig_save = torch.save  ##rewrite torch save
     setattr(torch, 'save', save)
 
     pruners = _generate_pruners(config, model)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/data/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/base_dataloader.py` & `neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/base_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/dataloader.py` & `neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/default_dataloader.py` & `neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/default_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/fetcher.py` & `neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/fetcher.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/mxnet_dataloader.py` & `neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/mxnet_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/onnxrt_dataloader.py` & `neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/onnxrt_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/pytorch_dataloader.py` & `neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/pytorch_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/sampler.py` & `neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/sampler.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/dataloaders/tensorflow_dataloader.py` & `neural_compressor-2.2/neural_compressor/experimental/data/dataloaders/tensorflow_dataloader.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/data/datasets/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/bert_dataset.py` & `neural_compressor-2.2/neural_compressor/experimental/data/datasets/bert_dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/coco_dataset.py` & `neural_compressor-2.2/neural_compressor/experimental/data/datasets/coco_dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/dataset.py` & `neural_compressor-2.2/neural_compressor/experimental/data/datasets/dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/dummy_dataset.py` & `neural_compressor-2.2/neural_compressor/experimental/data/datasets/dummy_dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/dummy_dataset_v2.py` & `neural_compressor-2.2/neural_compressor/experimental/data/datasets/dummy_dataset_v2.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/imagenet_dataset.py` & `neural_compressor-2.2/neural_compressor/experimental/data/datasets/imagenet_dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/datasets/style_transfer_dataset.py` & `neural_compressor-2.2/neural_compressor/experimental/data/datasets/style_transfer_dataset.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/filters/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/data/filters/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/filters/coco_filter.py` & `neural_compressor-2.2/neural_compressor/experimental/data/filters/coco_filter.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/filters/filter.py` & `neural_compressor-2.2/neural_compressor/experimental/data/filters/filter.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/transforms/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/data/transforms/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/transforms/imagenet_transform.py` & `neural_compressor-2.2/neural_compressor/experimental/data/transforms/imagenet_transform.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/transforms/tokenization.py` & `neural_compressor-2.2/neural_compressor/experimental/data/transforms/tokenization.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/data/transforms/transform.py` & `neural_compressor-2.2/neural_compressor/data/transforms/transform.py`

 * *Files 0% similar despite different names*

```diff
@@ -2634,14 +2634,16 @@
                 process="postprocess", framework="tensorflow, tensorflow_itex")
 class TFModelZooCollectTransform(CollectTransform):
     """Postprocess the predictions of model zoo, collect data."""
 
     def __call__(self, sample):
         """Collect postprocess data."""
         all_results, label = sample
+        if len(all_results) == 1:
+            all_results = all_results.reshape((2, 1, 384))
         all_results = zip(all_results[0], all_results[1])
         for start_logits, end_logits in all_results:
             if len(self.unique_id) < self.length:
                 self.unique_id.append(self.idx)
                 self.start_logits.append(start_logits)
                 self.end_logits.append(end_logits)
                 self.idx += 1
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/distillation.py` & `neural_compressor-2.2/neural_compressor/experimental/distillation.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 from ..utils import logger
 from ..utils.create_obj_from_config import create_dataloader, create_eval_func, create_train_func
 from ..model import BaseModel
 from .common import Model
 from ..adaptor import FRAMEWORKS
 from neural_compressor.experimental.common import Criterions, Optimizers
 from ..conf.config import DistillationConf
-from ..conf.pythonic_config import Config
+from ..conf.pythonic_config import Config, DotDict
 
 class Distillation(Component):
     """Distillation class derived from Component class.
     
     Distillation class abstracted the pipeline of knowledge distillation, 
     transfer the knowledge of the teacher model to the student model.
        
@@ -161,18 +161,23 @@
     def create_criterion(self):
         """Create the criterion for training."""
         self.init_train_cfg()
         if self.criterion is None:
             assert 'criterion' in self._train_cfg.keys(), \
                 "criterion part in train field of distillation section in yaml file " \
                 "must be configured for distillation if criterion is NOT set."
-            criterion_cfg = self._train_cfg.criterion
+            
+            if isinstance(self._train_cfg.criterion, DotDict):
+                criterion_cfg = self._train_cfg.criterion
+            else:
+                criterion_cfg = self._train_cfg.criterion.config
+
             assert len(criterion_cfg) == 1, "There must be exactly one loss in " \
                 "criterion part, instead got {} loss.".format(len(criterion_cfg))
-            loss = list(criterion_cfg.keys())[0]
+            loss = [i for i in criterion_cfg.keys()][0]
             loss_cfg = criterion_cfg[loss]
             criterion_builder = Criterions(self.framework)[loss](loss_cfg)
             criterion_tuple = criterion_builder()
             if self.teacher_model and self.student_model:
                 if self.framework == 'tensorflow':  # new, for tf
                     teacher_model = self.teacher_model._model
                     student_model = self.student_model._model
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/export/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/export/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/export/qlinear2qdq.py` & `neural_compressor-2.2/neural_compressor/experimental/export/qlinear2qdq.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/export/tf2onnx.py` & `neural_compressor-2.2/neural_compressor/experimental/export/tf2onnx.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/export/utils.py` & `neural_compressor-2.2/neural_compressor/strategy/utils/utility.py`

 * *Files 27% similar despite different names*

```diff
@@ -11,63 +11,79 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Utility functions to export model from PyTorch/TensorFlow to ONNX."""
+"""Tuning utility."""
+import os
+import pickle
+from collections import OrderedDict
+from typing import List, Optional, Any
 
-import numpy as np
-from neural_compressor.utils.utility import LazyImport
+import prettytable
 
-ort = LazyImport('onnxruntime')
-ortq = LazyImport('onnxruntime.quantization')
+from neural_compressor.utils import logger
+from neural_compressor.utils.utility import print_table, dump_table, OpEntry
 
 
-def ONNX2Numpy_dtype(onnx_node_type):
-    """Get Numpy data type from onnx data type.
+class OrderedDefaultDict(OrderedDict):
+    """Ordered default dict."""
+
+    def __missing__(self, key):
+        """Initialize value for the missing key."""
+        self[key] = value = OrderedDefaultDict()
+        return value
+
+
+def extract_data_type(data_type: str) -> str:
+    """Extract data type and signed from data type.
 
     Args:
-        onnx_node_type (str): data type description.
+        data_type: The original data type such as uint8, int8.
 
     Returns:
-        dtype: numpy data type
+        (signed or unsigned, data type without signed)
+    """
+    return ('signed', data_type) if data_type[0] != 'u' else ('unsigned', data_type[1:])
+
+
+def reverted_data_type(signed_flag: str, data_type: str) -> str:
+    """Revert the data type."""
+    return data_type if signed_flag == 'signed' else 'u' + data_type
+
+
+def get_adaptor_name(adaptor):
+    """Get adaptor name.
+
+    Args:
+        adaptor: adaptor instance.
     """
-    # Only record sepcial data type
-    ONNX2Numpy_dtype_mapping = {
-        "tensor(float)": np.float32,
-        "tensor(double)": np.float64,
-    }
-    if onnx_node_type in ONNX2Numpy_dtype_mapping:
-        dtype = ONNX2Numpy_dtype_mapping[onnx_node_type]
-        return dtype
-    else:
-        tmp = onnx_node_type.lstrip('tensor(').rstrip(')')
-        dtype = eval(f'np.{tmp}')
-        return dtype
-
-
-class DummyDataReader(ortq.CalibrationDataReader):
-    """Build dummy datareader for onnx static quantization."""
-
-    def __init__(self, fp32_onnx_path):
-        """Initialize data reader.
-
-        Args:
-            fp32_onnx_path (str): path to onnx file
-        """
-        session = ort.InferenceSession(fp32_onnx_path, None)
-        input_tensors = session.get_inputs()
-        input = {}
-        for node in input_tensors:
-            shape = []
-            for dim in node.shape:
-                shape.append(dim if isinstance(dim, int) else 1)
-            dtype = ONNX2Numpy_dtype(node.type)
-            input[node.name] = np.ones(shape).astype(dtype)
-        self.data = [input]
-        self.data = iter(self.data)
-
-    def get_next(self):
-        """Generate next data."""
-        return next(self.data, None)
+    adaptor_name = type(adaptor).__name__.lower()
+    adaptor_name_lst = ['onnx', 'tensorflow', 'pytorch']
+    for name in adaptor_name_lst:
+        if adaptor_name.startswith(name):
+            return name
+    return ""
+
+
+def build_slave_faker_model():
+    """Slave does not have a model, so construct a fake model.
+
+    Returns:
+        object:  a class object where all properties and methods are virtual.
+    """
+    from ...utils import logger
+    class FakerModel:
+
+        def __call__(self, *args, **kwargs):
+            logger.warning("Slave node has no quantized model, please handle it yourself.")
+
+        def __getitem__(self, key):
+            return self.__getattr__(str(key))
+
+        def __getattr__(self, name):
+            logger.warning("Slave node has no quantized model, please handle it yourself.")
+            return self
+
+    return FakerModel()
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/graph_optimization.py` & `neural_compressor-2.2/neural_compressor/experimental/graph_optimization.py`

 * *Files 0% similar despite different names*

```diff
@@ -21,15 +21,15 @@
 import random
 import tempfile
 import sys
 import numpy as np
 import yaml
 from ..conf.config import Graph_Optimization_Conf
 from ..conf.dotdict import deep_get, deep_set, DotDict
-from ..strategy import STRATEGIES
+from .strategy import EXP_STRATEGIES
 from ..utils import logger
 from ..utils.create_obj_from_config import create_dataloader
 from ..utils.utility import CpuInfo, time_limit
 from .common import Model as NCModel
 from ..model import BaseModel
 from ..model.model import get_model_fwk_name
 
@@ -135,28 +135,28 @@
                 if eval_dataloader_cfg is None:
                     self._eval_func = None
                 else:
                     self._eval_dataloader = create_dataloader(self.framework, eval_dataloader_cfg)
 
         strategy = cfg.tuning.strategy.name.lower()
 
-        assert strategy in STRATEGIES, "Tuning strategy {} is NOT supported".format(strategy)
+        assert strategy in EXP_STRATEGIES, "Tuning strategy {} is NOT supported".format(strategy)
 
         _resume = None
         # check if interrupted tuning procedure exists. if yes, it will resume the
         # whole auto tune process.
         self.resume_file = os.path.abspath(os.path.expanduser(cfg.tuning.workspace.resume)) \
                            if cfg.tuning.workspace and cfg.tuning.workspace.resume else None
         if self.resume_file:
             assert os.path.exists(self.resume_file), \
                 "The specified resume file {} doesn't exist!".format(self.resume_file)
             with open(self.resume_file, 'rb') as f:
                 _resume = pickle.load(f).__dict__
 
-        self.strategy = STRATEGIES[strategy](
+        self.strategy = EXP_STRATEGIES[strategy](
             self._model,
             self.conf,
             None,
             None,
             self._eval_dataloader,
             self._eval_func,
             _resume)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/metric/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/metric/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/metric/bleu.py` & `neural_compressor-2.2/neural_compressor/experimental/metric/bleu.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/metric/bleu_util.py` & `neural_compressor-2.2/neural_compressor/experimental/metric/bleu_util.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/metric/coco_label_map.py` & `neural_compressor-2.2/neural_compressor/experimental/metric/coco_label_map.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/metric/coco_tools.py` & `neural_compressor-2.2/neural_compressor/experimental/metric/coco_tools.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/metric/evaluate_squad.py` & `neural_compressor-2.2/neural_compressor/experimental/metric/evaluate_squad.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/metric/f1.py` & `neural_compressor-2.2/neural_compressor/experimental/metric/f1.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/metric/metric.py` & `neural_compressor-2.2/neural_compressor/experimental/metric/metric.py`

 * *Files 0% similar despite different names*

```diff
@@ -867,24 +867,24 @@
             squares_size = sum(self._hvd.allgather_object(squares_size))       
         return squares_sum / squares_size
 
 
 @metric_registry('topk', 'tensorflow, tensorflow_itex')
 class TensorflowTopK(BaseMetric):
     """Compute Top-k Accuracy classification score for Tensorflow model.
-    
+
     This metric computes the number of times where the correct label is among
     the top k labels predicted.
-    
+
     Attributes:
         k (int): The number of most likely outcomes considered to find the correct label.
         num_correct: The number of predictions that were correct classified.
         num_sample: The total number of predictions.
     """
-    
+
     def __init__(self, k=1):
         """Initialize the k, number of samples and correct predictions.
 
         Args:
             k: The number of most likely outcomes considered to find the correct label.
         """
         self.k = k
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/mixed_precision.py` & `neural_compressor-2.2/neural_compressor/experimental/mixed_precision.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,15 +20,15 @@
 import pickle
 import random
 import sys
 import numpy as np
 from ..conf.config import MixedPrecision_Conf
 from ..conf.pythonic_config import Config
 from ..conf.dotdict import deep_get
-from ..strategy import STRATEGIES
+from .strategy import EXP_STRATEGIES
 from ..utils import logger
 from ..utils.create_obj_from_config import create_dataloader
 from ..utils.utility import CpuInfo, time_limit
 from ..model import BaseModel
 from .graph_optimization import GraphOptimization
 
 class MixedPrecision(GraphOptimization):
@@ -145,28 +145,28 @@
                 if eval_dataloader_cfg is None:
                     self._eval_func = None
                 else:
                     self._eval_dataloader = create_dataloader(self.framework, eval_dataloader_cfg)
 
         strategy = cfg.tuning.strategy.name.lower()
 
-        assert strategy in STRATEGIES, "Tuning strategy {} is NOT supported".format(strategy)
+        assert strategy in EXP_STRATEGIES, "Tuning strategy {} is NOT supported".format(strategy)
 
         _resume = None
         # check if interrupted tuning procedure exists. if yes, it will resume the
         # whole auto tune process.
         self.resume_file = os.path.abspath(os.path.expanduser(cfg.tuning.workspace.resume)) \
                            if cfg.tuning.workspace and cfg.tuning.workspace.resume else None
         if self.resume_file: # pragma: no cover
             assert os.path.exists(self.resume_file), \
                 "The specified resume file {} doesn't exist!".format(self.resume_file)
             with open(self.resume_file, 'rb') as f:
                 _resume = pickle.load(f).__dict__
 
-        self.strategy = STRATEGIES[strategy](
+        self.strategy = EXP_STRATEGIES[strategy](
             self._model,
             self.conf,
             None,
             None,
             self._eval_dataloader,
             self._eval_func,
             _resume)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/model_conversion.py` & `neural_compressor-2.2/neural_compressor/experimental/model_conversion.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/nas/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/nas/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/nas/basic_nas.py` & `neural_compressor-2.2/neural_compressor/experimental/nas/basic_nas.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/nas/dynas.py` & `neural_compressor-2.2/neural_compressor/experimental/nas/dynas.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/nas/nas.py` & `neural_compressor-2.2/neural_compressor/experimental/nas/nas.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/nas/nas_utils.py` & `neural_compressor-2.2/neural_compressor/experimental/nas/nas_utils.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/nas/search_algorithms.py` & `neural_compressor-2.2/neural_compressor/experimental/nas/search_algorithms.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/gradient_sensitivity.py` & `neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/gradient_sensitivity.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/group_lasso.py` & `neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/group_lasso.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/magnitude.py` & `neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/magnitude.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/pattern_lock.py` & `neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/pattern_lock.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruner_legacy/pruner.py` & `neural_compressor-2.2/neural_compressor/experimental/pruner_legacy/pruner.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruning.py` & `neural_compressor-2.2/neural_compressor/experimental/pruning.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruning_recipes/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/pruning_recipes/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruning_recipes/patterns/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/pruning_recipes/patterns/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruning_recipes/patterns/pattern.py` & `neural_compressor-2.2/neural_compressor/experimental/pruning_recipes/patterns/pattern.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruning_recipes/patterns/tile_pattern.py` & `neural_compressor-2.2/neural_compressor/experimental/pruning_recipes/patterns/tile_pattern.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pruning_v2.py` & `neural_compressor-2.2/neural_compressor/experimental/pruning_v2.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/logger.py` & `neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/logger.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/patterns.py` & `neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/patterns.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/prune_utils.py` & `neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/prune_utils.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/pruner.py` & `neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/pruner.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/pruning.py` & `neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/pruning.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/pytorch_pruner/scheduler.py` & `neural_compressor-2.2/neural_compressor/experimental/pytorch_pruner/scheduler.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/quantization.py` & `neural_compressor-2.2/neural_compressor/experimental/quantization.py`

 * *Files 3% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 
 import os
 import pickle
 import random
 import numpy as np
 from .component import Component
 from ..conf.dotdict import deep_get, deep_set, DotDict
-from ..strategy import STRATEGIES
+from .strategy import EXP_STRATEGIES
 from ..utils import logger
 from ..utils.utility import time_limit
 from ..utils.create_obj_from_config import create_dataloader
 from ..model import BaseModel
 from ..model.tensorflow_model import TensorflowQATModel
 from ..model.model import get_model_fwk_name
 from ..conf.config import QuantConf
@@ -140,46 +140,42 @@
             logger.info(f"On the premise that the accuracy meets the conditions, improve the performance.")
 
         if strategy == "mse_v2":
             if not (self.framework.startswith("tensorflow") or self.framework == 'pytorch_fx'):
                 strategy = "basic"
                 logger.warning(f"MSE_v2 does not support {self.framework} now, use basic instead.")
                 logger.warning("Only tensorflow, pytorch_fx is supported by MSE_v2 currently.")
-        assert strategy in STRATEGIES, "Tuning strategy {} is NOT supported".format(strategy)
+        assert strategy in EXP_STRATEGIES, "Tuning strategy {} is NOT supported".format(strategy)
 
         _resume = None
         # check if interrupted tuning procedure exists. if yes, it will resume the
         # whole auto tune process.
         self.resume_file = os.path.abspath(os.path.expanduser(cfg.tuning.workspace.resume)) \
                            if cfg.tuning.workspace and cfg.tuning.workspace.resume else None
         if self.resume_file:
             assert os.path.exists(self.resume_file), \
                 "The specified resume file {} doesn't exist!".format(self.resume_file)
             with open(self.resume_file, 'rb') as f:
                 _resume = pickle.load(f).__dict__
 
-        self.strategy = STRATEGIES[strategy](
+        self.strategy = EXP_STRATEGIES[strategy](
             self._model,
             self.conf,
             self._calib_dataloader,
             self._calib_func,
             self._eval_dataloader,
             self._eval_func,
             _resume,
             self.hooks)
 
         if getattr(self._calib_dataloader, 'distributed', False):
             self.register_hook('on_train_begin', self.strategy.adaptor._pre_hook_for_hvd)
 
     def execute(self):
-        """Quantization execute routinue based on strategy design."""
-        # check here the distributed flag
-        logger.info("..............use_distributed_tuning: {}".format(self.conf.usr_cfg.tuning.use_distributed_tuning))
-        if self.conf.usr_cfg.tuning.use_distributed_tuning:
-            return self.distributed_execute()
+        """Quantization execute routine based on strategy design."""
         try:
             with time_limit(self.conf.usr_cfg.tuning.exit_policy.timeout):
                 logger.debug("Dump user yaml configuration:")
                 logger.debug(self.conf.usr_cfg)
                 self.strategy.traverse()
         except KeyboardInterrupt:
             pass
@@ -196,42 +192,14 @@
             else:
                 logger.error(
                     "Specified timeout or max trials is reached! "
                     "Not found any quantized model which meet accuracy goal. Exit.")
 
             return self.strategy.best_qmodel
 
-    def distributed_execute(self):
-        """Quantization distributed execute routinue based on strategy design."""
-        from ..utils.utility import LazyImport
-        MPI = LazyImport("mpi4py.MPI")
-        comm = MPI.COMM_WORLD
-        try:
-            with time_limit(self.conf.usr_cfg.tuning.exit_policy.timeout):
-                self.strategy.traverse()
-        except KeyboardInterrupt:
-            pass
-        except Exception as e:
-            logger.error("Unexpected exception {} happened during tuning.".format(repr(e)))
-            import traceback
-            traceback.print_exc()
-        finally:
-            if self.strategy.best_qmodel:
-                logger.info(
-                    "Specified timeout or max trials is reached! "
-                    "Found a quantized model which meet accuracy goal. Exit.")
-                self.strategy.deploy_config()
-            else:
-                if comm.Get_rank() != 0:    # slaves have no q model
-                    return None
-                logger.error(
-                    "Specified timeout or max trials is reached! "
-                    "Not found any quantized model which meet accuracy goal. Exit.")
-
-            return self.strategy.best_qmodel
 
     def __call__(self):
         """Automatic quantization tuning main entry point.
 
            This interface works on all the DL frameworks that neural_compressor supports
            and provides three usages:
            a) Fully yaml configuration: User specifies all the info through yaml,
```

### Comparing `neural_compressor-2.1.1/neural_compressor/experimental/scheduler.py` & `neural_compressor-2.2/neural_compressor/experimental/scheduler.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/metric/__init__.py` & `neural_compressor-2.2/neural_compressor/metric/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -14,20 +14,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 """Intel Neural Compressor Metric."""
 
-from .metric import METRICS, Metric, BaseMetric, TensorflowTopK, metric_registry, COCOmAPv2, SquadF1, GeneralTopK
+from .metric import (METRICS, Metric, BaseMetric, TensorflowTopK, metric_registry, COCOmAPv2, SquadF1, GeneralTopK,
+                     register_customer_metric)
 from os.path import dirname, basename, isfile, join
 import glob
 
 modules = glob.glob(join(dirname(__file__), "*.py"))
 
 for f in modules:
     if isfile(f) and not f.startswith('__') and not f.endswith('__init__.py'):
         __import__(basename(f)[:-3], globals(), locals(), level=1)
 
 
 __all__ = ["METRICS", "Metric", "BaseMetric", "TensorflowTopK", "metric_registry",
-           "COCOmAPv2", "SquadF1", "GeneralTopK"]
+           "COCOmAPv2", "SquadF1", "GeneralTopK", "register_customer_metric"]
```

### Comparing `neural_compressor-2.1.1/neural_compressor/metric/bleu.py` & `neural_compressor-2.2/neural_compressor/metric/bleu.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/metric/bleu_util.py` & `neural_compressor-2.2/neural_compressor/metric/bleu_util.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/metric/coco_label_map.py` & `neural_compressor-2.2/neural_compressor/metric/coco_label_map.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/metric/coco_tools.py` & `neural_compressor-2.2/neural_compressor/metric/coco_tools.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/metric/evaluate_squad.py` & `neural_compressor-2.2/neural_compressor/metric/evaluate_squad.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/metric/f1.py` & `neural_compressor-2.2/neural_compressor/metric/f1.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/metric/metric.py` & `neural_compressor-2.2/neural_compressor/metric/metric.py`

 * *Files 3% similar despite different names*

```diff
@@ -33,64 +33,64 @@
 
 class Metric(object):
     """A wrapper of the information needed to construct a Metric.
 
     The metric class should take the outputs of the model as the metric's inputs,
     neural_compressor built-in metric always take (predictions, labels) as inputs, it's
     recommended to design metric_cls to take (predictions, labels) as inputs.
+
+    Args:
+        metric_cls (cls): Should be a instance of sub_class of neural_compressor.metric.BaseMetric
+                          or a customer's metric, which takes (predictions, labels) as inputs.
+        name (str, optional): Name for metric. Defaults to 'user_metric'.
     """
-    
-    def __init__(self, metric_cls, name='user_metric', **kwargs):
-        """Initialize a Metric with needed information.
-        
-        Args:
-            metric_cls (cls): Should be a sub_class of neural_compressor.metric.BaseMetric, 
-                which takes (predictions, labels) as inputs
-            name (str, optional): Name for metric. Defaults to 'user_metric'.
-        """
+
+    def __init__(self, name='user_metric', metric_cls=None, **kwargs):
+        """Initialize a Metric with needed information."""
         self.metric_cls = metric_cls
         self.name = name
         self.kwargs = kwargs
 
+
 @singleton
 class TensorflowMetrics(object):
     """Tensorflow metrics collection.
 
     Attributes:
         metrics: A dict to maintain all metrics for Tensorflow model.
     """
-    
+
     def __init__(self) -> None:
         """Initialize the metrics collection."""
         self.metrics = {}
         self.metrics.update(TENSORFLOW_METRICS)
 
 
 @singleton
 class PyTorchMetrics(object):
     """PyTorch metrics collection.
 
     Attributes:
         metrics: A dict to maintain all metrics for PyTorch model.
     """
-    
+
     def __init__(self) -> None:
         """Initialize the metrics collection."""
         self.metrics = {}
         self.metrics.update(PYTORCH_METRICS)
 
 
 @singleton
 class MXNetMetrics(object):
     """MXNet metrics collection.
 
     Attributes:
         metrics: A dict to maintain all metrics for MXNet model.
     """
-    
+
     def __init__(self) -> None:
         """Initialize the metrics collection."""
         from neural_compressor.adaptor.mxnet_utils.util import check_mx_version
         if check_mx_version('2.0.0'):
             import mxnet.gluon.metric as mx_metrics
         else:
             import mxnet.metric as mx_metrics
@@ -106,29 +106,29 @@
 @singleton
 class ONNXRTQLMetrics(object):
     """ONNXRT QLinear metrics collection.
 
     Attributes:
         metrics: A dict to maintain all metrics for ONNXRT QLinear model.
     """
-    
+
     def __init__(self) -> None:
         """Initialize the metrics collection."""
         self.metrics = {}
         self.metrics.update(ONNXRT_QL_METRICS)
 
 
 @singleton
 class ONNXRTITMetrics(object):
     """ONNXRT Integer metrics collection.
 
     Attributes:
         metrics: A dict to maintain all metrics for ONNXRT Integer model.
     """
-    
+
     def __init__(self) -> None:
         """Initialize the metrics collection."""
         self.metrics = {}
         self.metrics.update(ONNXRT_IT_METRICS)
 
 
 framework_metrics = {"tensorflow": TensorflowMetrics,
@@ -176,15 +176,15 @@
     def __init__(self, framework: str):
         """Initialize the metrics collection based on the framework name.
 
         Args:
             framework: The framwork name.
         """
         assert framework in ("tensorflow", "tensorflow_itex", "keras",
-                            "pytorch", "pytorch_ipex", "pytorch_fx", "onnxrt_qdq",
+                             "pytorch", "pytorch_ipex", "pytorch_fx", "onnxrt_qdq",
                              "onnxrt_qlinearops", "onnxrt_integerops", "mxnet",
                              "onnxruntime"), \
                              "framework support tensorflow pytorch mxnet onnxrt"
         self.metrics = framework_metrics[framework]().metrics
 
     def __getitem__(self, metric_type: str):
         """Get the metric based on the specified type.
@@ -208,26 +208,26 @@
             metric_cls: The metric class.
         """
         assert name not in self.metrics.keys(), 'registered metric name already exists.'
         self.metrics.update({name: metric_cls})
 
 def metric_registry(metric_type: str, framework: str):
     """Decorate for registering all Metric subclasses.
-    
+
     The cross-framework metric is supported by specifying the framework param
     as one of tensorflow, pytorch, mxnet, onnxrt.
-       
+
     Args:
         metric_type: The metric type.
         framework: The framework name.
-    
+
     Returns:
         decorator_metric: The function to register metric class.
     """
-    
+
     def decorator_metric(cls):
         for single_framework in [fwk.strip() for fwk in framework.split(',')]:
             assert single_framework in [
                 "tensorflow",
                 "tensorflow_itex",
                 "keras",
                 "mxnet",
@@ -245,16 +245,16 @@
             registry_metrics[single_framework][metric_type] = cls
         return cls
     return decorator_metric
 
 
 class BaseMetric(object):
     """The base class of Metric."""
-    
-    def __init__(self, metric, single_output = False, hvd = None):
+
+    def __init__(self, metric, single_output=False, hvd=None):
         """Initialize the basic metric.
 
         Args:
             metric: The metric class.
             single_output: Whether the output is single or not, defaults to False.
             hvd: The Horovod class for distributed trainig, defaults to None.
         """
@@ -329,18 +329,18 @@
             hvd: The Horovod class for distributed trainig.
         """
         self._hvd = hvd
 
 
 class WrapPyTorchMetric(BaseMetric):
     """The wrapper of Metric class for PyTorch."""
-    
+
     def update(self, preds, labels=None, sample_weight=None):
         """Convert the prediction to torch.
-        
+
         Args:
             preds: The prediction result.
             labels: The reference. Defaults to None.
             sample_weight: The sampling weight. Defaults to None.
         """
         if self._single_output:
             output = torch.as_tensor(preds)
@@ -355,18 +355,18 @@
     def result(self):
         """Evaluate the difference between predictions and labels."""
         return self._metric.compute()
 
 
 class WrapMXNetMetric(BaseMetric):
     """The wrapper of Metric class for MXNet."""
-    
+
     def update(self, preds, labels=None, sample_weight=None):
         """Convert the prediction to MXNet array.
-           
+
         Args:
             preds: The prediction result.
             labels: The reference. Defaults to None.
             sample_weight: The sampling weight. Defaults to None.
         """
         preds = mx.nd.array(preds)
         labels = mx.nd.array(labels)
@@ -374,27 +374,27 @@
 
     def reset(self):
         """Clear the predictions and labels."""
         self._metric.reset()
 
     def result(self):
         """Evaluate the difference between predictions and labels.
-        
+
         Returns:
             acc: The evaluated result.
         """
         acc_name, acc = self._metric.get()
         return acc
 
 class WrapONNXRTMetric(BaseMetric):
     """The wrapper of Metric class for ONNXRT."""
-    
+
     def update(self, preds, labels=None, sample_weight=None):
         """Convert the prediction to NumPy array.
-           
+
         Args:
             preds: The prediction result.
             labels: The reference. Defaults to None.
             sample_weight: The sampling weight. Defaults to None.
         """
         preds = np.array(preds)
         labels = np.array(labels)
@@ -402,15 +402,15 @@
 
     def reset(self):
         """Clear the predictions and labels."""
         self._metric.reset()
 
     def result(self):
         """Evaluate the difference between predictions and labels.
-        
+
         Returns:
             acc: The evaluated result.
         """
         acc_name, acc = self._metric.get()
         return acc
 
 def _topk_shape_validate(preds, labels):
@@ -482,24 +482,24 @@
             'Shape mismatch, label shape {} vs pred shape {}'.format(label.shape, pred.shape)
     return preds, labels
 
 
 @metric_registry('F1', 'tensorflow, tensorflow_itex, pytorch, mxnet, onnxrt_qlinearops, onnxrt_integerops')
 class F1(BaseMetric):
     """F1 score of a binary classification problem.
-    
+
     The F1 score is the harmonic mean of the precision and recall. 
     It can be computed with the equation: 
     F1 = 2 * (precision * recall) / (precision + recall)
     """
-    
+
     def __init__(self):
         """Initialize the F1 score list."""
         self._score_list = []
-    
+
     def update(self, preds, labels):
         """Add the predictions and labels.
 
         Args:
             preds: The predictions.
             labels: The labels corresponding to the predictions.
         """
@@ -571,24 +571,24 @@
             update_type = 'multilabel'
     return update_type
 
 
 @metric_registry('Accuracy', 'tensorflow, tensorflow_itex, pytorch, onnxrt_qlinearops, onnxrt_integerops')
 class Accuracy(BaseMetric):
     """The Accuracy for the classification tasks.
-    
+
     The accuracy score is the proportion of the total number of predictions
     that were correct classified.
 
     Attributes:
         pred_list: List of prediction to score.
         label_list: List of labels to score.
         sample: The total number of samples.
     """
-    
+
     def __init__(self):
         """Initialize predictions, labels and sample."""
         self.pred_list = []
         self.label_list = []
         self.sample = 0
 
     def update(self, preds, labels, sample_weight=None):
@@ -638,18 +638,18 @@
             allgather_sample = sum(self._hvd.allgather_object(self.sample))
             return allghter_correct_num / allgather_sample
         return correct_num / self.sample
 
 
 class PyTorchLoss():
     """A dummy PyTorch Metric.
-    
+
     A dummy metric that computes the average of predictions and prints it directly.
     """
-    
+
     def __init__(self):
         """Initialize the number of examples, sum of prediction. and device."""
         self._num_examples = 0
         self._device = torch.device('cpu')
         self._sum = torch.tensor(0.0, device=self._device)
 
     def reset(self):
@@ -682,22 +682,22 @@
                                       before it can be computed.")
         return self._sum.item() / self._num_examples
         
         
 @metric_registry('Loss', 'tensorflow, tensorflow_itex, pytorch, onnxrt_qlinearops, onnxrt_integerops')
 class Loss(BaseMetric):
     """A dummy Metric.
-    
+
     A dummy metric that computes the average of predictions and prints it directly.
-    
+
     Attributes:
         sample: The number of samples.
         sum: The sum of prediction.
     """
-    
+
     def __init__(self):
         """Initialize the number of samples, sum of prediction."""
         self.sample = 0
         self.sum = 0
 
     def update(self, preds, labels, sample_weight=None):
         """Add the predictions and labels.
@@ -714,15 +714,15 @@
     def reset(self):
         """Reset the number of samples and total cases to zero."""
         self.sample = 0
         self.sum = 0
 
     def result(self):
         """Compute the  average of predictions.
-        
+
         Returns:
             The dummy loss.
         """
         if getattr(self, '_hvd', None) is not None:
             allgather_sum = sum(self._hvd.allgather_object(self.sum))
             allgather_sample = sum(self._hvd.allgather_object(self.sample))
             return allgather_sum / allgather_sample
@@ -786,20 +786,20 @@
             aes_size = sum(self._hvd.allgather_object(aes_size))       
         return aes_sum / aes_size
 
 
 @metric_registry('RMSE', 'tensorflow, tensorflow_itex, pytorch, mxnet, onnxrt_qlinearops, onnxrt_integerops')
 class RMSE(BaseMetric):
     """Computes Root Mean Squared Error (RMSE) loss.
-    
+
     Attributes:
         mse: The instance of MSE Metric.
 
     """
-    
+
     def __init__(self, compare_label=True):
         """Initialize the mse.
 
         Args:
             compare_label (bool): Whether to compare label. False if there are no labels
               and will use FP32 preds as labels.
         """
@@ -830,26 +830,26 @@
         return np.sqrt(self.mse.result())
 
 
 
 @metric_registry('MSE', 'tensorflow, tensorflow_itex, pytorch, onnxrt_qlinearops, onnxrt_integerops')
 class MSE(BaseMetric):
     """Computes Mean Squared Error (MSE) loss.
-    
+
     Mean Squared Error(MSE) represents the average of the squares of errors.
     For example, the average squared difference between the estimated values
     and the actual values.
-    
+
     Attributes:
         pred_list: List of prediction to score.
         label_list: List of references corresponding to the prediction result.
         compare_label (bool): Whether to compare label. False if there are no labels
                               and will use FP32 preds as labels.
     """
-    
+
     def __init__(self, compare_label=True):
         """Initialize the list of prediction and labels.
 
         Args:
             compare_label: Whether to compare label. False if there are no 
               labels and will use FP32 preds as labels.
         """
@@ -889,24 +889,24 @@
             squares_size = sum(self._hvd.allgather_object(squares_size))       
         return squares_sum / squares_size
 
 
 @metric_registry('topk', 'tensorflow, tensorflow_itex')
 class TensorflowTopK(BaseMetric):
     """Compute Top-k Accuracy classification score for Tensorflow model.
-    
+
     This metric computes the number of times where the correct label is among
     the top k labels predicted.
-    
+
     Attributes:
         k (int): The number of most likely outcomes considered to find the correct label.
         num_correct: The number of predictions that were correct classified.
         num_sample: The total number of predictions.
     """
-    
+
     def __init__(self, k=1):
         """Initialize the k, number of samples and correct predictions.
 
         Args:
             k: The number of most likely outcomes considered to find the correct label.
         """
         self.k = k
@@ -956,24 +956,24 @@
             return allgather_num_correct / allgather_num_sample 
         return self.num_correct / self.num_sample
 
 
 @metric_registry('topk', 'pytorch, mxnet, onnxrt_qlinearops, onnxrt_integerops')
 class GeneralTopK(BaseMetric):
     """Compute Top-k Accuracy classification score.
-    
+
     This metric computes the number of times where the correct label is among
     the top k labels predicted.
-    
+
     Attributes:
         k (int): The number of most likely outcomes considered to find the correct label.
         num_correct: The number of predictions that were correct classified.
         num_sample: The total number of predictions.
     """
-    
+
     def __init__(self, k=1):
         """Initialize the k, number of samples and correct predictions.
 
         Args:
             k: The number of most likely outcomes considered to find the correct label.
         """
         self.k = k
@@ -1019,15 +1019,15 @@
             logger.warning("Sample num during evaluation is 0.")
             return 0
         elif getattr(self, '_hvd', None) is not None:
             allgather_num_correct = sum(self._hvd.allgather_object(self.num_correct))
             allgather_num_sample = sum(self._hvd.allgather_object(self.num_sample))
             return allgather_num_correct / allgather_num_sample
         return self.num_correct / self.num_sample
-    
+
 
 @metric_registry('COCOmAPv2', 'tensorflow, tensorflow_itex, onnxrt_qlinearops, onnxrt_integerops')
 class COCOmAPv2(BaseMetric):
     """Compute mean average precision of the detection task."""
 
     def __init__(self, 
                  anno_path=None, 
@@ -1611,7 +1611,62 @@
         """Compute the ROC score."""
         import sklearn.metrics
         scores = np.squeeze(self.pred_list)
         targets = np.squeeze(self.label_list)
         roc_auc = sklearn.metrics.roc_auc_score(targets, scores)
         acc = sklearn.metrics.accuracy_score(targets, np.round(scores))
         return acc
+
+
+def register_customer_metric(user_metric, framework):
+    """register customer metric class or a dict of built-in metric configures.
+
+    1. neural_compressor have many built-in metrics,
+       user can pass a metric configure dict to tell neural compressor what metric will be use.
+       You also can set multi-metrics to evaluate the performance of a specific model.
+            Single metric:
+                {topk: 1}
+            Multi-metrics:
+                {topk: 1,
+                 MSE: {compare_label: False},
+                 weight: [0.5, 0.5],
+                 higher_is_better: [True, False]
+                }
+    For the built-in metrics, please refer to below link:
+    https://github.com/intel/neural-compressor/blob/master/docs/source/metric.md#supported-built-in-metric-matrix.
+
+    2. User also can get the built-in metrics by neural_compressor.Metric:
+        Metric(name="topk", k=1)
+    3. User also can set specific metric through this api. The metric class should take the outputs of the model or
+       postprocess(if have) as inputs, neural_compressor built-in metric always take(predictions, labels)
+       as inputs for update, and user_metric.metric_cls should be sub_class of neural_compressor.metric.BaseMetric.
+
+    Args:
+        user_metric(neural_compressor.metric.Metric or a dict of built-in metric configurations):
+            The object of Metric or a dict of built-in metric configurations.
+
+        framework: framework, such as: tensorflow, pytorch......
+
+    """
+    if isinstance(user_metric, dict):
+        metric_cfg = user_metric
+    else:
+        if isinstance(user_metric, Metric):
+            if user_metric.metric_cls is None:
+                name = user_metric.name
+                metric_cls = METRICS(framework).metrics[name]
+                metric_cfg = {name: {**user_metric.kwargs}}
+                return metric_cfg
+            else:
+                name = user_metric.name
+                metric_cls = user_metric.metric_cls
+                metric_cfg = {name: {**user_metric.kwargs}}
+        else:
+            for i in ['reset', 'update', 'result']:
+                assert hasattr(user_metric, i), 'Please realise {} function' \
+                                                'in user defined metric'.format(i)
+            metric_cls = type(user_metric).__name__
+            name = 'user_' + metric_cls
+            metric_cfg = {name: id(user_metric)}
+        metrics = METRICS(framework)
+        metrics.register(name, metric_cls)
+    return metric_cfg
```

### Comparing `neural_compressor-2.1.1/neural_compressor/model/__init__.py` & `neural_compressor-2.2/neural_compressor/model/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/model/base_model.py` & `neural_compressor-2.2/neural_compressor/model/base_model.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/model/keras_model.py` & `neural_compressor-2.2/neural_compressor/model/keras_model.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/model/mxnet_model.py` & `neural_compressor-2.2/neural_compressor/model/mxnet_model.py`

 * *Files 1% similar despite different names*

```diff
@@ -50,15 +50,15 @@
     def model(self, model):
         """Set model."""
         self._model = model
 
     def save(self, root=None):
         """Save MXNet model."""
         if root is None:
-            from neural_compressor.conf import config as cfg
+            from neural_compressor import config as cfg
             root = cfg.default_workspace
         root = os.path.abspath(os.path.expanduser(root))
         os.makedirs(os.path.dirname(root), exist_ok=True)
 
         if isinstance(self._model, mx.gluon.HybridBlock):
             self._model.export(root, remove_amp_cast=False)
             logger.info("Save quantized hybrid block model to {}.".format(root))
```

### Comparing `neural_compressor-2.1.1/neural_compressor/model/nets_factory.py` & `neural_compressor-2.2/neural_compressor/model/nets_factory.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/model/onnx_model.py` & `neural_compressor-2.2/neural_compressor/model/onnx_model.py`

 * *Files 11% similar despite different names*

```diff
@@ -40,19 +40,19 @@
         """
         self._model = model if not isinstance(model, str) else onnx.load(model)
         self._model_path = None if not isinstance(model, str) else model
         self._is_large_model = False
         try:
             ort.InferenceSession(self._model.SerializeToString())
         except Exception as e:  # pragma: no cover
-            if 'Message onnx.ModelProto exceeds maximum protobuf size of 2GB' in str(e):
+            if 'maximum protobuf size of 2GB' in str(e) or 'string length exceeds max size' in str(e):
                 self._is_large_model = True
                 if self._model_path is None:
-                    logger.warning('Please use model path instead of onnx model '
-                                   'object to quantize')
+                    logger.warning('Please use model path instead of onnx model object to quantize')
+
         self.node_name_counter = {}
         self._output_name_to_node = {}
         self._input_name_to_nodes = {}
         self._get_input_name_to_nodes(self._model.graph.node)
         self._get_output_name_to_node(self._model.graph.node)
         self._graph_info = {}
         self._get_graph_info()
@@ -136,15 +136,15 @@
             raise ValueError('"root" directory does not exists.')
         if self.is_large_model: # pragma: no cover
             from onnx.external_data_helper import convert_model_to_external_data, \
                 load_external_data_for_model
             load_external_data_for_model(self._model, os.path.split(self._model_path)[0])
             convert_model_to_external_data(self._model,
                                            all_tensors_to_one_file=True,
-                                           location="weights.pb",
+                                           location="int8_weights.pb",
                                            convert_attribute=False)
         onnx.save(self._model, root)
 
     def nodes(self):
         """Return model nodes."""
         return self._model.graph.node
 
@@ -195,14 +195,21 @@
     def get_initializer(self, name):
         """Get an initializer by name."""
         for tensor in self._model.graph.initializer:
             if tensor.name == name:
                 return tensor
         return None
 
+    def get_node(self, name):
+        """Get a node by name."""
+        for node in self._model.graph.node:
+            if node.name == name:
+                return node
+        return None
+
     def remove_initializer(self, tensor):
         """Remove an initializer from model."""
         if tensor in self._model.graph.initializer:
             self._model.graph.initializer.remove(tensor)
 
     def remove_initializers(self, init_to_remove):
         """Remove initializers from model."""
@@ -318,19 +325,29 @@
         return nodes
 
     def get_scale_zero(self, tensor):
         """Help function to get scale and zero_point."""
         if not tensor.endswith('_quantized'):
             logger.debug("Find {} in the quantized graph is not quantized.".format(tensor))
             return None, None
-        input_name_to_nodes = self._input_name_to_nodes
-        node = input_name_to_nodes[tensor][0]
-        scale = "_".join(tensor.split('_')[:-1] + ['scale'])
+        node = self._input_name_to_nodes[tensor][0]
+        parent = self._output_name_to_node[tensor] if tensor in self._output_name_to_node else None
+        direct_int8 = ['Reshape', 'Transpose', 'Squeeze', 'Unsqueeze', 'MaxPool', 'Pad']
+        if parent is not None and parent.op_type in direct_int8:
+            fp32_tensor_name = \
+                parent.input[0].replace('_quantized', '').replace('_QuantizeLinear', '').replace('_QuantizeInput', '')
+        elif node.op_type in ['Gather']:
+            fp32_tensor_name = \
+                node.output[0].replace('_quantized', '').replace('_QuantizeLinear', '').replace('_QuantizeInput', '')
+        else:
+            fp32_tensor_name = \
+                tensor.replace('_quantized', '').replace('_QuantizeLinear', '').replace('_QuantizeInput', '')
+        scale = fp32_tensor_name + '_scale'
         scale_tensor = self.get_initializer(scale)
-        zo = "_".join(tensor.split('_')[:-1] + ['zero_point'])
+        zo = fp32_tensor_name + '_zero_point'
         zo_tensor = self.get_initializer(zo)
 
         #TODO check if scale_tensor and zero_point is needed
         # for bias of qlinearconv, scale and zero_point is not needed
         if (node.op_type == 'QLinearConv' and tensor == node.input[-1]) or \
             (node.op_type == 'QGemm' and tensor == node.input[-3]):
             pass
@@ -465,35 +482,131 @@
                 wait.clear()
         nodes = [i[1] for i in all_nodes.items()]
         assert len(list(set([n.name for n in nodes]))) == \
             len(list(set([n.name for n in self.model.graph.node])))
         self.model.graph.ClearField('node')
         self.model.graph.node.extend(nodes)
 
-    def get_nodes_chain(self, start_node, stop_node, result_chain=[]):
+    def get_nodes_chain(self, start, stop, result_chain=[]):
         """Get nodes chain with given start node and stop node."""
         from collections import deque
-        if not all([isinstance(node, str) for node in start_node]):
-            start_node = deque([node.name for node in start_node])
-        if not all([isinstance(node, str) for node in stop_node]):
-            stop_node = [node.name for node in stop_node]
+        from onnx import NodeProto
+
+        # process start node list
+        start_node = deque()
+        for node in start:
+            if isinstance(node, str):
+                start_node.append(node)
+            elif isinstance(node, NodeProto):
+                start_node.append(node.name)
+            else:
+                assert False, "'get_nodes_chain' function only support list[string]" \
+                              "or list[NodeProto] params"
+        
+        # process stop node list
+        stop_node = []
+        for node in stop:
+            if isinstance(node, str):
+                stop_node.append(node)
+            elif isinstance(node, NodeProto):
+                stop_node.append(node.name)
+            else:
+                assert False, "'get_nodes_chain' function only support list[string]" \
+                              "or list[NodeProto] params"
+
         while start_node:
             node_name = start_node.popleft()
             if node_name in stop_node:
                 continue
             if node_name not in result_chain:
                 result_chain.append(node_name)
             else:
                 continue
 
             node = ortq.find_by_name(node_name, list(self.model.graph.node))
             for parent in self.get_parents(node):
                 start_node.append(parent.name)
 
         return result_chain
+    
+    def find_qkv_in_attention(self, find_all=False):
+        """Find qkv MatMul in Attention.
+
+        Args:
+            find_all (bool, optional): find all qkv MatMul. Defaults to False
+
+        Returns:
+            qkv (list): qkv MatMul list
+        """
+        qkv = []
+        for node in self._model.graph.node:
+            start_node, qkv_nodes_list = None, None
+            if node.op_type == 'SkipLayerNormalization':
+                start_node = node
+                qkv_nodes_list = [
+                    self.match_parent_path(
+                        start_node,
+                        ["MatMul", "Reshape", "Transpose", "Reshape", "MatMul"],
+                        [None, 0, 0, 0, 0],)
+                ]
+            if node.op_type == 'Add':
+                start_node = node
+                qkv_nodes_list = [
+                    # match base attention structure
+                    self.match_parent_path(
+                        start_node,
+                        ["Add", "MatMul", "Reshape", "Transpose", "MatMul"],
+                        [0, None, 0, 0, 0],),
+                    self.match_parent_path(
+                        start_node,
+                        ["Add", "MatMul", "Reshape", "Transpose", "MatMul"],
+                        [1, None, 0, 0, 0]),
+
+                    # match gpt attention no past structure
+                    self.match_parent_path(
+                        start_node,
+                        ["Reshape", "Gemm", "Reshape", "Reshape", "Transpose", "MatMul"],
+                        [ None, 0, 0, 0, 0, 0],
+                        output_name_to_node=self.output_name_to_node,
+                        return_indice=[]),
+
+                    # match bart attention structure
+                    self.match_parent_path(
+                        start_node,
+                        ["Add", "MatMul", "Reshape", "Transpose", "Reshape", "MatMul"],
+                        [0, None, 0, 0, 0, 0]),
+                    self.match_parent_path(
+                        start_node,
+                        ["Add", "MatMul", "Reshape", "Transpose", "Reshape", "MatMul"],
+                        [1, None, 0, 0, 0, 0]),
+                    ]
+
+            if not start_node:
+                continue
+            if not any(qkv_nodes_list):
+                continue
+            qkv_nodes = [qkv for qkv in qkv_nodes_list if qkv is not None][-1]
+            other_inputs = []
+            for input in start_node.input:
+                if input not in self.output_name_to_node:
+                    continue
+                if input == qkv_nodes[0].output[0]:
+                    continue
+                other_inputs.append(input)
+            if len(other_inputs) != 1:
+                continue
+            root_input = other_inputs[0]
+            input_name_to_nodes = self.input_name_to_nodes
+            children = input_name_to_nodes[root_input]
+            children_types = [child.op_type for child in children]
+            if children_types.count("MatMul") == 3:
+                qkv.append([child.name for child in children if child.op_type == "MatMul"])
+                if not find_all:
+                    break
+        return qkv
 
     def export(self, save_path, conf):
         """Export Qlinear to QDQ model."""
         from neural_compressor.experimental.export import onnx_qlinear_to_qdq
         from neural_compressor.config import ONNXQlinear2QDQConfig
         if isinstance(conf, ONNXQlinear2QDQConfig):
             add_nodes, remove_nodes, inits = onnx_qlinear_to_qdq(self._model,
@@ -643,8 +756,8 @@
             )
             if matched_parent is None:
                 return None
 
             matched_parents.append(matched_parent)
             current_node = matched_parent
 
-        return matched_parents
+        return matched_parents
```

### Comparing `neural_compressor-2.1.1/neural_compressor/model/tensorflow_model.py` & `neural_compressor-2.2/neural_compressor/model/tensorflow_model.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,18 +20,19 @@
 import copy
 import os
 import shutil
 import importlib
 from abc import abstractmethod
 import tempfile
 import sys
+import json
 from neural_compressor.utils.utility import LazyImport, compute_sparsity
 from neural_compressor.utils.utility import version1_lt_version2, version1_gt_version2, version1_gte_version2
 from neural_compressor.utils import logger
-from neural_compressor.conf import config as cfg
+from neural_compressor import config as cfg
 from neural_compressor.model.base_model import BaseModel
 
 tf = LazyImport('tensorflow')
 np = LazyImport('numpy')
 
 tensor_to_node = lambda s: list(set([x.split(':')[0] for x in s]))
 
@@ -57,15 +58,20 @@
                 model = tf.keras.models.load_model(model)
                 if isinstance(model, tf.keras.Model) and hasattr(model, 'to_json'):
                     return 'keras'
                 return 'saved_model'
             except:
                 pass
     if isinstance(model, tf.keras.Model) and hasattr(model, 'to_json'):
-        return 'keras'
+        if json.loads(model.to_json())["class_name"] in ["Sequential","Functional"]:
+            # Keras adaptor only support Sequential or Functional model
+            return 'keras'
+        else:
+            # otherwise, the backend will fallback to tensorflow_itex
+            return 'AutoTrackable'
     if isinstance(model, tf.Graph):
         return 'graph'
     elif isinstance(model, tf.compat.v1.GraphDef):
         return 'graph_def'
     elif isinstance(model, tf.compat.v1.estimator.Estimator):
         return 'estimator'
     elif isinstance(model, str):
@@ -178,64 +184,26 @@
         from tensorflow._api.v1.config import experimental
         list_physical_devices = experimental.list_physical_devices
     else:
         list_physical_devices = tf.config.list_physical_devices
 
     try:
         with graph.as_default():
-            if device == "cpu":
-                cpus = list_physical_devices("CPU")
-                node_device = cpus[0].name.replace('physical_device:', '')
-                with graph.device(node_device):
-                    tf.import_graph_def(model, name='')
-            else: # pragma: no cover
-                found_device = False
-                gpus = list_physical_devices("GPU")
-                for gpu in gpus:
-                    if gpu.name.replace('physical_device:', '') == device:
-                        found_device = True
-                xpus = list_physical_devices("XPU")
-                for xpu in xpus:
-                    if xpu.name.replace('physical_device:', '') == device:
-                        found_device = True
-                if found_device:
-                    with graph.device(device):
-                        tf.import_graph_def(model, name='')
-                else:
-                    tf.import_graph_def(model, name='')
+            tf.import_graph_def(model, name='')
     except:
         input_tensor_names, output_tensor_names = validate_and_inference_input_output(\
             model, input_tensor_names, output_tensor_names)
         from neural_compressor.adaptor.tf_utils.util import fix_ref_type_of_graph_def
         from neural_compressor.adaptor.tf_utils.util import strip_unused_nodes
         model = fix_ref_type_of_graph_def(model)
         input_node_names = tensor_to_node(input_tensor_names)
         output_node_names = tensor_to_node(output_tensor_names)
         model = strip_unused_nodes(model, input_node_names, output_node_names)
         with graph.as_default():
-            if device == "cpu":
-                cpus = list_physical_devices("CPU")
-                node_device = cpus[0].name.replace('physical_device:', '')
-                with graph.device(node_device):
-                    tf.import_graph_def(model, name='')
-            else: # pragma: no cover
-                found_device = False
-                gpus = list_physical_devices("GPU")
-                for gpu in gpus:
-                    if gpu.name.replace('physical_device:', '') == device:
-                        found_device = True
-                xpus = list_physical_devices("XPU")
-                for xpu in xpus:
-                    if xpu.name.replace('physical_device:', '') == device:
-                        found_device = True
-                if found_device:
-                    with graph.device(device):
-                        tf.import_graph_def(model, name='')
-                else:
-                    tf.import_graph_def(model, name='')
+            tf.import_graph_def(model, name='')
 
     return graph_session(graph, input_tensor_names, output_tensor_names, **kwargs)
 
 def frozen_pb_session(model, input_tensor_names, output_tensor_names, **kwargs):
     """Build session with frozen pb.
 
     Args:
@@ -584,29 +552,15 @@
         if device == "cpu":
             cpus = list_physical_devices("CPU")
             node_device = cpus[0].name.replace('physical_device:', '')
             with graph.device(node_device):
                 saver = tf.compat.v1.train.import_meta_graph(\
                     os.path.join(model, ckpt_prefix + '.meta'), clear_devices=True)
         else: # pragma: no cover
-            found_device = False
-            gpus = list_physical_devices("GPU")
-            for gpu in gpus:
-                if gpu.name.replace('physical_device:', '') == device:
-                    found_device = True
-            xpus = list_physical_devices("XPU")
-            for xpu in xpus:
-                if xpu.name.replace('physical_device:', '') == device:
-                    found_device = True
-            if found_device:
-                with graph.device(device):
-                    saver = tf.compat.v1.train.import_meta_graph(\
-                        os.path.join(model, ckpt_prefix + '.meta'), clear_devices=True)
-            else:
-                saver = tf.compat.v1.train.import_meta_graph(\
+            saver = tf.compat.v1.train.import_meta_graph(\
                     os.path.join(model, ckpt_prefix + '.meta'), clear_devices=True)
 
         sess.run(tf.compat.v1.global_variables_initializer())
         saver.restore(sess, os.path.join(model, ckpt_prefix))
 
     from neural_compressor.adaptor.tf_utils.util import get_input_output_node_names
     if validate_graph_node(sess.graph.as_graph_def(), tensor_to_node(input_tensor_names)):
@@ -709,14 +663,25 @@
         self._input_tensor_names = []
         self._output_tensor_names = []
         self._model_type = ''
         self._sess = None
         self._iter_op = None
         self._workspace_path = ''
         self._q_config = None
+        self._model_path = None if not isinstance(model, str) else model
+
+    @property
+    def model_path(self):
+        """Return model path."""
+        return self._model_path
+
+    @model_path.setter
+    def model_path(self, path):
+        """Set model path."""
+        self._model_path = path
 
     def framework(self):
         """Return framework."""
         return 'tensorflow'
 
     @property
     def name(self):
```

### Comparing `neural_compressor-2.1.1/neural_compressor/model/torch_model.py` & `neural_compressor-2.2/neural_compressor/model/torch_model.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,15 +20,15 @@
 import copy
 import os
 import inspect
 import sys
 from collections import OrderedDict, UserDict
 from neural_compressor.utils.utility import LazyImport, compute_sparsity
 from neural_compressor.utils import logger
-from neural_compressor.conf import config as cfg
+from neural_compressor import config as cfg
 from neural_compressor.model.base_model import BaseModel
 
 torch = LazyImport('torch')
 yaml = LazyImport('yaml')
 json = LazyImport('json')
 np = LazyImport('numpy')
 onnx = LazyImport('onnx')
@@ -344,45 +344,48 @@
 
     def export(
         self,
         save_path: str,
         conf,
     ):
         """Export PyTorch model to ONNX model."""
+        from packaging.version import Version
+        from ..adaptor.pytorch import get_torch_version
+        version = get_torch_version()
+        if version.release < Version("1.12.0").release: # pragma: no cover
+            assert False, "PyTorch to ONNX export function requires a minimum torch version of {}, " \
+                "but the torch version found is {}".format(Version("1.12.0"), version)
+
         from neural_compressor.experimental.export import (
             torch_to_fp32_onnx,
-            torch_to_int8_onnx
-        )
+            torch_to_int8_onnx)
+
         if conf.dtype == 'int8':
             torch_to_int8_onnx(
-                self.fp32_model,
                 self.model,
-                self.q_config,
                 save_path,
                 conf.example_inputs,
+                self.q_config,
                 opset_version=conf.opset_version,
                 dynamic_axes=conf.dynamic_axes,
                 input_names=conf.input_names,
                 output_names=conf.output_names,
                 quant_format=conf.quant_format,
-                dtype='U8S8',
-                recipe=conf.recipe,
-            )
+                verbose=True,)
         elif conf.dtype == 'fp32':
             torch_to_fp32_onnx(
-                self.fp32_model,
+                self.model,
                 save_path,
                 conf.example_inputs,
                 opset_version=conf.opset_version,
                 dynamic_axes=conf.dynamic_axes,
                 input_names=conf.input_names,
                 output_names=conf.output_names,
                 do_constant_folding=True,
-                verbose=True,
-            )
+                verbose=True,)
         else:   # pragma: no cover
             assert False, "Not allowed dtype: {}, pleas use 'fp32' or 'int8'.".format(conf.dtype)
 
 
 class PyTorchFXModel(PyTorchModel):
     """Build PyTorchFXModel object."""
```

### Comparing `neural_compressor-2.1.1/neural_compressor/objective.py` & `neural_compressor-2.2/neural_compressor/objective.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/__init__.py` & `neural_compressor-2.2/neural_compressor/strategy/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/auto.py` & `neural_compressor-2.2/neural_compressor/strategy/auto.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,94 +11,101 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """The auto tuning strategy."""
-import copy
 from copy import deepcopy
-import numpy as np
-from collections import OrderedDict
 from .strategy import strategy_registry, TuneStrategy, STRATEGIES
 from ..utils import logger
 
-from .utils.tuning_sampler import OpTypeWiseTuningSampler, FallbackTuningSampler, ModelWiseTuningSampler
-from .utils.tuning_structs import OpTuningConfig
-from .utils.constant import TUNING_ITEMS_LST
-
 @strategy_registry
 class AutoTuneStrategy(TuneStrategy):
     """The auto tuning strategy.
-    
+
     There are three stages executed by auto strategy sequentially,
     and the tuning process ends once the condition meets the exit policy.
     """
-    
-    def __init__(self, model, conf, q_dataloader=None, q_func=None, \
-        eval_dataloader=None, eval_func=None, resume=None, q_hooks=None):
+
+    def __init__(self,
+                 model,
+                 conf,
+                 q_dataloader=None,
+                 q_func=None,
+                 eval_func=None,
+                 eval_dataloader=None,
+                 eval_metric=None,
+                 resume=None,
+                 q_hooks=None):
         """Init an auto tuning strategy.
 
         Args:
             model: The FP32 model specified for low precision tuning.
             conf: The Conf class instance includes all user configurations.
             q_dataloader: Data loader for calibration, mandatory for post-training quantization.  Defaults to None.
             q_func: Training function for quantization aware training. Defaults to None. Defaults to None.
-            eval_dataloader: Data loader for evaluation. Defaults to None.
-            eval_func: The evaluation function provided by user. This function takes model as parameter, and 
-                evaluation dataset and metrics should be encapsulated in this function implementation and 
+            eval_func: The evaluation function provided by user. This function takes model as parameter, and
+                evaluation dataset and metrics should be encapsulated in this function implementation and
                 outputs a higher-is-better accuracy scalar value.
+            eval_dataloader: Data loader for evaluation. Defaults to None.
+            eval_metric: Metric for evaluation. Defaults to None.
             resume: The dict containing resume information. Defaults to None.
             q_hooks: The dict of training hooks, supported keys are: on_epoch_begin, on_epoch_end, on_step_begin,
                 on_step_end. Their values are functions to be executed in adaptor layer.. Defaults to None.
         """
-        super().__init__(model, conf, q_dataloader, q_func, eval_dataloader,\
-            eval_func, resume, q_hooks)
+        super().__init__(model=model,
+                         conf=conf,
+                         q_dataloader=q_dataloader,
+                         q_func=q_func,
+                         eval_func=eval_func,
+                         eval_dataloader=eval_dataloader,
+                         eval_metric=eval_metric,
+                         resume=resume,
+                         q_hooks=q_hooks)
         logger.info(f"*** Initialize auto tuning")
-        self.model = model
-        self.conf = conf
-        self.q_dataloader = q_dataloader
-        self.q_func = q_func
-        self.eval_dataloader = eval_dataloader
-        self.eval_func = eval_func
-        self.resume = resume
-        self.q_hooks = q_hooks
         self.strategies_sequence = ['conservative', 'basic']
-        
+
     def sequential_traverse(self):
         """Try different strategies sequentially."""
         pre_strategy = self
         for strategy_name in self.strategies_sequence:
             logger.info(f"*** Start {strategy_name} tuning.")
-            strategy = STRATEGIES[strategy_name](self.model, self.conf, self.q_dataloader, self.q_func, \
-                self.eval_dataloader, self.eval_func, self.resume, self.q_hooks)
-            if pre_strategy:
-                #TODO add tuning history from the previous stage to current stage.
-                strategy.baseline = deepcopy(pre_strategy.baseline)
-                strategy.trials_count = pre_strategy.trials_count
-                strategy.objectives.baseline = deepcopy(pre_strategy.baseline)
+            strategy = STRATEGIES[strategy_name](
+                model = self.model,
+                conf = self.conf,
+                q_dataloader=self.calib_dataloader,
+                q_func=self.q_func,
+                eval_func=self.eval_func,
+                eval_dataloader=self.eval_dataloader,
+                eval_metric=self.eval_metric,
+                resume=self._resume,
+                q_hooks=self.q_hooks,
+                pre_strategy = pre_strategy
+                )
+
             pre_strategy = strategy
             strategy.traverse()
             self.best_qmodel = strategy.best_qmodel
             if self.best_qmodel:
-                return 
+                return
 
     def next_tune_cfg(self):
         """Generate and yield the default tuning config.
 
         Returns:
             tune_config (dict): A dict containing the tuning configuration for quantization.
         """
         tuning_space = self.tuning_space
         calib_sampling_size_lst = tuning_space.root_item.get_option_by_name('calib_sampling_size').options
         _, _, op_tuning_cfg = self.initial_tuning_cfg()
         op_tuning_cfg['calib_sampling_size'] = calib_sampling_size_lst[0]
         logger.info(f"Quantize the model with default config.")
         yield op_tuning_cfg
-     
+
     def traverse(self):
         """Traverse the tuning space."""
         # Quantize model with default config
         super().traverse()
         if self.best_qmodel:
             return
         else:
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/auto_mixed_precision.py` & `neural_compressor-2.2/neural_compressor/experimental/strategy/auto_mixed_precision.py`

 * *Files 0% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 
 """The auto-mixed precision strategy."""
 
 import copy
 import numpy as np
 from collections import OrderedDict
 from .strategy import strategy_registry, TuneStrategy
-from ..utils import logger
+from ...utils import logger
 
 from .utils.tuning_sampler import OpTypeWiseTuningSampler, FallbackTuningSampler
 from .utils.tuning_structs import OpTuningConfig
 
 
 @strategy_registry
 class AutoMixedPrecisionTuneStrategy(TuneStrategy):
@@ -108,20 +108,21 @@
                 yield op_tuning_cfg
 
     def traverse(self):
         """Traverse the tuning space according to auto-mixed precision strategy."""
         # get fp32 model baseline
         self._eval_baseline()
 
+        trials_count = 0
         for op_tuning_cfg in self.next_tune_cfg():
             # add tune_cfg here as quantize use tune_cfg
             tune_cfg = self._tune_cfg_converter(op_tuning_cfg)
-            self.trials_count += 1
+            trials_count += 1
             tuning_history = self._find_tuning_history(tune_cfg)
-            if tuning_history and self.trials_count < self.cfg.tuning.exit_policy.max_trials:
+            if tuning_history and trials_count < self.cfg.tuning.exit_policy.max_trials:
                 self.last_tune_result = tuning_history['last_tune_result']
                 self.best_tune_result = tuning_history['best_tune_result']
                 logger.warn("Find evaluated tuning config, skip.")
                 continue
 
             logger.debug("Dump current mixed precision configuration:")
             logger.debug(tune_cfg)
@@ -134,15 +135,15 @@
                 self._add_tuning_history(copy.deepcopy(tune_cfg), (-1, [0]), q_config=self.last_qmodel.q_config)
                 return
             self.last_tune_cfg = copy.deepcopy(tune_cfg)
             if self.eval_dataloader or self.eval_func:
                 q_config = copy.deepcopy(self.last_qmodel.q_config)
                 self.last_tune_result = self._evaluate(self.last_qmodel)
                 self.cur_best_acc, self.cur_best_tuning_cfg = self.update_best_op_tuning_cfg(op_tuning_cfg)
-                need_stop = self.stop(self.cfg.tuning.exit_policy.timeout, self.trials_count)
+                need_stop = self.stop(self.cfg.tuning.exit_policy.timeout, trials_count)
                 # record the tuning history
                 saved_tune_cfg = copy.deepcopy(tune_cfg)
                 saved_last_tune_result = copy.deepcopy(self.last_tune_result)
                 self._add_tuning_history(saved_tune_cfg, saved_last_tune_result, q_config=q_config)
             else:
                 # If the eval_dataloader was not specified under the config yaml file,
                 # We only converted the model with customized precisions.
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/basic.py` & `neural_compressor-2.2/neural_compressor/strategy/basic.py`

 * *Files 14% similar despite different names*

```diff
@@ -11,60 +11,65 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """The basic tuning strategy."""
-import copy
-import numpy as np
+
+from copy import deepcopy
 from collections import OrderedDict
 from .strategy import strategy_registry, TuneStrategy
 from ..utils import logger
 
-from .utils.tuning_sampler import OpTypeWiseTuningSampler, FallbackTuningSampler, ModelWiseTuningSampler
+from .utils.tuning_sampler import (
+    OpTypeWiseTuningSampler,
+    FallbackTuningSampler,
+    BlockFallbackTuningSampler,
+    LowerBitsSampler)
+
 from .utils.tuning_structs import OpTuningConfig
-from .utils.constant import TUNING_ITEMS_LST
+from .utils.constant import TUNING_ITEMS_LST, PRECISION_LIST, LOWER_BIT_LIST
 
 @strategy_registry
 class BasicTuneStrategy(TuneStrategy):
     """The basic tuning strategy.
-    
+
     There are three stages executed by Basic strategy sequentially,
     and the tuning process ends once the condition meets the exit policy.
     """
 
     def distributed_next_tune_cfg_lst(self, comm):
         """Generate and yield the next tuning config list with below order.
-        
+
             1. OP Type Wise Tuning
             2. Fallback OP One by One
             3. Fallback Multiple OPs Accumulated
 
         Yields:
             tuning_config_list (list): A list containing dicts of the tuning configuration for quantization.
         """
         from copy import deepcopy
         tuning_space = self.tuning_space
         calib_sampling_size_lst = tuning_space.root_item.get_option_by_name('calib_sampling_size').options
         rank = comm.Get_rank()
         for calib_sampling_size in calib_sampling_size_lst:
-            # Initialize the tuning config for each op according to the quantization approach 
+            # Initialize the tuning config for each op according to the quantization approach
             op_item_dtype_dict, quant_mode_wise_items, initial_op_tuning_cfg = self.initial_tuning_cfg()
             # Optype-wise tuning tuning items: the algorithm/scheme/granularity of activation(weight)
             early_stop_tuning = False
             stage1_cnt = 0
             quant_ops = quant_mode_wise_items['static'] if 'static' in quant_mode_wise_items else []
             quant_ops += quant_mode_wise_items['dynamic'] if 'dynamic' in quant_mode_wise_items else []
             stage1_max = 1e9  # TODO set a more appropriate value
-            op_wise_tuning_sampler = OpTypeWiseTuningSampler(tuning_space, [], [], 
+            op_type_wise_tuning_sampler = OpTypeWiseTuningSampler(tuning_space, [], [],
                                                              op_item_dtype_dict, initial_op_tuning_cfg)
             # stage 1: yield op_tune_cfg_lst
             op_tuning_cfg_lst_stage_1 = []
-            for op_tuning_cfg in op_wise_tuning_sampler:
+            for op_tuning_cfg in op_type_wise_tuning_sampler:
                 stage1_cnt += 1
                 if early_stop_tuning and stage1_cnt > stage1_max:
                     logger.info("Early stopping the stage 1.")
                     break
                 op_tuning_cfg['calib_sampling_size'] = calib_sampling_size
                 op_tuning_cfg_lst_stage_1.append(deepcopy(op_tuning_cfg))
             logger.info("yield op_tuning_cfg_lst_stage_1 with length {}".format(len(op_tuning_cfg_lst_stage_1)))
@@ -76,15 +81,15 @@
                 comm.bcast(cur_best_tuning_cfg, root=0)
             else:
                 self.cur_best_tuning_cfg = comm.bcast(cur_best_tuning_cfg, root=0)
 
             # stage 2: yield new_op_tuning_cfg_lst (length of stage 1)
             # Fallback the ops supported both static and dynamic from static to dynamic
             # Tuning items: None
-            if self.cfg.quantization.approach == 'post_training_auto_quant':
+            if self.config.approach == 'post_training_auto_quant':
                 static_dynamic_items = [item for item in tuning_space.query_items_by_quant_mode('static') if
                                         item in tuning_space.query_items_by_quant_mode('dynamic')]
                 if static_dynamic_items:
                     logger.info("Fallback all ops that support both dynamic and static to dynamic.")
                 else:
                     logger.info("Non ops that support both dynamic")
 
@@ -157,17 +162,82 @@
                     for op_tuning_cfg in fallback_sampler:
                         op_tuning_cfg['calib_sampling_size'] = calib_sampling_size
                         # yield op_tuning_cfg
                         op_tuning_cfg_lst_stage_4.append(deepcopy(op_tuning_cfg))
                     logger.info("yield op_tuning_cfg_lst_stage_4 with length {}".format(len(op_tuning_cfg_lst_stage_4)))
                     yield op_tuning_cfg_lst_stage_4
 
+    def fallback_by_block(self, fallback_items_lst, best_op_tuning_cfg_stage1, target_dtype, tuning_space,\
+        calib_sampling_size):
+        """Fallback ops by block.
+
+        Step 1. block by block
+        Step 2. accumulate block
+
+        Args:
+            fallback_items_lst (list): list of fallback items
+            best_op_tuning_cfg_stage1 (dict): best op tuning cfg of stage1
+            target_dtype (str): target dtype
+            tuning_space (TuningSpace): Tuning space
+
+        Yields:
+            dict: op_tuning_cfg fall-backed by block
+        """
+        from copy import deepcopy
+        op_block_lst = self.capability.get('block_wise', [])
+        if op_block_lst:
+            # Fallback block by block
+            fallback_items_name_lst = [item.name for item in fallback_items_lst]
+            op_block_fallback_lst = []
+            for op_block_index, op_block in enumerate(op_block_lst):
+                if not fallback_items_name_lst:
+                    break
+                matches = [item for item in op_block if item in fallback_items_name_lst]
+                if matches:
+                    op_block_fallback_lst.append(op_block)
+
+            initial_op_tuning_cfg = deepcopy(best_op_tuning_cfg_stage1)
+            # Fallback by accumulating blocks
+            if op_block_fallback_lst:
+                logger.info(f"Start to fallback op to {target_dtype} by blocks")
+            block_fallback_sampler = BlockFallbackTuningSampler(tuning_space=tuning_space,
+                                                                tuning_order_lst=[],
+                                                                initial_op_tuning_cfg=initial_op_tuning_cfg,
+                                                                op_block_lst=op_block_fallback_lst,
+                                                                accumulate=True,
+                                                                target_dtype=target_dtype)
+            for op_block_index, op_tuning_cfg in enumerate(block_fallback_sampler):
+                op_tuning_cfg['calib_sampling_size'] = calib_sampling_size
+                yield op_tuning_cfg
+
+    def quant_to_lower_bits(self, initial_op_tuning_cfg, calib_sampling_size):
+        """Quantize ops into lower bits, such as int4.
+
+        Args:
+            initial_op_tuning_cfg: the initial tuning config
+            calib_sampling_size: _description_
+
+        Yields:
+            tuning config
+        """
+        for quant_bit in LOWER_BIT_LIST:
+            logger.info(f"Start to quantize ops into {quant_bit}")
+            ops = self.tuning_space.collect_op_by_quant_bits(quant_bit)
+            op_item_dtype_dict = {op.name: quant_bit for op in ops}
+            lower_bits_sampler = LowerBitsSampler(deepcopy(self.tuning_space), [],
+                                                  initial_op_tuning_cfg, op_item_dtype_dict,
+                                                  accumulate=False, skip_first=True)
+            for tune_cfg in lower_bits_sampler:
+                tune_cfg['calib_sampling_size'] = calib_sampling_size
+                yield tune_cfg
+
+
     def next_tune_cfg(self):
         """Generate and yield the next tuning config with below order.
-        
+
             1. OP Type Wise Tuning: tries to quantize the OPs as many as possible
                 and traverse all OP type wise tuning configs
             2. Fallback OP One by One: it performs high-precision OP (FP32, BF16 ...)
                 fallbacks one by one based on the tuning config with the best result
                 in the previous stage, and records the impact of each OP.
             3. Fallback Multiple OPs Accumulated: first sorted the OPs list
                 according to the impact score in stage II, and tries to incrementally
@@ -184,38 +254,47 @@
             op_item_dtype_dict, quant_mode_wise_items, initial_op_tuning_cfg = self.initial_tuning_cfg()
             # Optype-wise tuning tuning items: the algorithm/scheme/granularity of activation(weight)
             early_stop_tuning = False
             stage1_cnt = 0
             quant_ops = quant_mode_wise_items.get('static', [])
             quant_ops += quant_mode_wise_items.get('dynamic', [])
             stage1_max = 1e9  # TODO set a more appropriate value
-            op_wise_tuning_sampler = OpTypeWiseTuningSampler(tuning_space, [], [], 
-                                                             op_item_dtype_dict, initial_op_tuning_cfg)
-            for index, op_tuning_cfg in enumerate(op_wise_tuning_sampler):
+            op_type_wise_tuning_sampler = OpTypeWiseTuningSampler(tuning_space, [], [],\
+                op_item_dtype_dict, initial_op_tuning_cfg)
+            for index, op_tuning_cfg in enumerate(op_type_wise_tuning_sampler):
+                initial_op_tuning_cfg['calib_sampling_size'] = calib_sampling_size
+                if not self.cur_best_tuning_cfg:
+                    self.cur_best_tuning_cfg = deepcopy(initial_op_tuning_cfg)
                 op_tuning_cfg['calib_sampling_size'] = calib_sampling_size
+                # try to quantizing ops into lower bits, such as int4,
+                # if accuracy meets the requirements after first trial and max_trials > 1
+                if index == 1 and self.objectives.accuracy_meet_req(deepcopy(self.last_tune_result)):
+                    for op_tuning_cfg in self.quant_to_lower_bits(self.cur_best_tuning_cfg, calib_sampling_size):
+                        yield op_tuning_cfg
                 # Apply all recipes, if not got the qmodel that meet the requirements, discard it.
                 if index == 1 and not self.applied_all_recipes_flag:
                     logger.info("Apply all recipes.")
                     self.applied_all_recipes_flag = True
                     yield self.apply_all_tuning_recipes(deepcopy(self.cur_best_tuning_cfg))
                 stage1_cnt += 1
                 if early_stop_tuning and stage1_cnt > stage1_max:
                     logger.info("Early stopping the stage 1.")
                     break
                 yield op_tuning_cfg
-            
+
+            # TODO Add the lower bits quantization here
             # Apply all recipes, if not got the qmodel that meet the requirements, discard it.
             if stage1_cnt == 1 and not self.applied_all_recipes_flag:
                 logger.info("Apply all recipes.")
                 self.applied_all_recipes_flag = True
                 yield self.apply_all_tuning_recipes(deepcopy(self.cur_best_tuning_cfg))
-            
+
             # Fallback the ops supported both static and dynamic from static to dynamic
             # Tuning items: None
-            if self.cfg.quantization.approach == 'post_training_auto_quant':
+            if self.config.approach == 'post_training_auto_quant':
                 static_dynamic_items = [item for item in tuning_space.query_items_by_quant_mode('static') if
                                         item in tuning_space.query_items_by_quant_mode('dynamic')]
                 if static_dynamic_items:
                     logger.info("Fallback all ops that support both dynamic and static to dynamic.")
                 else:
                     logger.info("Non ops that support both dynamic")
 
@@ -228,17 +307,25 @@
 
             logger.info("Apply recipe one by one.")
             for tune_cfg in self.apply_recipe_one_by_one(deepcopy(self.cur_best_tuning_cfg)):
                 yield tune_cfg
             best_op_tuning_cfg_stage1 = deepcopy(self.cur_best_tuning_cfg)
 
             # Fallback
-            for target_dtype in ['bf16', 'fp32']:
+            for target_dtype in PRECISION_LIST:
                 target_type_lst = set(tuning_space.query_items_by_quant_mode(target_dtype))
                 fallback_items_lst = [item for item in quant_ops if item in target_type_lst]
+
+                # Fallback block by block
+                for op_tuning_cfg in self.fallback_by_block(fallback_items_lst, best_op_tuning_cfg_stage1,
+                                                             target_dtype,
+                                                             tuning_space,
+                                                            calib_sampling_size):
+                    yield op_tuning_cfg
+
                 if fallback_items_lst:
                     logger.info(f"Start to fallback op to {target_dtype} one by one.")
                     self._fallback_started()
                 fallback_items_name_lst = [item.name for item in fallback_items_lst][::-1] # from bottom to up
                 op_dtypes = OrderedDict(zip(fallback_items_name_lst, [target_dtype] * len(fallback_items_name_lst)))
                 initial_op_tuning_cfg = deepcopy(best_op_tuning_cfg_stage1)
                 fallback_sampler = FallbackTuningSampler(tuning_space, tuning_order_lst=[],
@@ -262,15 +349,15 @@
                     initial_op_tuning_cfg = deepcopy(best_op_tuning_cfg_stage1)
                     fallback_sampler = FallbackTuningSampler(tuning_space, tuning_order_lst=[],
                                                             initial_op_tuning_cfg=initial_op_tuning_cfg,
                                                             op_dtypes=op_dtypes, accumulate=True)
                     for op_tuning_cfg in fallback_sampler:
                         op_tuning_cfg['calib_sampling_size'] = calib_sampling_size
                         yield op_tuning_cfg
-                        
+
     def _initial_dynamic_cfg_based_on_static_cfg(self, op_static_cfg:OpTuningConfig):
         op_state = op_static_cfg.get_state()
         op_name = op_static_cfg.op_name
         op_type = op_static_cfg.op_type
         op_name_type = (op_name, op_type)
         op_quant_mode = 'dynamic'
         tuning_space = self.tuning_space
@@ -287,9 +374,8 @@
                     dynamic_state[att_and_method_name] = method_val
                 else:
                     quant_mode_item = tuning_space.get_item_by_path((op_name_type, *full_path[att]))
                     if quant_mode_item and quant_mode_item.get_option_by_name(att_and_method_name):
                         tuning_item = quant_mode_item.get_option_by_name(att_and_method_name)
                         dynamic_state[att_and_method_name] = tuning_item.options[0] if tuning_item else None
         return OpTuningConfig(op_name, op_type, op_quant_mode, tuning_space, kwargs=dynamic_state)
-        
-        
+
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/bayesian.py` & `neural_compressor-2.2/neural_compressor/experimental/strategy/bayesian.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,28 +13,25 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """The Bayesian tuning strategy."""
 
-import copy
 import warnings
 import numpy as np
 from scipy.optimize import minimize
 from sklearn.gaussian_process.kernels import Matern
 from sklearn.gaussian_process import GaussianProcessRegressor
 
-from collections import OrderedDict
 from copy import deepcopy
 
-from ..utils import logger
+from ...utils import logger
 from .strategy import strategy_registry, TuneStrategy
 from .utils.tuning_sampler import OpWiseTuningSampler
-from .utils.tuning_structs import OpTuningConfig
 
 
 @strategy_registry
 class BayesianTuneStrategy(TuneStrategy):
     """The Bayesian tuning strategy."""
     
     def __init__(self, model, conf, q_dataloader, q_func=None, eval_dataloader=None,
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/conservative.py` & `neural_compressor-2.2/neural_compressor/strategy/conservative.py`

 * *Files 12% similar despite different names*

```diff
@@ -29,25 +29,56 @@
 from ..utils import logger
 from ..utils.utility import Statistics
 from ..algorithm import AlgorithmScheduler
 
 @strategy_registry
 class ConservativeTuneStrategy(TuneStrategy):
     """Tuning strategy with accuracy first, performance second.
-    
+
     The quantization level O0 is designed for user who want to keep the accuracy
     of the model after quantization. It starts with the original(fp32) model,
     and then quantize the OPs to lower precision OP type wisely and OP wisely.
     """
 
-    def __init__(self, model, conf, q_dataloader, q_func=None, eval_dataloader=None, 
-                 eval_func=None, dicts=None, q_hooks=None):
-        """Init conservative tuning strategy."""
-        super().__init__(model, conf, q_dataloader, q_func, eval_dataloader, 
-                         eval_func, dicts, q_hooks)
+    def __init__(self,
+                 model,
+                 conf,
+                 q_dataloader=None,
+                 q_func=None,
+                 eval_func=None,
+                 eval_dataloader=None,
+                 eval_metric=None,
+                 resume=None,
+                 q_hooks=None):
+        """Init conservative tuning strategy.
+
+        Args:
+            model: The FP32 model specified for low precision tuning.
+            conf: The Conf class instance includes all user configurations.
+            q_dataloader: Data loader for calibration, mandatory for post-training quantization.  Defaults to None.
+            q_func: Training function for quantization aware training. Defaults to None. Defaults to None.
+            eval_func: The evaluation function provided by user. This function takes model as parameter, and
+                evaluation dataset and metrics should be encapsulated in this function implementation and
+                outputs a higher-is-better accuracy scalar value.
+            eval_dataloader: Data loader for evaluation. Defaults to None.
+            eval_metric: Metric for evaluation. Defaults to None.
+            resume: The dict containing resume information. Defaults to None.
+            q_hooks: The dict of training hooks, supported keys are: on_epoch_begin, on_epoch_end, on_step_begin,
+                on_step_end. Their values are functions to be executed in adaptor layer.. Defaults to None.
+        """
+        super().__init__(model=model,
+                         conf=conf,
+                         q_dataloader=q_dataloader,
+                         q_func=q_func,
+                         eval_func=eval_func,
+                         eval_dataloader=eval_dataloader,
+                         eval_metric=eval_metric,
+                         resume=resume,
+                         q_hooks=q_hooks)
+        logger.info(f"*** Initialize conservative tuning")
         self.acc_meet_flag = False
         self.quant_op_type_lst = ['conv', 'matmul', 'linear']
         res_lst = [None] * len(self.quant_op_type_lst)
         self.quant_status = {k : v for k, v in zip(self.quant_op_type_lst, res_lst)}
 
     def next_tune_cfg(self):
         """Generate and yield the next tuning config with below order.
@@ -94,28 +125,28 @@
         self.re_quant = False
 
     def _get_op_type_priority(self):
         optypewise_cap = self.capability['optypewise']
         op_type_priority = list(optypewise_cap.keys())
         return op_type_priority
 
-    def _sorted_item_by_op_type(self, 
-                                items_lst, 
+    def _sorted_item_by_op_type(self,
+                                items_lst,
                                 op_type_priority: List[str]) -> OrderedDict[str, List]:
         """Scoring the tuning items according to its op type.
-        
+
         Args:
             items_lst: The tuning item list. # [(op_item, quant_mode), ... ]
             op_type_priority: The op type list with the order. # [optype_1, optype_2]
 
         Returns:
             The tuning items list that sorted according to its op type.
             OrderDict:
                 # op_type: [(TuningItem, quant_mode), ...]
-                conv: [(TuningItem, static), (TuningItem, static)] 
+                conv: [(TuningItem, static), (TuningItem, static)]
                 linear: [(TuningItem, static), (TuningItem, static)]
                 matmul: [(TuningItem, static), (TuningItem, static)]
         """
         sorted_items = COrderedDict()
         for op_item, quant_mode in items_lst:
             op_name, op_type = op_item.name
             for target_op_type in self.quant_op_type_lst:
@@ -130,19 +161,19 @@
 
         Initialize the tuning config for conservative tuning.
 
         Returns:
             op_item_dtype_dict (OrderedDict): key is (op_name, op_type); value is quantization mode.
             quant_mode_wise_items (OrderedDict): key is quant_mode/precision; value is item list.
             initial_op_tuning_cfg (OrderedDict): key is (op_name, op_type); value is the initialized tuning config.
-        
+
         """
         from .utils.constant import auto_query_order_o0 as query_order
         from .utils.tuning_space import initial_tuning_cfg_with_quant_mode
-        
+
         quant_mode_wise_items = OrderedDict() # mode, op_item_lst
         pre_items = set()
         # Collect op items supported the specified mode.
         for quant_mode in query_order:
             items = self.tuning_space.query_items_by_quant_mode(quant_mode)
             filtered_items = list(filter(lambda item: item not in pre_items, items))
             pre_items = pre_items.union(set(items))
@@ -162,25 +193,25 @@
                                                                                      quant_mode,
                                                                                      self.tuning_space)
         return op_item_dtype_dict, quant_mode_wise_items, initial_op_tuning_cfg
 
     def _quant_items_pool(self, op_type_priority: List[str]) -> OrderedDict[
         str, OrderedDict[str, List[Tuple[TuningItem, str]]]]:
         """Create the op queue to be quantized.
-        
+
         Args:
             op_type_priority: The optype list with priority.
-            
+
         Returns:
             The op item pool to convert into lower precision.
             quant_items_pool(OrderDict):
                 int8:
                     OrderDict:
                         # (TuningItem, quant_mode)
-                        conv2d: [(TuningItem, static), (TuningItem, static)] 
+                        conv2d: [(TuningItem, static), (TuningItem, static)]
                         linear: [(TuningItem, static), (TuningItem, static)]
         """
         quant_mode_wise_items = self.tuning_space.quant_mode_wise_items
         # Add all quantized pair into queue
         quant_items_pool = COrderedDict()
         op_item_pairs = []
         quant_ops_name_set = set()
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/exhaustive.py` & `neural_compressor-2.2/neural_compressor/experimental/strategy/exhaustive.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,20 +11,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """The exhaustive tuning strategy."""
-from collections import OrderedDict
 from .strategy import strategy_registry, TuneStrategy
-
-from .utils.tuning_sampler import OpWiseTuningSampler, FallbackTuningSampler, ModelWiseTuningSampler
-from .utils.tuning_structs import OpTuningConfig
-from ..utils import logger
+from .utils.tuning_sampler import OpWiseTuningSampler
 
 @strategy_registry
 class ExhaustiveTuneStrategy(TuneStrategy):
     """The exhaustive tuning strategy."""
 
     def next_tune_cfg(self):
         """Generate and yield the next tuning config using exhaustive search in tuning space.
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/hawq_v2.py` & `neural_compressor-2.2/neural_compressor/strategy/hawq_v2.py`

 * *Files 3% similar despite different names*

```diff
@@ -24,24 +24,24 @@
 from .utils.tuning_structs import OpTuningConfig
 from .utils.constant import TUNING_ITEMS_LST
 from ..utils import logger
 
 @strategy_registry
 class HAWQ_V2TuneStrategy(TuneStrategy):
     """The HAWQ V2 tuning strategy.
-    
+
     HAWQ_V2 implements the "Hawq-v2: Hessian aware trace-weighted quantization of neural networks".
     We made a small change to it by using the hessian trace to score the op impact and then
     fallback the OPs according to the scoring result.
-    
+
     """
 
     def next_tune_cfg(self):
         """Generate and yield the next tuning config using HAWQ v2 search in tuning space.
-    
+
         Returns:
             tune_config (dict): A dict containing the tuning configuration for quantization.
         """
         tuning_space = self.tuning_space
         calib_size = tuning_space.root_item.get_option_by_name('calib_sampling_size').options[0]
 
         # Initialize the tuning config for each op according to the quantization approach
@@ -59,23 +59,26 @@
             if early_stop_tuning and stage1_cnt > stage1_max:
                 logger.info("Early stopping the stage 1.")
                 break
             op_tuning_cfg['calib_sampling_size'] = calib_size
             yield op_tuning_cfg
         # Start compute the hessian trace
         logger.info(f"**************  Start compute the hessian trace  *****************")
-        target_dtype = "fp32"  
-        hawq_v2_criterion =self.cfg.tuning.strategy.hawq_v2_loss
+        target_dtype = "fp32"
+        hawq_v2_criterion = None
+        strategy_kwargs = self.config.tuning_criterion.strategy_kwargs
+        if strategy_kwargs:
+            hawq_v2_criterion = strategy_kwargs.get('hawq_v2_loss', None)
         # assert hawq_v2_criterion is not None, "HAWQ-V2 strategy needs model loss function to compute the gradient, \
         #     Please assign it by strategy_kwargs({'hawq_v2_loss': hawq_v2_loss})."
-        op_to_traces = self.adaptor.calculate_hessian_trace(fp32_model = self._fp32_model,
-                                                            dataloader = self.calib_dataloader,
-                                                            q_model = self.last_qmodel,
-                                                            criterion =hawq_v2_criterion,
-                                                            enable_act = False)
+        op_to_traces = self.adaptor.calculate_hessian_trace(fp32_model=self.model,
+                                                            dataloader=self.calib_dataloader,
+                                                            q_model=self.last_qmodel,
+                                                            criterion=hawq_v2_criterion,
+                                                            enable_act=False)
         sorted_op_to_traces = dict(sorted(op_to_traces.items(), key=lambda item: item[1], reverse=True))
         logger.info(f"**************  Hessian Trace  *****************")
         for op_name, trace in sorted_op_to_traces.items():
             logger.info(f"*** op: {op_name}, hessian trace : {trace}")
         logger.info(f"************************************************")
         # WA for op mapping
         ordered_ops_tmp = {}
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/mse.py` & `neural_compressor-2.2/neural_compressor/experimental/strategy/mse.py`

 * *Files 0% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 # limitations under the License.
 """MSE tuning strategy."""
 from copy import deepcopy
 import numpy as np
 from collections import OrderedDict
 from typing import Dict, Any, List
 from .strategy import strategy_registry, TuneStrategy
-from ..utils import logger
+from ...utils import logger
 from time import time 
 
 from .utils.tuning_sampler import OpTypeWiseTuningSampler, FallbackTuningSampler
 from .utils.tuning_structs import OpTuningConfig
 
 @strategy_registry
 class MSETuneStrategy(TuneStrategy):
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/mse_v2.py` & `neural_compressor-2.2/neural_compressor/strategy/mse_v2.py`

 * *Files 7% similar despite different names*

```diff
@@ -18,81 +18,60 @@
 import copy
 from copy import deepcopy
 import numpy as np
 from collections import OrderedDict
 from typing import Dict, Any, List
 from .strategy import strategy_registry, TuneStrategy
 from ..utils import logger
-from time import time 
+from time import time
 
 from .utils.tuning_sampler import OpTypeWiseTuningSampler
 from .utils.tuning_structs import OpTuningConfig
 from .utils.constant import PRECISION_LIST
 @strategy_registry
 class MSE_V2TuneStrategy(TuneStrategy):
     """The `mse_v2` tuning strategy.
-    
-    MSE_v2 is a strategy with a two stages fallback and revert fallback. 
+
+    MSE_v2 is a strategy with a two stages fallback and revert fallback.
     Note that, only tensorflow framework and pytorch FX backend is currently supported for mse_v2
     tuning strategy.
     """
-    
+
     def _tuning_record_msg(self, records):
         records_str_lst = [[str(e) for e in record] for record in records]
         record_msg = '\n'.join(','.join(record) for record in records_str_lst)
         return record_msg
 
     def next_tune_cfg(self):
         """Generate and yield the next tuning config with below order.
-           
+
            1. In the fallback stage, it uses multi-batch data to score the op impact
             and then fallback the op with the highest score util found the quantized model
             that meets accuracy criteria.
            2. In the revert fallback stage, it also scores
             the impact of fallback OPs in the previous stage and selects the op
             with the lowest score to revert the fallback until the quantized model
             that does not meets accuracy criteria.
-    
+
         Returns:
             tune_config (dict): A dict containing the tuning configuration for quantization.
         """
         from copy import deepcopy
         tuning_space = self.tuning_space
         initial_op_tuning_cfg = {}
-        for item in tuning_space.root_item.options:
-            if item.item_type == 'op':
-                op_name, op_type = item.name
-                initial_op_tuning_cfg[item.name] = OpTuningConfig(op_name, op_type, 'fp32', tuning_space)
         calib_sampling_size_lst = tuning_space.root_item.get_option_by_name('calib_sampling_size').options
         for calib_sampling_size in calib_sampling_size_lst:
-            # Collect the ops that support static and dynamic
-            quant_mode_wise_items = OrderedDict()
-            query_order = ['static', 'dynamic', 'bf16', 'fp16', 'fp32']
-            pre_items = set()
-            for quant_mode in query_order:
-                items = tuning_space.query_items_by_quant_mode(quant_mode)
-                filtered_items = [item for item in items if item not in pre_items]
-                pre_items = pre_items.union(set(items))
-                quant_mode_wise_items[quant_mode] = filtered_items
-
-            def initial_op_quant_mode(items_lst, target_quant_mode, op_item_dtype_dict):
-                for item in items_lst:
-                    op_item_dtype_dict[item.name] = target_quant_mode
-
-            op_item_dtype_dict = OrderedDict()
-            for quant_mode, quant_mode_items in quant_mode_wise_items.items():
-                initial_op_quant_mode(quant_mode_items, quant_mode, op_item_dtype_dict)
-
+            op_item_dtype_dict, quant_mode_wise_items, initial_op_tuning_cfg = self.initial_tuning_cfg()
             quant_ops = quant_mode_wise_items.get('static', [])
             quant_ops += quant_mode_wise_items.get('dynamic', [])
-            # Optype-wise tuning 
+            # Optype-wise tuning
             early_stop_tuning = True
             stage1_cnt = 0
             stage1_max = 2  # TODO set a more appropriate value
-            op_wise_tuning_sampler = OpTypeWiseTuningSampler(tuning_space, [], [], 
+            op_wise_tuning_sampler = OpTypeWiseTuningSampler(tuning_space, [], [],
                                                              op_item_dtype_dict, initial_op_tuning_cfg)
             for op_tuning_cfg in op_wise_tuning_sampler:
                 stage1_cnt += 1
                 if early_stop_tuning and stage1_cnt > stage1_max:
                     logger.info("Early stopping the stage 1.")
                     break
                 op_tuning_cfg['calib_sampling_size'] = calib_sampling_size
@@ -123,16 +102,19 @@
             #     2) fallback the op with higher sensitivity accumulatively
             # 2. after the accuracy requirements met:  # to improve the performance
             #     1) calculate the sensitivity of fp32 ops in the current state
             #     2) re-quantize the op with lower sensitivity accumulatively
             tune_cfg = deepcopy(self.cur_best_tuning_cfg)
             requantize_cfg = deepcopy(self._tune_cfg_converter(self.cur_best_tuning_cfg))
             self.output_op_names = self.adaptor.get_output_op_names(self.last_qmodel)
-            self.confidence_batches = (self.cfg.tuning.strategy.confidence_batches
-                                       if self.cfg.tuning.strategy.confidence_batches != None else 2)
+            confidence_batches = 2
+            strategy_kwargs = self.config.tuning_criterion.strategy_kwargs
+            if strategy_kwargs and strategy_kwargs.get('confidence_batches', None):
+                confidence_batches = strategy_kwargs.get('confidence_batches', None)
+
             tune_cfg_backup = deepcopy(tune_cfg)
             quant_ops_in_tune_cfg = self._collect_ops_by_quant_mode(tune_cfg, 'dynamic') + \
                                     self._collect_ops_by_quant_mode(tune_cfg, 'static')
             op_quant_cfgs = {op_info: tune_cfg_backup[op_info] for op_info in quant_ops_in_tune_cfg}
             fallback_records = []
             self.re_quant = True
             for target_dtype in PRECISION_LIST:
@@ -143,66 +125,66 @@
                     logger.info(f"Start to fallback op to {target_dtype}.")
                 else:
                     logger.info(f"No op support {target_dtype}.")
                     continue
                 while not self.objectives.compare(self.last_tune_result, self.baseline):
                     # Record the time of calculating the sensitivity
                     start = time()
-                    ops_lst = self.adaptor.calculate_op_sensitivity(self.model, 
-                                                                    self.calib_dataloader, 
-                                                                    deepcopy(self._tune_cfg_converter(tune_cfg)), 
+                    ops_lst = self.adaptor.calculate_op_sensitivity(self.model,
+                                                                    self.calib_dataloader,
+                                                                    deepcopy(self._tune_cfg_converter(tune_cfg)),
                                                                     self.output_op_names,
-                                                                    self.confidence_batches,
+                                                                    confidence_batches,
                                                                     fallback=True)
                     if not ops_lst:
                         logger.debug(f" Try to fallback to next data type.")
                         break
                     logger.debug(f"*** The op sensitivity analysis took {time() - start:.2f}s.")
                     select_op_info = ops_lst[0]
                     logger.debug(f"*** ops_lst({len(ops_lst)}): {ops_lst} ")
                     logger.info(f"*** The op {select_op_info} have the highest sensitivity in the current state, \
                         fallback it to {target_dtype}.")
                     tune_cfg[select_op_info] = OpTuningConfig(select_op_info[0],
                                                                 select_op_info[1],
-                                                                target_dtype, 
+                                                                target_dtype,
                                                                 self.tuning_space)
                     # Record the fallback history
-                    if not fallback_records: 
+                    if not fallback_records:
                         fallback_records = [[select_op_info]]
                     else:
                         fallback_records.append(fallback_records[-1] + [select_op_info])
                     logger.debug(f"*** The fallback ops record: \n{self._tuning_record_msg(fallback_records)}")
                     yield tune_cfg
 
             logger.info(f"*** The accuracy meeting the accuracy requirements, stop fallback ops.")
             while self.objectives.compare(self.last_tune_result, self.baseline):
                 if len(fallback_records) == 0 or len(fallback_records[-1]) <= 1:
                     logger.info(f"*** Stop re-quant due to no int8 op or only 1 int8 op left.")
                     break
                 logger.info(f"*** Start to re-quant the fallback op in the previous stage.")
                 # Track the current fallback ops
-                tmp_fallback_ops = fallback_records[-1] if fallback_records else [] 
+                tmp_fallback_ops = fallback_records[-1] if fallback_records else []
                 start = time()
-                ops_lst = self.adaptor.calculate_op_sensitivity(self.model, 
-                                                                self.calib_dataloader, 
+                ops_lst = self.adaptor.calculate_op_sensitivity(self.model,
+                                                                self.calib_dataloader,
                                                                 deepcopy(self._tune_cfg_converter(tune_cfg)),
-                                                                self.output_op_names, 
-                                                                self.confidence_batches,
+                                                                self.output_op_names,
+                                                                confidence_batches,
                                                                 fallback=False,
                                                                 requantize_cfgs=requantize_cfg['op'])
                 logger.debug(f"*** The op sensitivity analysis took {time() - start:.2f}s.")
-                if not ops_lst: 
+                if not ops_lst:
                     logger.warning("No op to be requantized")
                     break
                 for select_op_info in ops_lst:
                     #assert select_op_info in tmp_fallback_ops, f"{select_op_info} not in fallback list."
                     if select_op_info not in tmp_fallback_ops:
                         logger.debug(f"{select_op_info} not in fallback list.")
                         continue
-                    
+
                     new_fallback_ops = deepcopy(tmp_fallback_ops)
                     new_fallback_ops.remove(select_op_info)
                     if new_fallback_ops not in fallback_records:
                         logger.info(f"*** The op {select_op_info} have the lowest sensitivity in the current state, \
                                     re-quantize it.")
                         tune_cfg[select_op_info] = op_quant_cfgs[select_op_info]
                         fallback_records.append(new_fallback_ops)
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/random.py` & `neural_compressor-2.2/neural_compressor/experimental/strategy/random.py`

 * *Files 0% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 """The random tuning strategy."""
 import numpy as np
 from .strategy import strategy_registry, TuneStrategy
 from collections import OrderedDict
 
 from .utils.tuning_sampler import OpWiseTuningSampler, FallbackTuningSampler
 from .utils.tuning_structs import OpTuningConfig
-from ..utils import logger
+from ...utils import logger
 
 @strategy_registry
 class RandomTuneStrategy(TuneStrategy):
     """The random tuning strategy."""
 
     def next_tune_cfg(self):
         """Generate and yield the next tuning config by random searching in tuning space.
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/strategy.py` & `neural_compressor-2.2/neural_compressor/experimental/strategy/strategy.py`

 * *Files 14% similar despite different names*

```diff
@@ -27,56 +27,57 @@
 from collections import OrderedDict, defaultdict
 from pathlib import Path
 import yaml
 import numpy as np
 from typing import OrderedDict as T_OrderedDict
 
 from neural_compressor.adaptor.tensorflow import TensorFlowAdaptor
-from ..objective import MultiObjective
-from ..adaptor import FRAMEWORKS
-from ..utils.utility import Statistics, dump_data_to_local
-from ..utils.utility import fault_tolerant_file, equal_dicts, GLOBAL_STATE, MODE
-from ..utils.create_obj_from_config import create_eval_func, create_train_func
-from ..utils.utility import LazyImport
-from ..utils import logger
-from ..version import __version__
-from ..conf.dotdict import DotDict, deep_get, deep_set
-from ..algorithm import AlgorithmScheduler, ALGORITHMS
+from neural_compressor.config import options
+from ...objective import MultiObjective
+from ...adaptor import FRAMEWORKS
+from ...utils.utility import Statistics, dump_data_to_local
+from ...utils.utility import fault_tolerant_file, equal_dicts, GLOBAL_STATE, MODE
+from ...utils.create_obj_from_config import create_eval_func, create_train_func
+from ...utils.utility import LazyImport
+from ...utils import logger
+from ...version import __version__
+from ...conf.dotdict import DotDict, deep_get, deep_set
+from ...algorithm import AlgorithmScheduler, ALGORITHMS
 
 import copy
 import numpy as np
 from collections import OrderedDict
 from time import time
-from ..utils import logger
+from ...utils import logger
 import sys
 
 
 from .utils.tuning_space import TuningItem, TuningSpace
 from .utils.tuning_structs import OpTuningConfig
 from .utils.constant import FALLBACK_RECIPES_SET
 
 
-STRATEGIES = {}
+EXP_STRATEGIES = {}
 
 
 def strategy_registry(cls):
     """Class decorator used to register all TuneStrategy subclasses.
 
     Args:
         cls (class): The class of register.
 
     Returns:
         cls: The class of register.
     """
     assert cls.__name__.endswith(
         'TuneStrategy'
     ), "The name of subclass of TuneStrategy should end with \'TuneStrategy\' substring."
-    if cls.__name__[:-len('TuneStrategy')].lower() in STRATEGIES:
+    if cls.__name__[:-len('TuneStrategy')].lower() in EXP_STRATEGIES:
         raise ValueError('Cannot have two strategies with the same name')
-    STRATEGIES[cls.__name__[:-len('TuneStrategy')].lower()] = cls
+    EXP_STRATEGIES[cls.__name__[:-len('TuneStrategy')].lower()] = cls
     return cls
 
 @strategy_registry
 class TuneStrategy(object):
     """Basic class for tuning strategy."""
 
     def __init__(self, model, conf, q_dataloader=None, q_func=None, eval_dataloader=None,
@@ -148,15 +149,15 @@
         self.last_tune_cfg = None
         self.best_qmodel = None 
         self.best_tune_result = None
         self.best_tuning_cfg = None # track the best tuning config correspondence to the best quantized model
         self.cur_best_acc = self.initial_best_acc() # track the current best accuracy
         self.cur_best_tuning_cfg = {} # track tuning cfg with the current best accuracy
         self.re_quant = False
-        self.trials_count = 0
+
         self.capability = self.adaptor.query_fw_capability(model)
         logger.debug(self.capability)
         self.set_tuning_space(conf)
         
         #For algo scheduler
         self.algo_scheduler = AlgorithmScheduler(self.cfg.quantization.recipes)
         self.algo_scheduler.dataloader = self.calib_dataloader  # reuse the calibration iteration
@@ -195,16 +196,16 @@
             tune_config (dict): It's a dict containing the tuning configuration to traverse.
         """
         raise NotImplementedError
     
     def _initialize_recipe(self):
         """Divide the recipe into two categories tuning/not tuning."""
         from .utils.utility import get_adaptor_name
-        from ..utils.constant import RECIPES as fwk_recipes
-        from ..utils.constant import RECIPES_PRIORITY as fwk_recipes_priority
+        from ...utils.constant import RECIPES as fwk_recipes
+        from ...utils.constant import RECIPES_PRIORITY as fwk_recipes_priority
         # get all recipes supported by adaptor.
         adaptor_name = get_adaptor_name(self.adaptor)
         adaptor_recipes = fwk_recipes['common']
         # TODO WA due to smooth quant only supported by ort/pt currently.
         if not adaptor_name not in ['onnx', 'pytorch']:
             adaptor_recipes.pop('smooth_quant', None)
         for adaptor_name_key, adaptor_recipes_val in fwk_recipes.items():
@@ -227,272 +228,14 @@
                 if recipe_name == 'smooth_quant': continue
                 self._tuning_recipes[recipe_name] = adaptor_recipes[recipe_name]
                 self._tuning_recipes_default_values[recipe_name] = adaptor_recipes[recipe_name][0]
         logger.info(f"{len(self._not_tuning_recipes_values)} recipes specified by user.")
         logger.debug(self._not_tuning_recipes_values)
         logger.info(f"{len(self._tuning_recipes)} recipes require future tuning.")
         logger.debug(self._tuning_recipes)
-        
-
-    def distributed_next_tune_cfg_lst(self, comm):
-        """Interface for generate the distributed next tuning config list.
-
-        The generator of yielding next tuning config list to distributed traverse by concrete strategies or
-        quantization level according to tuning result and traverse logic.
-
-        It should be implemented by the sub-class. Currently, it is only implemented in the BasicTuneStrategy.
-        """
-        pass
-
-    def meet_acc_req(self, eval_res):
-        """Compare the result of last tuning with baseline to check whether the result meet requirements.
-
-        Args:
-            eval_res: The evaluation result of tuning.
-
-        Returns:
-            Return True if the accuracy meets requirements else False.
-        """
-        self.last_tune_result = eval_res
-        return self.objectives.accuracy_meet_req(deepcopy(self.last_tune_result))
-
-    def master_worker_handle(self, comm):
-        """Master worker handles the task assignment and result management.
-
-        Master node send all task ids to all free nodes, and wait until any result.
-        When receiving any result, directly send a new task id to the sender (it's free).
-
-        Args:
-            comm (MPI.COMM): The instance of communication for MPI.
-        """
-        MPI = LazyImport("mpi4py.MPI")
-        size = comm.Get_size()
-        for process_id in range(1, min(len(self.tune_cfg_lst) + 1, size)):
-            tune_cfg_id = process_id - 1
-            logger.info("[Rank {}]master sending tune cfg: {} to rank {}".format(comm.Get_rank(), \
-                tune_cfg_id, process_id))
-            comm.send(
-                obj=tune_cfg_id, # just send the tune cfg id is enough
-                dest=process_id, # rank 0 send to rank 1, 2, ...
-                tag=tune_cfg_id # tag, the index of tune cfg 0,1,2,3
-            )
-            import time as ttime
-            # WA for UT
-            ttime.sleep(0.5)
-
-        # master should be aware of the next config id to send
-        cur_cfg_id = min(len(self.tune_cfg_lst), size - 1)
-        # WA for UT
-        self.eval_results = {}
-        # record number of all response acks, break when it equals to len()
-        self.num_acks = 0
-        # used to obtain the source and the tag for each received message
-        status = MPI.Status()
-
-        self.already_ack_id_lst = set()
-        self.requirements_met_min_cfg_id = sys.maxsize
-
-        # stuck here to receive any result
-        while True:
-            eval_res = comm.recv(
-                source=MPI.ANY_SOURCE,
-                tag=MPI.ANY_TAG,
-                status=status   # get MPI status object
-            )
-            self.num_acks += 1
-            # sender rank
-            sender_rank = status.Get_source()
-            # the task id that is finished
-            tag = status.Get_tag()
-
-            logger.info("[Rank {}]master receiving eval result: {} from rank {}".format(comm.Get_rank(), \
-                eval_res, sender_rank))
-
-            # record eval_results for context coordination of stage 3
-            self.last_tune_result = eval_res
-            self.eval_results[tag] = eval_res
-            
-            self.overall_trials += 1
-            self.best_tune_cfg_id = None
-            self.already_ack_id_lst.add(tag)
-
-            # if meet accuracy requirement, then update minimum id that met requirement
-            if(self.meet_acc_req(eval_res)):
-                logger.info("[Rank {}]master has one tuning cfg meet acc: {}".format(comm.Get_rank(), tag))
-                self.met_flag = True
-                self.requirements_met_min_cfg_id = min(self.requirements_met_min_cfg_id, tag)
-                
-                # must ensure every id lower than current min_id has been acknowledged
-                # because a tune cfg (not acked yet) with lower id can have better acc
-                for i in range(self.requirements_met_min_cfg_id):
-                    if i not in self.already_ack_id_lst:
-                        logger.info("[Rank {}]master has one tuning cfg meet acc: {} but not collect all acks before"\
-                                    .format(comm.Get_rank(), tag))
-                        # not completely collected yet!
-                        self.met_flag = False
-                        break
-                
-                if self.met_flag:
-                    # found the best tune cfg!
-                    logger.info("[Rank {}]master has one tuning cfg meet acc: {} and also collect all acks before"\
-                                .format(comm.Get_rank(), tag))
-                    self.best_tune_cfg_id = self.requirements_met_min_cfg_id
-            else:
-                # get the current best acc but not meet requirements
-                logger.info("[Rank {}]master gets the current best acc: {} but not meet requirements"\
-                    .format(comm.Get_rank(), tag))
-                self.cur_best_acc, self.cur_best_tuning_cfg = self.update_best_op_tuning_cfg(self.tune_cfg_lst[tag])
-
-            if self.best_tune_cfg_id is not None:
-                # we find the best tune cfg id that meet requirements!
-                logger.info("[Rank {}]master finds best tune cfg id.".format(comm.Get_rank()))
-                logger.info(self.best_tune_cfg_id)
-                logger.info(self.tune_cfg_lst[self.best_tune_cfg_id])
-                break
-            
-            # send the next cfg if not exceed max trials
-            if self.overall_trials > self.cfg.tuning.exit_policy.max_trials:
-                self.max_trial_flag = True
-            # elif time.time() - self.overall_time_start > self.cfg.tuning.exit_policy.timeout:
-            #     self.max_time_flag = True
-            elif cur_cfg_id < len(self.tune_cfg_lst):
-                logger.info("[Rank {}]master sends new tuning cfg {} to rank: {}".format(comm.Get_rank(), \
-                    cur_cfg_id, sender_rank))
-                comm.send(obj=cur_cfg_id, dest=sender_rank, tag=cur_cfg_id)
-                cur_cfg_id += 1
-            else:                    
-                logger.info("[Rank {}]All tune configs are sent, no more sending, just collecting..."\
-                    .format(comm.Get_rank()))
-
-            # all collected (ack should collected == acks)
-            if len(self.tune_cfg_lst) == self.num_acks:
-                # all processes ended
-                # return self.requirements_met_min_cfg_id  if it has been updated
-                if self.requirements_met_min_cfg_id == sys.maxsize:
-                    logger.info("[Rank {}]Not found any tune cfg that meet requirements".format(comm.Get_rank()))
-                    self.cur_best_tuning_cfg = self.tune_cfg_lst[0] # TODO select cur_best_tuning_cfg
-                else:
-                    logger.info("[Rank {}]Find best tune cfg id".format(comm.Get_rank()))
-                    logger.info(self.requirements_met_min_cfg_id)
-                    self.met_flag = True
-                    self.best_tune_cfg_id = self.requirements_met_min_cfg_id
-                    logger.info(self.tune_cfg_lst[self.best_tune_cfg_id])
-                break
-
-        # send END signal to all other slaves
-        logger.info("[Rank {}]master sends END signal to all other slaves".format(comm.Get_rank()))
-        for process_id in range(1, size):
-            logger.info("[Rank {}]master sends END signal to rank: {}".format(comm.Get_rank(), process_id))
-            comm.send(
-                obj="MET" if self.met_flag else "NOT MET", # send whether met criterion in the current stage
-                dest=process_id, # rank 0 send to rank 1, 2, ...
-                tag=len(self.tune_cfg_lst)
-            )
-
-        if self.best_tune_cfg_id is not None:
-            self.best_qmodel = self.adaptor.quantize(
-                    copy.deepcopy(self.tune_cfg_lst[self.best_tune_cfg_id]), self.model, self.calib_dataloader, \
-                        self.q_func)
-
-
-    def slave_worker_handle(self, comm):
-        """Slave worker handles the task processing.
-
-        When receiving any task id, slave node finds it in self.tune_cfg_lst and run it.
-        Then slave node sends back the tune result to master node.
-
-        Args:
-            comm (MPI.COMM): The instance of communication for MPI.
-        """
-        MPI = LazyImport("mpi4py.MPI")
-        status = MPI.Status()
-        while True:
-            task = comm.recv(
-                    source=MPI.ANY_SOURCE,
-                    tag=MPI.ANY_TAG,
-                    status=status   # sender (master)
-                )
-            cfg_idx = status.Get_tag()
-            if status.Get_tag() >= len(self.tune_cfg_lst):
-                logger.info("[Rank {}]slave {} receiving END signal in the current stage".format(comm.Get_rank(),\
-                    comm.Get_rank()))
-                if task == "MET":
-                    logger.info("[Rank {}]met criterion in this stage!".format(comm.Get_rank()))
-                    self.met_flag = True
-                break
-            tune_cfg = self.tune_cfg_lst[cfg_idx]
-
-            # set the parameter for pre quantization algos and run
-            self.set_param_for_pre_quantization_algos(self.algo_scheduler, tune_cfg, self.model)
-            self.model = self.algo_scheduler('pre_quantization')
-            # quantize
-            q_model = self.adaptor.quantize(copy.deepcopy(tune_cfg), self.model, self.calib_dataloader, self.q_func)
-            assert self.adaptor.pre_optimized_model
-            # set the parameter for post quantization algos and run
-            self.set_param_for_post_quantization_algos(self.algo_scheduler, tune_cfg, self.adaptor.pre_optimized_model,
-                                                       q_model)
-            self.last_qmodel = self.algo_scheduler('post_quantization')
-            self.last_tune_cfg = copy.deepcopy(tune_cfg)
-            # Remove the reference to model
-            self.algo_scheduler.reset_exec_algorithms()
-            assert self.last_qmodel
-            self.last_tune_result = self._evaluate(self.last_qmodel)
-
-            # send back the tuning statistics
-            logger.debug("[Rank {}]Slave sends back the tuning statistics".format(comm.Get_rank()))
-            logger.debug(self.last_tune_result)
-            comm.send(
-                obj=self.last_tune_result,
-                dest=0, # rank 0 send to rank 1, 2, ...
-                tag=cfg_idx
-            )
-
-    def distributed_traverse(self):
-        """Distributed traverse the tuning space.
-
-        The main traverse logic which could be override by some concrete strategy which needs more hooks.
-        """
-        MPI = LazyImport("mpi4py.MPI")
-        comm = MPI.COMM_WORLD
-        rank = comm.Get_rank()
-
-        self.met_flag = False
-        # whether exceed max trials
-        self.max_trial_flag = False
-        # whether exceed max time
-        self.max_time_flag = False
-        self.overall_trials = 0
-        self.overall_time_start = time()
-
-        # for all the stages, handle the tune cfg lst
-        # the tune cfg lst is generated/yielded each time by distributed_next_self.tune_cfg_lst
-        # we must pass the comm to the specific strategy because slaves may not know
-        # contexts such as the best_tune_cfg
-        # master should make sure slaves have all the contexts needed before going to the next computation stage
-        for op_tuning_cfg_lst in self.distributed_next_tune_cfg_lst(comm):
-            self.tune_cfg_lst = [self._tune_cfg_converter(op_tuning_cfg) for op_tuning_cfg in op_tuning_cfg_lst]
-            if self.tune_cfg_lst == []:
-                # skip empty list at some stages
-                continue
-            if rank == 0:
-                self.master_worker_handle(comm)
-            else:
-                self.slave_worker_handle(comm)
-            logger.debug("# if self.met_flag or self.max_trial_flag or self.max_time_flag:" \
-                .format(self.met_flag or self.max_trial_flag or self.max_time_flag))
-            if self.met_flag or self.max_trial_flag or self.max_time_flag:
-                break
-
-    def _open_all_recipes(self):
-        """Open all tunable recipes."""
-        opened_recipes = {}
-        for recipe_name, recipe_val_lst in self._tuning_recipes.items():
-            opened_recipes[recipe_name] = recipe_val_lst[-1]
-        logger.info("Opened all recipes.")
-        logger.info(opened_recipes)
     
     def _fallback_ops(self, tune_cfg, recipe_op_lst, tuning_space):
         """Fallback ops in recipe op list."""
         for op_name_type in recipe_op_lst:
             tune_cfg.update({op_name_type: OpTuningConfig(op_name_type[0], \
                 op_name_type[1],'fp32', tuning_space)})
         return tune_cfg
@@ -511,14 +254,16 @@
         
     def apply_recipe_one_by_one(self, tune_cfg):
         """Apply the tunable recipes one by one.
         
         For recipes only have two options, apply the last one.
         For recipes with multiple values. such as alpha of smooth quant, apply it one by one.
         """
+        from .utils.tuning_sampler import TuningSamplerRegistry
+        all_registered_samplers = TuningSamplerRegistry.sampler_dict
         for recipe_name, recipe_vals in self._tuning_recipes.items():
             if recipe_name in FALLBACK_RECIPES_SET and 'recipes_ops' in self.capability and \
                 len(self.capability['recipes_ops'].get(recipe_name, [])) > 0:
                 logger.info(f"Applied recipe {recipe_name} with value {recipe_vals[-1]}")
                 new_tune_cfg = self._fallback_ops(copy.deepcopy(tune_cfg), \
                     self.capability['recipes_ops'][recipe_name], self.tuning_space)
                 yield new_tune_cfg
@@ -541,28 +286,28 @@
         """
         algo_scheduler.origin_model = fp32_model
         algo_scheduler.calib_iter = tune_cfg['calib_iteration']
         algo_scheduler.q_model = fp32_model
 
         recipe_cfgs = tune_cfg.get('recipe_cfgs', None)
         algo_scheduler.reset_exec_algorithms()
-        if recipe_cfgs and recipe_cfgs.get('smooth_quant', False):
+        if recipe_cfgs and recipe_cfgs.get('smooth_quant', False):  # pragma: no cover
             # skip assign alpha to sq first.
             # set the alpha to 0.5 by default
             smooth_quant_args = recipe_cfgs.get('smooth_quant_args', {'alpha': 0.5})
             sq_algo = ALGORITHMS()['smooth_quant']
             sq_algo.alpha = smooth_quant_args['alpha']
             if 'folding' not in smooth_quant_args:
                 smooth_quant_args['folding'] = True if self.framework in ['pytorch', 'pytorch_fx'] \
                   else False
                 logger.info("SmoothQuant args 'folding' is not set, it's {} now.".format(smooth_quant_args['folding']))
                 if self.framework == 'pytorch_ipex':
                     smooth_quant_args['folding'] = None # will reset it to True if IPEX version < 2.1.
             sq_algo.folding = smooth_quant_args['folding']
-            #logger.debug(f"Set smooth quant with alpha {smooth_quant_args['alpha']} as the pre-quantization algo.")
+            logger.debug(f"Set smooth quant with alpha {smooth_quant_args['alpha']} as the pre-quantization algo.")
             algo_scheduler.append_algorithm('pre_quantization', sq_algo)
             
             
     def set_param_for_post_quantization_algos(self, algo_scheduler, tune_cfg, pre_optimized_model, q_model) -> None:
         """Set the parameter for post-quantization algos, such as bias correction, weight correction.
 
         Args:
@@ -592,24 +337,22 @@
 
     def traverse(self):
         """Traverse the tuning space.
 
         The main traverse logic which could be override by some concrete strategy which needs more hooks.
         """
         self._eval_baseline()
-        if self.cfg.tuning.use_distributed_tuning:
-            logger.info("use distributed traverse: {}".format(self.cfg.tuning.use_distributed_tuning))
-            return self.distributed_traverse()
+        trials_count = 0
         traverse_start_time = time()
         for op_tuning_cfg in self.next_tune_cfg():
             tuning_start_time = time()
             tune_cfg = self._tune_cfg_converter(op_tuning_cfg)
-            self.trials_count += 1
+            trials_count += 1
             tuning_history = self._find_tuning_history(tune_cfg)
-            if tuning_history and self.trials_count < self.cfg.tuning.exit_policy.max_trials:
+            if tuning_history and trials_count < self.cfg.tuning.exit_policy.max_trials:
                 self.last_tune_result = tuning_history['last_tune_result']
                 self.best_tune_result = tuning_history['best_tune_result']
                 logger.warn("Find evaluated tuning config, skip.")
                 continue
             self._remove_redundant_qmodel()
             logger.debug("Dump current tuning configuration:")
             logger.debug(tune_cfg)
@@ -631,43 +374,43 @@
             # Return the last quantized model as a result. if performance only.
             if self.cfg.tuning.exit_policy.performance_only:
                 self.best_qmodel = self.last_qmodel
                 self._add_tuning_history(copy.deepcopy(tune_cfg), (-1, [0]), q_config=self.last_qmodel.q_config)
                 return
             self.last_tune_result = self._evaluate(self.last_qmodel)
             self.cur_best_acc, self.cur_best_tuning_cfg = self.update_best_op_tuning_cfg(op_tuning_cfg)
-            need_stop = self.stop(self.cfg.tuning.exit_policy.timeout, self.trials_count)
+            need_stop = self.stop(self.cfg.tuning.exit_policy.timeout, trials_count)
 
             # record the tuning history
             saved_tune_cfg = copy.deepcopy(tune_cfg)
             saved_last_tune_result = copy.deepcopy(self.last_tune_result)
             self._add_tuning_history(saved_tune_cfg,
                                     saved_last_tune_result,
                                     q_config=q_model.q_config)
             self.tune_result_record.append(copy.deepcopy(self.last_tune_result))
             self.tune_cfg = tune_cfg
             now_time = time()
             acc_res_msg = ""
-            performance_res_msg = ""
+            performace_res_msg = ""
             if self.tuning_result_data:
                 acc_res_msg = "[ " + "| ".join(self.tuning_result_data[0]) + " ]"
-                performance_res_msg = "[ " + "| ".join(self.tuning_result_data[1]) + " ]"
+                performace_res_msg = "[ " + "| ".join(self.tuning_result_data[1]) + " ]"
             logger.debug(f"*** The accuracy of last tuning is: {acc_res_msg}")
-            logger.debug(f"*** The performance of last tuning is: {performance_res_msg}")
+            logger.debug(f"*** The perfomance of last tuning is: {performace_res_msg}")
             logger.debug(f"*** The last tuning time: {(now_time - tuning_start_time):.2f} s")
             logger.debug(f"*** The tuning process lasted time: {(now_time - traverse_start_time):.2f} s")
 
             self._dump_tuning_process_statistics()
             if need_stop:
                 if self.re_quant:
                     logger.info("*** Do not stop the tuning process, re-quantize the ops.")
                     continue
                 # recover the best quantized model from tuning config
                 self._recover_best_qmodel_from_tuning_cfg()
-                if self.cfg.tuning.diagnosis and self.cfg.tuning.diagnosis.diagnosis_after_tuning:
+                if self.cfg.tuning.diagnosis:
                     logger.debug(f'*** Start to do diagnosis (inspect tensor).')
                     self._diagnosis()
                 if self.use_multi_objective and len(self.tune_result_record) > 1 and \
                     self.best_tune_result is not None:
                     best_trail, best_result = self.objectives.best_result(self.tune_result_record,
                                                                           copy.deepcopy(self.baseline))
                     if best_result != self.best_tune_result:
@@ -711,15 +454,15 @@
             logger.info("Force setting 'tuning.exit_policy.performance_only = True'.")
             
         if not self.cfg.tuning.exit_policy.performance_only:
             # get fp32 model baseline
             if self.baseline is None:
                 logger.info("Get FP32 model baseline.")
                 self._fp32_model = self.model
-                self.baseline = self._evaluate(self.model)
+                self.baseline = self._evaluate(self.model)       
                 self.objectives.baseline = self.baseline
                 # record the FP32 baseline
                 self._add_tuning_history()
             self.show_baseline_info()
 
     def _recover_best_qmodel_from_tuning_cfg(self):
         """Recover the best quantized model from tuning config."""
@@ -1062,15 +805,14 @@
                 self.cfg.model.framework = 'pytorch_ipex'
                 framework = 'pytorch_ipex'
             elif self.cfg.model.backend == 'default':
                 self.cfg.model.framework = 'pytorch_fx'
                 framework = 'pytorch_fx'
             if self.mixed_precision_mode:
                 framework_specific_info.update({"approach": "post_training_dynamic_quant"})
-            framework_specific_info.update({'recipes': self.cfg.quantization.get('recipes', {})})
             framework_specific_info.update({"q_dataloader": q_dataloader})
             framework_specific_info.update({"use_bf16": self.cfg.use_bf16 \
                             if self.cfg.use_bf16 is not None else True})
             framework_specific_info.update(
                 {"workspace_path": os.path.dirname(self.deploy_path)})
             if self.cfg['quantization']['op_wise'] is not None \
                and 'default_qconfig' in self.cfg['quantization']['op_wise']:
@@ -1185,41 +927,14 @@
             yaml.add_representer(OrderedDict, represent_dict_order)
             yaml.add_representer(DotDict, represent_dict_order)
         setup_yaml()
         with open(self.deploy_path, 'w+') as f:
             yaml.dump(self.deploy_cfg, f)
             logger.info("Save deploy yaml to {}".format(self.deploy_path))
 
-    def _get_common_cfg(self, model_wise_cfg, op_wise_cfgs):
-        """Get the common parts from the model_wise_cfg.
-        
-            This function is focused on composing the configuration that consists of
-            model-wise field and op-wise unique field data.
-
-        Args:
-            model_wise_cfg ([DotDict]): The model-wise configuration.
-            op_wise_cfgs ([List]): The list of each op's config in DotDict type.
-
-        Returns:
-            [DotDict]: The combined configration with the op-wise unique field.
-        """
-        model_wise_keys = model_wise_cfg.keys()
-
-        result = op_wise_cfgs[0]
-        for each_op_wise_cfg in op_wise_cfgs:
-            tmp_cfg = {}
-            for k in model_wise_keys:
-                tmp_cfg[k] = each_op_wise_cfg[k]
-
-            if model_wise_cfg == tmp_cfg:
-                result = each_op_wise_cfg
-                break
-
-        return result
-
     @property
     def evaluation_result(self):
         """Evaluate the given model.
 
         Returns:
             The objective value evaluated.
         """
@@ -1410,15 +1125,15 @@
             best_tune_msg = 'n/a'
             for name in self.tune_data.keys() - {'baseline'}:
                 if len(self.tune_data[name]) == 2:
                     self.tune_data[name].append('n/a')
                 else:
                     self.tune_data[name][2] = 'n/a'
 
-        logger.info("Tune {} result is: {}, Best tune result is: {}".format(self.trials_count,
+        logger.info("Tune {} result is: {}, Best tune result is: {}".format(trials_count,
                                                                             last_tune_msg,
                                                                             best_tune_msg))
         output_data = [[info_type,
             '{:.4f} '.format(self.tune_data[info_type][0]) if \
             not isinstance(self.tune_data[info_type][0], str) else self.tune_data[info_type][0],
             '{:.4f} '.format(self.tune_data[info_type][1]) if \
             not isinstance(self.tune_data[info_type][1], str) else self.tune_data[info_type][1],
@@ -1430,23 +1145,23 @@
             '{:.4f} '.format(self.baseline[1][i]) if self.baseline else 'n/a',
             '{:.4f} '.format(self.last_tune_result[1][i]) if self.last_tune_result else 'n/a',
             '{:.4f} '.format(self.best_tune_result[1][i]) if self.best_tune_result else 'n/a'] \
             for i, obj in enumerate(self.objectives.representation)])
         self.tuning_result_data = output_data
         Statistics(output_data,
                    header='Tune Result Statistics',
-                   field_names=['Info Type', 'Baseline', 'Tune {} result'.format(self.trials_count), \
+                   field_names=['Info Type', 'Baseline', 'Tune {} result'.format(trials_count), \
                                                                 'Best tune result']).print_stat()
 
 
         if self.cfg.tuning.exit_policy.performance_only:
             need_stop = True
         elif timeout == 0 and self.best_tune_result:
             need_stop = True
-        elif self.trials_count >= self.cfg.tuning.exit_policy.max_trials:
+        elif trials_count >= self.cfg.tuning.exit_policy.max_trials:
             need_stop = True
         else:
             need_stop = False
 
         return need_stop
 
     def _save(self):
@@ -1541,23 +1256,23 @@
             if isinstance(op_config, OpTuningConfig) and quant_mode in op_config.op_quant_mode:
                 ops_lst.append(op_info)
         return ops_lst
 
     def _diagnosis(self):
         import logging
         logger = logging.getLogger("neural_compressor")
-        iteration_list = self.cfg.tuning.diagnosis.iteration_list
-        inspect_type = self.cfg.tuning.diagnosis.inspect_type
-        save_to_disk = self.cfg.tuning.diagnosis.save_to_disk
-        save_path = self.cfg.tuning.diagnosis.save_path
+        iteration_list = [1]
+        inspect_type = 'all'
+        save_to_disk = True
+        save_path = os.path.join(options.workspace, 'inspect_saved')
         inspect_node_lst, updated_cfg = self.adaptor.diagnosis_helper(self._fp32_model,
                                                                       self.last_qmodel,
                                                                       self.tune_cfg,
                                                                       save_path = save_path)
-        op_list = self.cfg.tuning.diagnosis.op_list
+        op_list = []
         if not op_list:
             op_list = list(inspect_node_lst)
         else:
             op_list = list(set(op_list).intersection(inspect_node_lst))
 
         logger.debug(f'*** Start to inspect tensor :{op_list} in  fp32 model.')
         self.adaptor.inspect_tensor(self._fp32_model,
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/utils/__init__.py` & `neural_compressor-2.2/neural_compressor/experimental/strategy/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/utils/constant.py` & `neural_compressor-2.2/neural_compressor/experimental/strategy/utils/constant.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,24 +13,23 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Strategy constant."""
 
-PRECISION_LIST = ['bf16', 'fp16' , 'fp32']
+PRECISION_SET = {'bf16', 'fp16' , 'fp32',}
 QUANT_MODE_SET = {'static', 'dynamic'}
 QUNAT_BIT_SET = {'int8', 'uint8', 'int4', 'uint4'}
 
 TUNING_ITEMS_LST = [('activation','scheme'), ('activation','algorithm'), ('activation','granularity'),
                     ('weight','scheme'), ('weight','algorithm'), ('weight','granularity'), 'sampling_size']
 
 PRECISION_SET_V2_0 = {'fp32', 'bf16'}
 
 auto_query_order = ['static', 'dynamic', 'bf16', 'fp16', 'fp32']
 static_query_order = ['static', 'bf16', 'fp16', 'fp32']
 dynamic_query_order = ['dynamic', 'bf16', 'fp16', 'fp32']
-auto_query_order_o0 = ['bf16', 'fp16', 'fp32', 'static', 'dynamic']
 
 
-FALLBACK_RECIPES_SET = {'first_conv_or_matmul_quantization', 'last_conv_or_matmul_quantization', \
+FALLBACK_RECIPES_SET = {'first_conv_or_matmul_quantization', 'last_conv_or_matmul_quantization' \
     'pre_post_process_quantization'}
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/utils/tuning_sampler.py` & `neural_compressor-2.2/neural_compressor/experimental/strategy/utils/tuning_sampler.py`

 * *Files 8% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 
 from itertools import product
 import copy
 from collections import deque, OrderedDict, defaultdict
 from typing import List, Dict, Any
 from .tuning_space import TuningSpace, pattern_to_internal, pattern_to_path, quant_mode_from_pattern
 from .tuning_structs import OpTuningConfig
-from ...utils import logger
+from ....utils import logger
 
 TUNING_ITEM_PRIORITY = [('activation','scheme'), ('activation','algorithm'),('activation','granularity'), 
                         ('activation','compute_dtype'), ('weight','scheme'), ('weight','algorithm'), \
                         ('weight','granularity')]
 
 
 
@@ -425,7 +425,39 @@
 
             new_tune_cfg.update({op_name_type: new_op_config})
             if self.accumulate and skip_first:  # skip the first one
                 skip_first = False
                 continue
             logger.debug(f"fallback {op_name_type} to {target_dtype}")
             yield new_tune_cfg  # need to skip the first one
+
+@TuningSamplerRegistry.register("smooth_quant")
+class SmoothQuantSampler(TuningSampler):
+    """Sampler for the hyperparameter tuning of smooth quantization."""
+    
+    def __init__(self,
+                 tuning_space: TuningSpace,
+                 tuning_order_lst: List[TuningOrder],
+                 initial_op_tuning_cfg: Dict,
+                 kwargs: Dict ={}):
+        """Initialize the sampler."""
+        super().__init__(tuning_space, tuning_order_lst, initial_op_tuning_cfg, kwargs)
+        # TODO use the alpha list specified by user
+        self._kwargs = kwargs
+        self._alpha_lst = [0.5]
+        if kwargs.get('smooth_quant_agrs', {}):
+            self._alpha_lst = kwargs['smooth_quant_agrs'].get('alpha_lst', [0.5])
+
+    def __iter__(self, tune_cfg=None) -> OpTuningConfig:
+        """Yield the next tuning config with update alpha.
+
+        Args:
+            tune_cfg: tuning config. Defaults to None.
+        """
+        for alpha in self._alpha_lst:
+            new_tune_cfg = copy.deepcopy(self.initial_op_tuning_cfg) if not tune_cfg else copy.deepcopy(tune_cfg)
+            sq_args = {'smooth_quant': True, 'smooth_quant_args': {'alpha': alpha}}
+            if 'recipe_cfgs' not in new_tune_cfg:
+                new_tune_cfg['recipe_cfgs'] = sq_args
+            else:
+                new_tune_cfg['recipe_cfgs'].update(sq_args)
+            yield new_tune_cfg
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/utils/tuning_space.py` & `neural_compressor-2.2/neural_compressor/strategy/utils/tuning_space.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,27 +14,27 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tuning space."""
 
 from collections import defaultdict, OrderedDict
-import os
 import re
-from typing import Dict, Tuple
+from typing import Dict, Tuple, List
 from copy import deepcopy
+import itertools
 from ...utils import logger
 from .utility import OrderedDefaultDict
 from .tuning_structs import OpTuningConfig
 
 from .constant import TUNING_ITEMS_LST
 
 class TuningItem:
     """Not displayed in API Docs."""
-    
+
     def __init__(self, name, options=[], item_type=None):
         """Init the tuning item.
 
         Args:
             name: tuning item name.
             options: The options. Defaults to [].
             item_type: The item type. Defaults to None.
@@ -47,15 +47,15 @@
     def options(self):
         """Return all options.
 
         Returns:
             All options.
         """
         return self._options
-    
+
     def get_options_name(self):
         """Return the name list of the options."""
         return [o.name for o in self.options]
 
     def append(self, option):
         """Append option.
 
@@ -85,15 +85,15 @@
         for option in self.options:
             if isinstance(option, TuningItem) and option.name == option_name:
                 return option
         return None
 
     def get_details(self, depth=0):
         """Get the tuning item and its options recursively.
-        
+
         Args:
             depth: recursion depth. Defaults to 0.
 
         Returns:
             The tuning item and its options as a string.
         """
         details = ['\t' * depth + f"{self.name},  {self.item_type}"]
@@ -103,19 +103,19 @@
             else:
                 details.append(option.get_details(depth + 1))
         return "\n".join(details)
 
 
 class TuningSpace:
     """Not displayed in API Docs.
-    
+
     1) capability -> internal format -> merge -> tuning space (tree)
 
     """
-    
+
     def __init__(self, capability, conf, framework=None):
         """Init the tuning space.
 
         Args:
             capability: framework capability.
             conf: user configuration
             framework: framework name. Defaults to None.
@@ -123,24 +123,31 @@
         self.capability = capability
         self.conf = conf
         self.root_item = TuningItem(name='root', options=[], item_type='root')
         self.quant_mode_wise_items = defaultdict(list)  # quant_mode/precision_name: {(op_name, op_type),...}
         self.op_type_wise_items = defaultdict(list)  # op_type: {(op_name, op_type), ...}
         self.framework = framework
         self.ops_dtype = defaultdict(OrderedDict)
-        usr_cfg = conf.usr_cfg if conf else None
+        self._usr_cfg = self._init_usr_cfg()
         self.op_items = {}
         # {(op_name, op_type): {(path): data type}}
         self.ops_data_type = OrderedDefaultDict()
         self.ops_attr = {'activation': set(), 'weight': set()}
         # {(op_name, op_type): {path1, path2, ...}
         self.ops_path_set = defaultdict(set)
-        
-        self._create_tuning_space(capability, usr_cfg)
-        
+        self._create_tuning_space(capability, self._usr_cfg)
+
+    def _init_usr_cfg(self):
+        """Init user config."""
+        usr_cfg = {'quantization': {}}
+        usr_cfg['quantization']['model_wise'] =  None
+        usr_cfg['quantization']['optype_wise'] = self.conf.op_type_dict if self.conf else None
+        usr_cfg['quantization']['op_wise'] = self.conf.op_name_dict if self.conf else None
+        return usr_cfg
+
     def _parse_capability(self, capability: Dict) -> None:
         """Parse the capability and construct the tuning space(a tree).
 
         Args:
             capability: merged framework capability.
         """
         calib = TuningItem(name='calib_sampling_size',
@@ -176,54 +183,47 @@
                     acc_item = q_option.get_option_by_name('activation')
                     if acc_item and acc_item.options:
                         for dtype_item in acc_item.options:
                             self.quant_mode_wise_items[dtype_item.name].append(op_item)
                 else:
                     self.quant_mode_wise_items[q_option.name].append(op_item)
 
-    def _create_tuning_item(self, tuning_items: Dict, attr_name: str, quant_mode_item: TuningItem):
-        for tuning_item_name, options in tuning_items.items():
-            if tuning_item_name not in ['dtype', 'quant_mode']:
-                name = (attr_name, tuning_item_name)
-                tuning_item = TuningItem(name=name, options=options, item_type=name)
-                quant_mode_item.append(tuning_item)
-
     def _merge_op_cfg(self, cur_op_cap, op_user_cfg, fw_op_cap):
         """Merge the op cfg with user cfg.
-        
+
         op_user_cfg:{
             'activation':{
                 'dtype': ['fp32']
                 },
             'weight':{
                 'dtype': ['fp32']
                 }
             }
-            
+
         Step1. merge dtype, get the intersection between fw_op_cap and op_user_cfg.
         Step2. merge method options.
-        
+
         # if dtype and type intersection with precision set -> only keep the intersection precision
         # and remove the quantization.
         # else(no dtype, or no intersection) -> merge the method
 
         Args:
             cur_op_cap: current capability.
-            op_user_cfg: The user capability. 
+            op_user_cfg: The user capability.
             fw_op_cap: The fwk capability(baseline).
-            
+
         Returns:
             Return the merged capability.
         """
         from .utility import extract_data_type, reverted_data_type
         fw_op_cap = deepcopy(fw_op_cap)
         new_op_cap = deepcopy(cur_op_cap)
         for att in ['activation', 'weight']:
             if op_user_cfg.get(att, None) is not None:
-                user_dtype_lst = op_user_cfg[att]['dtype'] if op_user_cfg[att]['dtype'] is not None else []
+                user_dtype_lst = op_user_cfg[att]['dtype'] if op_user_cfg[att].get('dtype', None) is not None else []
                 # Merge the precision part.
                 fwk_att_precision_cap = fw_op_cap['precision'].get(att, {})
                 fwk_precision_set = set(fwk_att_precision_cap.keys())
                 # The intersection of user cfg and fwk capability.
                 valid_precision_set = set(fwk_precision_set).intersection(set(user_dtype_lst))
                 if len(valid_precision_set) != 0:
                     new_op_cap = dict(filter(lambda item: item[0] == 'precision', new_op_cap.items()))
@@ -251,43 +251,48 @@
         return new_op_cap
 
     def _merge_optype_wise_cfg(self, cap: Dict, optype_wise_usr_cfg: Dict, fw_cap: Dict):
         for op_type, op_user_cfg in optype_wise_usr_cfg.items():
             op_type_pattern = re.compile(op_type)
             op_lst = [op_name_type for op_name_type in cap['op'] if op_type_pattern.fullmatch(op_name_type[1])]
             for op_name_type in op_lst:
-                cap['op'][op_name_type] = self._merge_op_cfg(cap['op'][op_name_type], 
+                cap['op'][op_name_type] = self._merge_op_cfg(cap['op'][op_name_type],
                                                              op_user_cfg,
                                                              fw_cap['op'][op_name_type])
 
     def _merge_model_wise_cfg(self, cap: Dict, model_wise_usr_cfg: Dict, fw_cap: Dict):
         for op_name_type in cap['op'].keys():
-            cap['op'][op_name_type] = self._merge_op_cfg(cap['op'][op_name_type], 
+            cap['op'][op_name_type] = self._merge_op_cfg(cap['op'][op_name_type],
                                                          model_wise_usr_cfg,
                                                          fw_cap['op'][op_name_type])
 
     def _merge_op_wise_cfg(self, cap: Dict, op_wise_usr_cfg: Dict, fw_cap: Dict):
         op_name_types = {key[0]: key for key in cap['op'].keys()}
         for op_name_pattern, op_user_cfg in op_wise_usr_cfg.items():
-            op_name_pattern = re.compile(op_name_pattern)
+            if isinstance(op_name_pattern, str):
+                op_name_pattern = re.compile(op_name_pattern)
+                str_flag=True
+            else:
+                str_flag=False
             for op_name in op_name_types:
-                if op_name_pattern.fullmatch(op_name):
+                if str_flag and op_name_pattern.fullmatch(str(op_name)) \
+                  or op_name_pattern == op_name:
                     op_name_type = op_name_types[op_name]
-                    cap['op'][op_name_type] = self._merge_op_cfg(cap['op'][op_name_type], 
+                    cap['op'][op_name_type] = self._merge_op_cfg(cap['op'][op_name_type],
                                                                  op_user_cfg,
                                                                  fw_cap['op'][op_name_type])
-             
+
     def _merge_with_user_cfg(self, capability: Dict, user_cfg: Dict):
         """Merge the capability with user config.
-        
+
         Merge the capability queried from the adaptor with user config in the order of
         model-wise, optype-wise, and op-wise if needed.
         The optype-wise user config will override the model-wise user config for their
         intersection parts, the same as the op-wise and optype-wise.
-        
+
         Here is an example:
         capability:{
             ('op1','type1'): {
                 'item1': [item1_option1, item1_option2, item1_option3],
                 'item2': [item2_option1, item2_option2, item2_option3],
                 }
             ('op2','type1'): {
@@ -299,15 +304,15 @@
                 'item2': [item2_option1, item2_option2],
                 }
             ('op4','type2'): {
                 'item1': [item1_option1, item1_option2],
                 'item2': [item2_option1, item2_option2],
                 }
                 }
-        
+
         user_config{
             model-wise:{
                 'item1': [item1_option1]
             }
             optype-wise: {
                 'type1': {
                     'item1': [item1_option1, item1_option2]
@@ -384,18 +389,18 @@
         fw_capability = deepcopy(capability)
         if user_cfg['model_wise'] is not None:
             self._merge_model_wise_cfg(capability, user_cfg['model_wise'], fw_capability)
         if user_cfg['optype_wise'] is not None:
             self._merge_optype_wise_cfg(capability, user_cfg['optype_wise'], fw_capability)
         if user_cfg['op_wise'] is not None:
             self._merge_op_wise_cfg(capability, user_cfg['op_wise'], fw_capability)
-            
+
     def _parse_cap_helper(self, cap):
         """Convert the cpa to internal format.
-        
+
         Parsed result:
         (op_name, op_type):
             {
                 'static':{
                     'act':{
                         'int8':{
                             'signed':{ # (op_name, op_type): ('static', (('int8', 'signed'),(...)))
@@ -466,26 +471,26 @@
                                 # The dtype should be a string, need to align with fwk.yaml.
                                 self.ops_data_type[op_name_type][(quant_mode, att, _data_type, signed_flag)] = \
                                     item_options[0] if isinstance(item_options, list) else item_options
                             if item_name not in ['dtype', 'quant_mode']:
                                 parsed_op_cap[quant_mode][att][_data_type][signed_flag][item_name] = item_options
                     else:
                         # Parse the data info for itemwith unique value.
-                        att_dtype = op_cap[att]['dtype'] 
+                        att_dtype = op_cap[att]['dtype']
                         if isinstance(att_dtype, list):
                             att_dtype = att_dtype[0]
                         parsed_op_cap['precision'][att][att_dtype] = {'dtype': att_dtype}
                         self.ops_data_type[op_name_type][('precision', att, att_dtype)] = att_dtype
 
             parsed_cap[op_name_type] = parsed_op_cap
         return parsed_cap
-    
+
     def _create_tuning_space(self, capability, usr_cfg):
         """Create tuning space.
-        
+
         steo1. convert the capability into internal format.
         step2. merge the capability with usr_cfg
         step3. create the tuning space
         :param capability:
         :param usr_cfg:
         :return:
         """
@@ -543,25 +548,25 @@
         # set the first option as the default for each tuning item
         op_tuning_config = OpTuningConfig(op_name_type[0],
                                           op_name_type[1],
                                           quant_mode,
                                           self,
                                           kwargs=config_args)
         return op_tuning_config
-    
+
     def get_item_by_path(self, path, default=None):
         """Get the item according to the path."""
         item = self.root_item
         for val in path:
             if item is None:
-                logger.warning(f"Did not found the item according to the path {path}")
+                logger.debug(f"Did not found the item according to the path {path}")
                 return default
             item = item.get_option_by_name(val)
         if item is None:
-            logger.warning(f"Did not found the item according to the path {path}")
+            logger.debug(f"Did not found the item according to the path {path}")
         return item
 
     def get_default_full_path(self, op_name_type, path):
         """Complete the path.
 
         Args:
             op_name_type: (op_name, op_path)
@@ -572,15 +577,15 @@
         """
         # For precision
         if path[0] == 'precision':
             # If the path is ('precision', 'activation', dtype), return it directly.
             if len(path) == 3: return path
             assert len(path) == 2, f"Got the path: {path}, please provide the path include activation or weight."
             att_item = self.get_item_by_path((op_name_type, *path))
-            if not att_item or len(att_item.options) == 0: 
+            if not att_item or len(att_item.options) == 0:
                 logger.debug(f"Could not found item for {op_name_type} with path {path}")
                 return None
             dtype = att_item.options[0].name
             return (*path, dtype)
         else:
             # For quantization
             assert len(path) >= 2, f"Got the path: {path}, please provide the path include activation or weight."
@@ -599,15 +604,15 @@
             return new_path
 
     def query_quant_mode_item_by_full_path(self, op_name_type, path) -> Tuple[TuningItem, Tuple]:
         """Query the mode item by full path."""
         new_path = (op_name_type, *path)
         item = self.get_item_by_path(new_path)
         return item
-    
+
     def query_items_by_quant_mode(self, quant_mode):
         """Collect all op items that support the specified mode.
 
         Args:
             quant_mode: dynamic/static/bf16/fp32/fp16
 
         Returns:
@@ -617,51 +622,73 @@
 
     def get_op_default_path_by_pattern(self, op_name_type, pattern):
         """Get the default path by quant mode.
 
         Args:
             op_name_type: (op_name, op_type)
             pattern: 'static', 'dynamic', ('static', 'int8'), ('precision', 'fp32')
-            
+
         Returns:
-            result(Dict): The default full path of activation and weight if have. 
+            result(Dict): The default full path of activation and weight if have.
         """
         internal_pattern = pattern_to_internal(pattern)
         full_path = {'activation': None, 'weight': None}
         full_path['activation'], full_path['weight'] = pattern_to_path(internal_pattern)
         result = {}
         has_weight = op_name_type in self.ops_attr['weight']
         att_lst = ['activation', 'weight'] if has_weight else ['activation']
         for att in att_lst:
             result[att] = self.get_default_full_path(op_name_type, full_path[att])
-        return result        
-        
-def get_op_mode_by_query_order(tuning_space: TuningSpace, query_order):
-    """Get the op mode according to the query order."""
-    quant_mode_wise_items = OrderedDict() # mode, op_item_lst
-    pre_items = set()
-    # Collect op items supported the specified mode.
-    for quant_mode in query_order:
-        items = tuning_space.query_items_by_quant_mode(quant_mode)
-        filtered_items = list(filter(lambda item: item not in pre_items, items))
-        pre_items = pre_items.union(set(items))
-        quant_mode_wise_items[quant_mode] = filtered_items
-
-    def initial_op_quant_mode(items_lst, target_quant_mode, op_item_dtype_dict):
-        for item in items_lst:
-            op_item_dtype_dict[item.name] = target_quant_mode
-    op_item_dtype_dict = OrderedDict()
-    for quant_mode, quant_mode_items in quant_mode_wise_items.items():
-        initial_op_quant_mode(quant_mode_items, quant_mode, op_item_dtype_dict)
-    
-    return op_item_dtype_dict
+        return result
+
+    def get_op_default_path_by_quant_bits(self, op_name_type, quant_bits):
+        """Get the full path according to the target bits.
+
+        Args:
+            op_name_type: (op name, op type)
+            quant_bits: quantization bits, like int4, int8
+
+        Returns:
+            A dict includes the full path.
+        """
+        quant_modes = ['static', 'dynamic']
+        attribute_options = ['activation', 'weight']
+        quant_bits = [quant_bits]
+        support_attributes = {'activation': ('precision', 'activation', 'fp32'),\
+            'weight': ('precision', 'weight', 'fp32')}
+        for path in itertools.product(quant_modes, attribute_options, quant_bits):
+            if self.query_quant_mode_item_by_full_path(op_name_type, path):
+                support_attributes[path[1]] = path
+        full_path = {}
+        for att in support_attributes:
+            full_path[att] = self.get_default_full_path(op_name_type, support_attributes[att])
+        return full_path
+
+    def collect_op_by_quant_bits(self, quant_bits: str) -> List[TuningItem]:
+        """Collect all OP items that either activation or weight supporting the target bits.
+
+        Args:
+            quant_bits: the target quantization bits, like int4, int8.
+        """
+        quant_modes = ['static', 'dynamic']
+        attribute_options = ['activation', 'weight']
+        quant_bits = [quant_bits]
+
+        quant_op_items = set(self.query_items_by_quant_mode('static')).union(self.query_items_by_quant_mode('dynamic'))
+        op_items = []
+        for op in quant_op_items:
+            for path in itertools.product(quant_modes, attribute_options, quant_bits):
+                if self.query_quant_mode_item_by_full_path(op.name, path):
+                    op_items.append(op)
+                    break
+        return op_items
 
 def pattern_to_internal(pattern, default_dtype='int8'):
     """Convert pattern to internal format.
-    
+
     'static' -> ('static', (('int8'),('int8')))
     'dynamic' -> ('dynamic', (('int8'),('int8')))
     'fp32' -> ('precision', (('fp32'), ('fp32')))
     'bf16' -> ('precision', (('bf16'), ('bf16')))
     ('static', 'int8') -> ('static', (('int8'),('int8')))
     ('dynamic', 'int8') -> ('dynamic', (('int8'),('int8')))
     ('precision', 'fp32') -> ('precision', (('fp32'), ('fp32')))) # (('fp32'), ('fp32')) or ('fp32', 'fp32')
@@ -690,23 +717,23 @@
 def initial_tuning_cfg_with_quant_mode(op_name_type, quant_mode, tuning_space: TuningSpace) -> OpTuningConfig:
     """Initialize the tuning cfg.
 
     Args:
         op_name_type: (op name, op type)
         quant_mode: dynamic/static/fp32/bf16/fp16
         tuning_space: tuning space.
-        
-    step1, convert the quant_mode into internal format.    
+
+    step1, convert the quant_mode into internal format.
     step2, complete the path based.
     step3, get the mode item.
     step4, use the first option as value for method.
     step5, create the op tuning config.
-    
+
     Returns:
-        The initial tuning config. 
+        The initial tuning config.
     """
     internal_pattern = pattern_to_internal(quant_mode)
     full_path = {'activation': None, 'weight': None}
     full_path['activation'], full_path['weight'] = pattern_to_path(internal_pattern)
     has_weight = op_name_type in tuning_space.ops_attr['weight']
 
     config_args = {}
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/utils/tuning_structs.py` & `neural_compressor-2.2/neural_compressor/strategy/utils/tuning_structs.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,20 +14,19 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tuning structure."""
 
 from typing import Dict
-from .constant import QUANT_MODE_SET, TUNING_ITEMS_LST, PRECISION_LIST
-from ...utils import logger
+from .constant import TUNING_ITEMS_LST, PRECISION_LIST
 
 class OpTuningConfig:
     """Op tuning config."""
-    
+
     def __init__(self, op_name, op_type, op_quant_mode, tuning_space, kwargs={}):
         """Create the tuning config.
 
         Args:
             op_name: op name.
             op_type: op type.
             op_quant_mode: quantization mode.
@@ -39,32 +38,32 @@
         self.op_name_type = (self.op_name, self.op_type)
         self.op_quant_mode = op_quant_mode  # static/dynamic/fp32/bf16/fp16
         self.kwargs = kwargs
         self.act_dtype = None
         self.weight_dtype = None
         self.has_weight = self.op_name_type in tuning_space.ops_attr['weight']
         self._set_dtype()
-        
+
     def _set_dtype(self):
         """Set the date type."""
         if self.op_quant_mode in PRECISION_LIST:
             self.act_dtype, self.weight_dtype = self.op_quant_mode, self.op_quant_mode
         else:
             self.act_dtype = self.kwargs.get('activation_dtype', None)
             self.weight_dtype = self.kwargs.get('weight_dtype', None)
         assert self.act_dtype and isinstance(self.act_dtype, str),\
             (f"Didn't assign the activation data type for {self.op_name, self.op_type}", \
                 f"with quant_mode {self.op_quant_mode}")
         # if self.has_weight:
         #     assert self.weight_dtype, \
         #         (f"Didn't assign the weight data type for {self.op_name, self.op_type}", \
         #             f"with quant_mode {self.op_quant_mode}")
-        
 
-    def __str__(self) -> str:
+
+    def __repr__(self) -> str:
         """Display the tuning config as string.
 
         Returns:
             msg: the tuning config as string.
         """
         msg =  f"op name: {self.op_name}, op type : {self.op_type} \n"
         msg += f"\t activation dtype: {self.act_dtype} \n"
@@ -72,15 +71,15 @@
         for key, val in self.kwargs.items():
             if key in TUNING_ITEMS_LST:
                 msg += f"\t {key[0]} {key[1]}: {val}\n"
         return msg
 
     def get_state(self):
         """Return the op tuning configuration.
-        
+
         Returns:
             Dict: The op tuning state.
         """
         result = {}
         if self.has_weight:
             result['weight'] = {
                 'dtype': self.weight_dtype,
```

### Comparing `neural_compressor-2.1.1/neural_compressor/strategy/utils/utility.py` & `neural_compressor-2.2/neural_compressor/experimental/strategy/utils/utility.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/training.py` & `neural_compressor-2.2/neural_compressor/training.py`

 * *Files 19% similar despite different names*

```diff
@@ -11,215 +11,201 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """The configuration of the training loop."""
-import copy
+import os
+import pickle
+import numpy as np
+import random
+
+from .adaptor import FRAMEWORKS
 from .compression.callbacks import QuantizationAwareTrainingCallbacks, DistillationCallbacks, PruningCallbacks
+from .config import _Config, options
+from .metric import register_customer_metric
 from .model.model import Model
 from .utils import logger
+from .utils.utility import time_limit
+from neural_compressor.strategy.strategy import STRATEGIES
 from neural_compressor import (DistillationConfig, QuantizationAwareTrainingConfig,
                                WeightPruningConfig)
 from typing import Callable, List, Union
-from .compression import prepare_pruning
 
 
 class CompressionManager:
     """CompressionManager is uesd in train loop for what user want to deal with additional.
 
     Arguments:
-        model: A model to be compressed. It should be neural compressor model.
-        callbacks: A list of Callbacks instances.
-                   Such as: DistillationCallbbacks, QuantizationAwareTrainingCallbacks, PruningCallbacks.
+        model: A model to be compressed.
+        confs: The instance of QuantizationAwareTrainingConfig, PruningConfig and distillationConfig, or a list of
+               config for orchestration optimization.
 
     Examples::
 
         import neural_compressor.training.prepare_compression
-        compression_manager = prepare_compression(nc_model, confs)
+        compression_manager = prepare_compression(model, confs)
         compression_manager.callbacks.on_train_begin()
         model = compression_manager.model
-        train_loop:
-            for epoch in range(epochs):
-                compression_manager.callbacks.on_epoch_begin(epoch)
-                for i, batch in enumerate(dataloader):
-                    compression_manager.callbacks.on_step_begin(i)
-                    ......
-                    output = model(batch)
-                    loss = ......
-                    loss = compression_manager.callbacks.on_after_compute_loss(batch, output, loss)
-                    loss.backward()
-                    compression_manager.callbacks.on_before_optimizer_step()
-                    optimizer.step()
-                    compression_manager.callbacks.on_step_end()
-                compression_manager.callbacks.on_epoch_end()
+        # train_loop:
+        for epoch in range(epochs):
+            compression_manager.callbacks.on_epoch_begin(epoch)
+            for i, (batch, label) in enumerate(dataloader):
+                compression_manager.callbacks.on_step_begin(i)
+                ......
+                output = model(batch)
+                loss = ......
+                loss = compression_manager.callbacks.on_after_compute_loss(batch, output, loss)
+                loss.backward()
+                compression_manager.callbacks.on_before_optimizer_step()
+                optimizer.step()
+                compression_manager.callbacks.on_step_end()
+            compression_manager.callbacks.on_epoch_end()
         compression_manager.callbacks.on_train_end()
         compression_manager.save("path_to_save")
     """
-    def __init__(self, model, callbacks_list):
+    def __init__(self, model: Callable, confs: Union[Callable, List], **kwargs):
         """Initialize the CompressionManager's parameters.
 
-        model: A model to be compressed. It should be neural compressor model.
-        callbacks: A list of Callbacks instances.
-                   Such as: DistillationCallbbacks, QuantizationAwareTrainingCallbacks, PruningCallbacks.
+        model: A model to be compressed.
+        confs: The instance of QuantizationAwareTrainingConfig, PruningConfig and distillationConfig, or a list of
+               config for orchestration optimization.
         """
-        self.callbacks = CallBacks(callbacks_list)
-        self.model = model
-        self._train_func = None
-        self._eval_func = None
-        self.quantizer = None
+        callbacks_list = []
+        self.model = None
+        q_conf = None
+        p_conf = None
+        d_conf = None
+        self.adaptor = None
+
+        if isinstance(confs, List) and len(confs) > 1:
+            for conf in confs:
+                if isinstance(conf, QuantizationAwareTrainingConfig) or isinstance(conf, WeightPruningConfig):
+                    self.model = Model(model, conf=conf)
+            if self.model is None:
+                self.model = Model(model)
+
+            for conf in confs:
+                if isinstance(conf, QuantizationAwareTrainingConfig):
+                    q_conf = conf
+
+                    framework_specific_info = {
+                        'device': conf.device,
+                        'random_seed': options.random_seed,
+                        'workspace_path': options.workspace,
+                        'q_dataloader': None,
+                        'backend': getattr(confs, "backend", 'default'),
+                        'format': getattr(confs, "quant_format", 'default'),
+                        'approach': conf.approach,
+                    }
+                    if 'tensorflow' in conf.framework:
+                        framework_specific_info.update(
+                            {"inputs": conf.inputs,
+                             "outputs": conf.outputs})
+                    
+                    # TODO: will be removed once 'op_type_dict' and 'op_name_dicts' 
+                    # for quant_aware_training can be handled in strategy
+                    framework_specific_info['qat_optype_wise'] = conf.op_type_dict
+                    framework_specific_info['qat_op_wise'] = conf.op_name_dict
+
+                    self.adaptor = FRAMEWORKS[conf.framework](framework_specific_info)
+                    self.adaptor.model = self.model
+                    callbacks_list.append(QuantizationAwareTrainingCallbacks(conf, adaptor=self.adaptor))
+                elif isinstance(conf, WeightPruningConfig):
+                    p_conf = conf
+                    callbacks_list.append(PruningCallbacks(conf, model=self.model))
+                elif isinstance(conf, DistillationConfig):
+                    d_conf = conf
+                    callbacks_list.append(DistillationCallbacks(conf, model=self.model))
+                else:
+                    assert False, "Unsupported configure: {}".format(type(conf))
+            self.conf = _Config(quantization=q_conf, benchmark=None, pruning=p_conf, distillation=d_conf, nas=None)
+        else:
+            if isinstance(confs, List):
+                confs = confs[0]
+            if isinstance(confs, QuantizationAwareTrainingConfig):
+                self.model = Model(model, conf=confs)
+
+                framework_specific_info = {
+                    'device': confs.device,
+                    'random_seed': options.random_seed,
+                    'workspace_path': options.workspace,
+                    'q_dataloader': None,
+                    'backend': getattr(confs, "backend", 'default'),
+                    'format': getattr(confs, "quant_format", 'default'),
+                    'approach': confs.approach,
+                }
+                if 'tensorflow' in confs.framework:
+                    framework_specific_info.update(
+                        {"inputs": confs.inputs,
+                         "outputs": confs.outputs})
+                
+                # TODO: will be removed once 'op_type_dict' and 'op_name_dicts' 
+                # for quant_aware_training can be handled in strategy
+                framework_specific_info['qat_optype_wise'] = confs.op_type_dict
+                framework_specific_info['qat_op_wise'] = confs.op_name_dict
+
+                self.adaptor = FRAMEWORKS[confs.framework](framework_specific_info)
+                self.adaptor.model = self.model
+                callbacks_list.append(QuantizationAwareTrainingCallbacks(confs, adaptor=self.adaptor))
+                self.conf = _Config(quantization=confs, benchmark=None, pruning=None, distillation=None, nas=None)
+            elif isinstance(confs, WeightPruningConfig):
+                self.model = Model(model, conf=confs)
+                callbacks_list.append(PruningCallbacks(confs, model=self.model))
+                self.conf = _Config(quantization=None, benchmark=None, pruning=confs, distillation=None, nas=None)
+            elif isinstance(confs, DistillationConfig):
+                self.model = Model(model)
+                callbacks_list.append(DistillationCallbacks(confs, model=self.model))
+                self.conf = _Config(quantization=None, benchmark=None, pruning=None, distillation=confs, nas=None)
+            else:
+                assert False, logger.error(
+                    "confs should be one of QuantizationAwareTrainingConfig, "
+                    "PruningConfig, DistillationConfig. not {}".format(type(confs))
+                )
 
         try:
             # TODO: export to ONNX model need original fp32 model now, will remove it
             #  when int8 model can be exported to ONNX model.
-            self.fp32_model = model
+            self.fp32_model = self.model
         except Exception as e:  # pragma: no cover
             logger.warning("Fail to deep copy the model due to {}.".format(repr(e)))
             self.fp32_model = None
 
-        for component in callbacks_list:
-            if isinstance(component, QuantizationAwareTrainingCallbacks):
-                self.quantizer = component
-
-    @property
-    def train_func(self):
-        """Not support get train_func."""
-        assert False, 'Should not try to get the value of `train_func` attribute.'
-
-    @train_func.setter
-    def train_func(self, user_train_func):
-        """Set training function.
-
-        Args:
-            user_train_func: This function takes "model" as input parameter
-                         and executes entire training process with self
-                         contained training hyper-parameters. If training_func set,
-                         an evaluation process must be triggered and user should
-                         set eval_dataloader with metric configured or directly eval_func
-                         to make evaluation of the model executed. training_func will return
-                         a trained model.
-        """
-        self.quantizer.train_func = user_train_func
-
-    @property
-    def eval_func(self):
-        """Not support get eval_func."""
-        assert False, 'Should not try to get the value of `eval_func` attribute.'
-        return None
-
-    @eval_func.setter
-    def eval_func(self, user_eval_func):
-        """Eval function for component.
-
-        Args:
-            user_eval_func: This function takes "model" as input parameter
-                         and executes entire evaluation process with self
-                         contained metrics. If eval_func set,
-                         an evaluation process must be triggered
-                         to make evaluation of the model executed.
-        """
-        assert self.quantizer is not None, "There is no quantizer to tune, " \
-                                           "please pass a QuantizationAwareTrainingConfig."
-        self.quantizer.eval_func = user_eval_func
-
-    @property
-    def eval_dataloader(self):
-        """Getter to eval dataloader."""
-        return self.quantizer.eval_dataloader
-
-    @eval_dataloader.setter
-    def eval_dataloader(self, dataloader):
-        """Set Data loader for evaluation of component.
-
-        It is iterable and the batched data should consists of yield (input, _).
-        the input in the batched data will be used for model inference, so it
-        should satisfy the input format of specific model.
-
-        Args:
-            dataloader(generator): user are supported to set a user defined dataloader
-                                   which meet the requirements that can yield tuple of
-                                   (input, label)/(input, _) batched data.
-        """
-        assert self.quantizer is not None, "There is no quantizer to tune, " \
-                                           "please pass a QuantizationAwareTrainingConfig."
-        assert hasattr(dataloader, '__iter__') and \
-            hasattr(dataloader, 'batch_size'), \
-            'dataloader must implement __iter__ method and batch_size attribute'
-
-        self.quantizer.eval_dataloader = dataloader
-
-    @property
-    def metric(self):
-        """Get `metric` attribute."""
-        assert False, 'Should not try to get the value of `metric` attribute.'
-
-    @metric.setter
-    def metric(self, user_metric):
-        """Set metric class or a dict of built-in metric configures.
-
-        1. neural_compressor have many built-in metrics,
-           user can pass a metric configure dict to tell neural compressor what metric will be use.
-           You can set multi-metrics to evaluate the performance of a specific model.
-                Single metric:
-                    {topk: 1}
-
-                Multi-metrics:
-                    {topk: 1,
-                     MSE: {compare_label: False},
-                    }
-        For the built-in metrics, please refer to below link:
-        https://github.com/intel/neural-compressor/blob/master/docs/source/metric.md#supported-built-in-metric-matrix.
-
-        2. User also can set specific metric through this api. The metric class should take the outputs of the model or
-           postprocess(if have) as inputs, neural_compressor built-in metric always take(predictions, labels)
-           as inputs for update, and user_metric.metric_cls should be sub_class of neural_compressor.metric.BaseMetric.
-
-        Args:
-            user_metric(neural_compressor.metric.Metric or a dict of built-in metric configurations):
-                The object of Metric or a dict of built-in metric configurations.
-        """
-        assert self.quantizer is not None, "There is no quantizer to tune, " \
-                                           "please pass a QuantizationAwareTrainingConfig."
-        self.quantizer.metric = user_metric
-
-    def fit(self):
-        """Compress model with tuning for quantization."""
-        self.model = self.quantizer.fit()
-        return self.model
+        self.callbacks = CallBacks(callbacks_list)
 
     def save(self, root=None):
         """Save compressed model.
 
         Args:
             root (str): path to save the model
         """
-        self.model.save(root)
+        self.model.save(root)  # pylint: disable=no-member
 
     def export(
         self,
         save_path: str,
         conf,
     ):
         """Convert the model to another type model, like `onnx` model and so on.
 
         Args:
             save_path (str): The path to save the model
             conf (Union[Callable, List]) : The configure for onnx exportation.
         """
-        self.model.export(save_path, conf)
+        self.model.export(save_path, conf)  # pylint: disable=no-member
 
 
 def fit(compression_manager,
         train_func,
         eval_func=None,
         eval_dataloader=None,
         eval_metric=None,
         **kwargs):
-    """Compress the model with tuning for quantization.
+    """Compress the model with accuracy tuning for quantization.
 
     Args:
         compression_manager (CompressionManager):  The Compression manager contains the model and
                                               callbacks.
         train_func (function, optional):      Training function for quantization aware training. It is optional.
                                               This function takes "model" as input parameter
                                               and executes entire inference process. If this
@@ -246,95 +232,172 @@
                                               not None, user needs to specify pre-defined
                                               evaluation metrics object and should set "eval_func" paramter as None.
                                               Tuner will combine model, eval_dataloader
                                               and pre-defined metrics to run evaluation
                                               process.
         eval_metric (dict or obj):            Set metric class or a dict of built-in metric configures,
                                               and neural_compressor will initialize this class when evaluation.
+
+    Returns:
+        A optimized model.
+
+    Examples::
+
+        from neural_compressor.training import fit, prepare_compression
+
+        compression_manager = prepare_compression(conf, model)
+
+        def train_func(model):
+            compression_manager.callbacks.on_train_begin()
+            for epoch in range(epochs):
+                compression_manager.callbacks.on_epoch_begin(epoch)
+                for i, (batch, label) in enumerate(dataloader):
+                    compression_manager.callbacks.on_step_begin(i)
+                    ......
+                    output = model(batch)
+                    loss = ......
+                    loss = compression_manager.callbacks.on_after_compute_loss(batch, output, loss)
+                    loss.backward()
+                    compression_manager.callbacks.on_before_optimizer_step()
+                    optimizer.step()
+                    compression_manager.callbacks.on_step_end()
+                compression_manager.callbacks.on_epoch_end()
+            compression_manager.callbacks.on_train_end()
+            return model
+
+        def eval_func(model):
+            for i, (batch, label) in enumerate(dataloader):
+                output = model(batch)
+                # compute metric
+                metric = top1(output, label)
+            return metric.results()
+
+        model = fit(compression_manager, train_func=train_func, eval_func=eval_func)
     """
-    assert compression_manager.quantizer is not None, "Only quantization supports tuning with accuracy driven."
-    compression_manager.train_func = train_func
-    if eval_func is not None:
-        compression_manager.eval_func = eval_func
-    if eval_dataloader is not None:
-        compression_manager.eval_dataloader = eval_dataloader
+    assert compression_manager.conf.quantization is not None, "Only quantization supports tuning with accuracy driven."
+    seed = options.random_seed
+    random.seed(seed)
+    np.random.seed(seed)
+
+    # Remove qat hooks if user want to tune accuracy with train function.
+    for callback in compression_manager.callbacks.callbacks_list:
+        if isinstance(callback, QuantizationAwareTrainingCallbacks):
+            callback.remove_hook("on_train_begin", compression_manager.adaptor._pre_hook_for_qat)
+            callback.remove_hook("on_train_end", compression_manager.adaptor._post_hook_for_qat)
+
     if eval_metric is not None:
-        compression_manager.eval_metric = eval_metric
-    return compression_manager.fit()
+        metric = register_customer_metric(eval_metric, compression_manager.conf.quantization.framework)
+    else:
+        metric = None
+
+    strategy_name = compression_manager.conf.quantization.tuning_criterion.strategy
+
+    if compression_manager.conf.quantization.quant_level == "auto":
+        strategy_name = "auto"
+    elif compression_manager.conf.quantization.quant_level == 0:
+        strategy_name = "conservative"
+
+    if strategy_name == "mse_v2":
+        if not (compression_manager.conf.quantization.framework.startswith("tensorflow")
+                or compression_manager.conf.quantization.framework == 'pytorch_fx'):  # pragma: no cover
+            strategy_name = "basic"
+            logger.warning(f"MSE_v2 does not support {compression_manager.conf.quantization.framework} now,"
+                           "use basic instead.")
+            logger.warning("Only tensorflow, pytorch_fx is supported by MSE_v2 currently.")
+    assert strategy_name in STRATEGIES, "Tuning strategy {} is NOT supported".format(strategy_name)
+
+    logger.info(f"Start {strategy_name} tuning.")
+    _resume = None
+    # check if interrupted tuning procedure exists. if yes, it will resume the
+    # whole auto tune process.
+    resume_file = os.path.abspath(os.path.expanduser(options.resume_from)) \
+        if options.workspace and options.resume_from else None
+    if resume_file:
+        assert os.path.exists(resume_file), \
+            "The specified resume file {} doesn't exist!".format(resume_file)
+        with open(resume_file, 'rb') as f:
+            _resume = pickle.load(f).__dict__
+
+    if eval_func is None and eval_dataloader is None:  # pragma: no cover
+        logger.info("Quantize model without tuning!")
+
+    strategy = STRATEGIES[strategy_name](
+        model=compression_manager.model,
+        conf=compression_manager.conf,
+        q_dataloader=None,
+        q_func=train_func,
+        eval_func=eval_func,
+        eval_dataloader=eval_dataloader,
+        eval_metric=metric,
+        resume=_resume,
+        q_hooks=None
+    )
+    try:
+        with time_limit(compression_manager.conf.quantization.tuning_criterion.timeout):
+            logger.debug("Dump user yaml configuration:")
+            logger.debug(compression_manager.conf)
+            strategy.traverse()
+    except KeyboardInterrupt:
+        pass
+    except Exception as e:
+        logger.error("Unexpected exception {} happened during tuning.".format(repr(e)))
+        import traceback
+        traceback.print_exc()
+    finally:
+        if strategy.best_qmodel:
+            logger.info(
+                "Specified timeout or max trials is reached! "
+                "Found a quantized model which meet accuracy goal. Exit.")
+            strategy.deploy_config()
+        else:
+            logger.error(
+                "Specified timeout or max trials is reached! "
+                "Not found any quantized model which meet accuracy goal. Exit.")
+
+        compression_manager.model = strategy.best_qmodel
+
+    return compression_manager.model
 
 
 def prepare_compression(model: Callable, confs: Union[Callable, List], **kwargs):
     """Summary.
 
     Args:
         model (Callable, optional):    The model to optimize.
         confs (Union[Callable, List]): The instance of QuantizationAwareTrainingConfig,
                                        PruningConfig and distillationConfig, or a list of
                                        config for orchestration optimization.
-        options (Options, optional):   The configure for random_seed, workspace,
-                                       resume path and tensorboard flag.
 
     Returns:
         An object of CompressionManager.
 
     Examples::
 
-        import neural_compressor.training.prepare_compression
+        from neural_compressor.training import prepare_compression
 
         compression_manager = prepare_compression(conf, model)
-        train_loop:
-            compression_manager.on_train_begin()
-            for epoch in range(epochs):
-                compression_manager.on_epoch_begin(epoch)
-                for i, batch in enumerate(dataloader):
-                    compression_manager.on_step_begin(i)
-                    ......
-                    output = model(batch)
-                    loss = ......
-                    loss = compression_manager.on_after_compute_loss(batch, output, loss)
-                    loss.backward()
-                    compression_manager.on_before_optimizer_step()
-                    optimizer.step()
-                    compression_manager.on_step_end()
-                compression_manager.on_epoch_end()
-            compression_manager.on_train_end()
+        model = compression_manager.model
+        # train_loop:
+        compression_manager.callbacks.on_train_begin()
+        for epoch in range(epochs):
+            compression_manager.callbacks.on_epoch_begin(epoch)
+            for i, (batch, label) in enumerate(dataloader):
+                compression_manager.callbacks.on_step_begin(i)
+                ......
+                output = model(batch)
+                loss = ......
+                loss = compression_manager.callbacks.on_after_compute_loss(batch, output, loss)
+                loss.backward()
+                compression_manager.callbacks.on_before_optimizer_step()
+                optimizer.step()
+                compression_manager.callbacks.on_step_end()
+            compression_manager.callbacks.on_epoch_end()
+        compression_manager.callbacks.on_train_end()
     """
-    callbacks_list = []
-    nc_model = None
-    if isinstance(confs, List) and len(confs) > 1:
-        for conf in confs:
-            if isinstance(conf, QuantizationAwareTrainingConfig):
-                nc_model = Model(model, backend=conf.backend, approach="quant_aware_training")
-                callbacks_list.append(QuantizationAwareTrainingCallbacks(conf, model=nc_model))
-            elif isinstance(conf, WeightPruningConfig):
-                callbacks_list.append(PruningCallbacks(conf, model=model))
-            elif isinstance(conf, DistillationConfig):
-                callbacks_list.append(DistillationCallbacks(conf, model=model))
-            else:
-                assert False, "Unsupported configure: {}".format(type(conf))
-    else:
-        if isinstance(confs, List):
-            confs = confs[0]
-        if isinstance(confs, QuantizationAwareTrainingConfig):
-            nc_model = Model(model, backend=confs.backend, approach="quant_aware_training")
-            callbacks_list.append(QuantizationAwareTrainingCallbacks(confs, model=nc_model))
-        elif isinstance(confs, WeightPruningConfig):
-            callbacks_list.append(PruningCallbacks(confs, model=model))
-        elif isinstance(confs, DistillationConfig):
-            callbacks_list.append(DistillationCallbacks(confs, model=model))
-        else:
-            assert False, logger.error(
-                "confs should be one of QuantizationAwareTrainingConfig, "
-                "PruningConfig, DistillationConfig. not {}".format(type(confs))
-            )
-
-    if nc_model is None:
-        nc_model = Model(model, backend="default")
-
-    compression_manager = CompressionManager(nc_model, callbacks_list=callbacks_list)
+    compression_manager = CompressionManager(model, confs, **kwargs)
 
     return compression_manager
 
 
 class CallBacks:
     """Define the basic command for the training loop."""
     def __init__(self, callbacks_list):
```

### Comparing `neural_compressor-2.1.1/neural_compressor/utils/__init__.py` & `neural_compressor-2.2/neural_compressor/utils/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -16,11 +16,11 @@
 # limitations under the License.
 
 """Utils: provide useful methods and auxiliary functionalities."""
 
 from .collect_layer_histogram import LayerHistogramCollector
 from .logger import log, info, debug, warn, warning, error, fatal
 from .options import OPTIONS
-from .utility import set_random_seed
+from .utility import alias_param
 
 __all__ = ["LayerHistogramCollector", "log", "info", "debug", "warn", "warning", "error", "fatal",
-           "OPTIONS", "set_random_seed"]
+           "OPTIONS", "alias_param"]
```

### Comparing `neural_compressor-2.1.1/neural_compressor/utils/collect_layer_histogram.py` & `neural_compressor-2.2/neural_compressor/utils/collect_layer_histogram.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/utils/constant.py` & `neural_compressor-2.2/neural_compressor/utils/constant.py`

 * *Files 8% similar despite different names*

```diff
@@ -64,14 +64,15 @@
 # Options for recipes, the first options is the default value.
 RECIPES = {
     "common":{
         # 'fast_bias_correction' : [False, True], # Disable it first
         # 'weight_correction' : [False, True], # Disable it first
         },
     "tensorflow": {
+        'smooth_quant': [False, True],
         'first_conv_or_matmul_quantization' : [True, False],
         'last_conv_or_matmul_quantization' : [True, False],
         },
     "onnx": {
         'smooth_quant': [False, True],
         'first_conv_or_matmul_quantization' : [True, False],
         'last_conv_or_matmul_quantization' : [True, False],
```

### Comparing `neural_compressor-2.1.1/neural_compressor/utils/create_obj_from_config.py` & `neural_compressor-2.2/neural_compressor/utils/create_obj_from_config.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/utils/kl_divergence.py` & `neural_compressor-2.2/neural_compressor/utils/kl_divergence.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/utils/load_huggingface.py` & `neural_compressor-2.2/neural_compressor/utils/load_huggingface.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/utils/logger.py` & `neural_compressor-2.2/neural_compressor/utils/logger.py`

 * *Files identical despite different names*

### Comparing `neural_compressor-2.1.1/neural_compressor/utils/options.py` & `neural_compressor-2.2/neural_compressor/utils/options.py`

 * *Files 10% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 """ONNX options."""
 
 from ..conf.dotdict import DotDict
 
 class onnxrt:
     """ONNX helper configuration."""
     graph_optimization = DotDict({'level': None, 'gemm2matmul': True})
-    qdq_setting = DotDict({'OpTypesToExcludeOutputQuantizatioin': None, 
+    qdq_setting = DotDict({'OpTypesToExcludeOutputQuantizatioin': [], 
                            'AddQDQPairToWeight': False,
                            'DedicatedQDQPair': False})
 
 OPTIONS = {'tensorflow': None,
            'tensorflow_itex': None,
            'pytorch': None,
            'pytorch_fx': None,
```

### Comparing `neural_compressor-2.1.1/neural_compressor/utils/pytorch.py` & `neural_compressor-2.2/neural_compressor/utils/pytorch.py`

 * *Files 2% similar despite different names*

```diff
@@ -256,14 +256,24 @@
     model.eval()
     approach_quant_mode = None
     if tune_cfg['approach'] == "post_training_dynamic_quant":
         approach_quant_mode = 'dynamic'
     elif tune_cfg['approach'] == "post_training_static_quant":
         approach_quant_mode = 'static'
 
+    recipe_cfgs = tune_cfg.get('recipe_cfgs', None)
+    if recipe_cfgs and recipe_cfgs.get('smooth_quant', False) \
+      and not recipe_cfgs['smooth_quant_args']['folding'] \
+      and approach_quant_mode != 'dynamic':
+        from ..adaptor.torch_utils.model_wrapper import _wrapper_sq_linear, _wrapper_qdq_linear
+        model = _wrapper_sq_linear(model, recipe_cfgs['smoothquant_op_info']['sq_linear'])
+        model = _wrapper_qdq_linear(model, recipe_cfgs['smoothquant_op_info']['qdq_linear'])
+        model.load_state_dict(stat_dict)
+        return model
+
     for _, op_cfg in tune_cfg['op'].items():
         if 'quant_mode' not in op_cfg['activation']:
             op_cfg['activation']['quant_mode'] = approach_quant_mode
 
     if tune_cfg['approach'] != "post_training_dynamic_quant":
         if version.release < Version("1.7.0").release:   # pragma: no cover
             q_mapping = tq.default_mappings.DEFAULT_MODULE_MAPPING
```

### Comparing `neural_compressor-2.1.1/neural_compressor/version.py` & `neural_compressor-2.2/neural_compressor/version.py`

 * *Files 18% similar despite different names*

```diff
@@ -12,8 +12,8 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Intel Neural Compressor: An open-source Python library supporting popular model compression techniques."""
-__version__ = "2.1.1"
+__version__ = "2.2"
```

### Comparing `neural_compressor-2.1.1/neural_compressor.egg-info/PKG-INFO` & `neural_compressor-2.2/neural_compressor.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 Metadata-Version: 2.1
 Name: neural-compressor
-Version: 2.1.1
+Version: 2.2
 Summary: Repository of Intel Neural Compressor
 Home-page: https://github.com/intel/neural-compressor
 Author: Intel AIA Team
 Author-email: feng.tian@intel.com, haihao.shen@intel.com, suyue.chen@intel.com
 License: Apache 2.0
-Keywords: quantization,auto-tuning,post-training static quantization,post-training dynamic quantization,quantization-aware training,tuning strategy
+Keywords: quantization,auto-tuning,post-training static quantization,post-training dynamic quantization,quantization-aware training
 Classifier: Intended Audience :: Science/Research
 Classifier: Programming Language :: Python :: 3
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: License :: OSI Approved :: Apache Software License
 Requires-Python: >=3.6.0
 Description-Content-Type: text/markdown
 License-File: LICENSE
 
 <div align="center">
-  
+
 Intel Neural Compressor
 ===========================
 <h3> An open-source Python library supporting popular model compression techniques on all mainstream deep learning frameworks (TensorFlow, PyTorch, ONNX Runtime, and MXNet)</h3>
 
 [![python](https://img.shields.io/badge/python-3.7%2B-blue)](https://github.com/intel/neural-compressor)
-[![version](https://img.shields.io/badge/release-2.1-green)](https://github.com/intel/neural-compressor/releases)
+[![version](https://img.shields.io/badge/release-2.2-green)](https://github.com/intel/neural-compressor/releases)
 [![license](https://img.shields.io/badge/license-Apache%202-blue)](https://github.com/intel/neural-compressor/blob/master/LICENSE)
 [![coverage](https://img.shields.io/badge/coverage-85%25-green)](https://github.com/intel/neural-compressor)
 [![Downloads](https://static.pepy.tech/personalized-badge/neural-compressor?period=total&units=international_system&left_color=grey&right_color=green&left_text=downloads)](https://pepy.tech/project/neural-compressor)
 
 [Architecture](./docs/source/design.md#architecture)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Workflow](./docs/source/design.md#workflow)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Results](./docs/source/validated_model_list.md)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Examples](./examples/README.md)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[Documentations](https://intel.github.io/neural-compressor)
 </div>
 
@@ -48,19 +48,19 @@
 ### Install from pypi
 ```Shell
 pip install neural-compressor
 ```
 > More installation methods can be found at [Installation Guide](./docs/source/installation_guide.md). Please check out our [FAQ](./docs/source/faq.md) for more details.
 
 ## Getting Started
-### Quantization with Python API    
+### Quantization with Python API
 
 ```shell
 # Install Intel Neural Compressor and TensorFlow
-pip install neural-compressor 
+pip install neural-compressor
 pip install tensorflow
 # Prepare fp32 model
 wget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6/mobilenet_v1_1.0_224_frozen.pb
 ```
 ```python
 from neural_compressor.config import PostTrainingQuantConfig
 from neural_compressor.data import DataLoader
@@ -72,51 +72,45 @@
 from neural_compressor.quantization import fit
 q_model = fit(
     model="./mobilenet_v1_1.0_224_frozen.pb",
     conf=PostTrainingQuantConfig(),
     calib_dataloader=dataloader,
     eval_dataloader=dataloader)
 ```
-> More quick samples can be found in [Get Started Page](./docs/source/get_started.md).
 
 ## Documentation
 
 <table class="docutils">
   <thead>
   <tr>
     <th colspan="8">Overview</th>
   </tr>
   </thead>
   <tbody>
     <tr>
       <td colspan="2" align="center"><a href="./docs/source/design.md#architecture">Architecture</a></td>
       <td colspan="2" align="center"><a href="./docs/source/design.md#workflow">Workflow</a></td>
-      <td colspan="2" align="center"><a href="https://intel.github.io/neural-compressor/latest/docs/source/api-doc/apis.html">APIs</a></td>
-      <td colspan="2" align="center"><a href="./docs/source/bench.md">GUI</a></td>
-    </tr>
-    <tr>
-      <td colspan="2" align="center"><a href="examples/README.md#notebook-examples">Notebook</a></td>
       <td colspan="2" align="center"><a href="examples/README.md">Examples</a></td>
-      <td colspan="4" align="center"><a href="https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html">Intel oneAPI AI Analytics Toolkit</a></td>
+      <td colspan="2" align="center"><a href="https://intel.github.io/neural-compressor/latest/docs/source/api-doc/apis.html">APIs</a></td>
     </tr>
   </tbody>
   <thead>
     <tr>
       <th colspan="8">Python-based APIs</th>
     </tr>
   </thead>
   <tbody>
     <tr>
         <td colspan="2" align="center"><a href="./docs/source/quantization.md">Quantization</a></td>
         <td colspan="2" align="center"><a href="./docs/source/mixed_precision.md">Advanced Mixed Precision</a></td>
-        <td colspan="2" align="center"><a href="./docs/source/pruning.md">Pruning (Sparsity)</a></td> 
+        <td colspan="2" align="center"><a href="./docs/source/pruning.md">Pruning (Sparsity)</a></td>
         <td colspan="2" align="center"><a href="./docs/source/distillation.md">Distillation</a></td>
     </tr>
     <tr>
-        <td colspan="2" align="center"><a href="./docs/source/orchestration.md">Orchestration</a></td>        
+        <td colspan="2" align="center"><a href="./docs/source/orchestration.md">Orchestration</a></td>
         <td colspan="2" align="center"><a href="./docs/source/benchmark.md">Benchmarking</a></td>
         <td colspan="2" align="center"><a href="./docs/source/distributed.md">Distributed Compression</a></td>
         <td colspan="2" align="center"><a href="./docs/source/export.md">Model Export</a></td>
     </tr>
   </tbody>
   <thead>
     <tr>
@@ -125,45 +119,55 @@
   </thead>
   <tbody>
     <tr>
         <td colspan="2" align="center"><a href="./neural_coder/docs/PythonLauncher.md">Launcher</a></td>
         <td colspan="2" align="center"><a href="./neural_coder/extensions/neural_compressor_ext_lab/README.md">JupyterLab Extension</a></td>
         <td colspan="2" align="center"><a href="./neural_coder/extensions/neural_compressor_ext_vscode/README.md">Visual Studio Code Extension</a></td>
         <td colspan="2" align="center"><a href="./neural_coder/docs/SupportMatrix.md">Supported Matrix</a></td>
-    </tr>    
+    </tr>
   </tbody>
   <thead>
       <tr>
         <th colspan="8">Advanced Topics</th>
       </tr>
   </thead>
   <tbody>
       <tr>
           <td colspan="2" align="center"><a href="./docs/source/adaptor.md">Adaptor</a></td>
           <td colspan="2" align="center"><a href="./docs/source/tuning_strategies.md">Strategy</a></td>
           <td colspan="2" align="center"><a href="./docs/source/distillation_quantization.md">Distillation for Quantization</a></td>
           <td colspan="2" align="center"><a href="./docs/source/smooth_quant.md">SmoothQuant</td>
       </tr>
   </tbody>
+  <thead>
+      <tr>
+        <th colspan="8">Innovations for Productivity</th>
+      </tr>
+  </thead>
+  <tbody>
+      <tr>
+          <td colspan="4" align="center"><a href="./neural_insights/README.md">Neural Insights</a></td>
+          <td colspan="4" align="center"><a href="./neural_solution/README.md">Neural Solution</a></td>
+      </tr>
+  </tbody>
 </table>
 
 ## Selected Publications/Events
+* Blog on Medium: [Intel Optimization at Netflix](https://medium.com/@amerather_9719/intel-optimization-at-netflix-79ef0efb9d2) (May 2023)
 * Blog on Medium: [Effective Post-training Quantization for Large Language Models with Enhanced SmoothQuant Approach](https://medium.com/@NeuralCompressor/effective-post-training-quantization-for-large-language-models-with-enhanced-smoothquant-approach-93e9d104fb98) (Apr 2023)
 * Blog by Intel: [Intel Xeon Processors Are Still the Only CPU With MLPerf Results, Raising the Bar By 5x](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Xeon-Processors-Are-Still-the-Only-CPU-With-MLPerf-Results/post/1472750) (Apr 2023)
-* Post on Social Media: [Adopt with Tencent TACO: Heterogeneous optimization is also key to improving AI computing power](https://mp.weixin.qq.com/s/I-FQqOuW7HTnwXegLGNAtw) (Mar 2023)
-* Post on Social Media: [Training and Inference for Stable Diffusion | Intel Business](https://www.youtube.com/watch?v=emCgSTlJaAg) (Jan 2023)
 * NeurIPS'2022: [Fast Distilbert on CPUs](https://arxiv.org/abs/2211.07715) (Oct 2022)
 * NeurIPS'2022: [QuaLA-MiniLM: a Quantized Length Adaptive MiniLM](https://arxiv.org/abs/2210.17114) (Oct 2022)
 
 > View our [Full Publication List](./docs/source/publication_list.md).
 
 ## Additional Content
 
 * [Release Information](./docs/source/releases_info.md)
 * [Contribution Guidelines](./docs/source/CONTRIBUTING.md)
 * [Legal Information](./docs/source/legal_information.md)
 * [Security Policy](SECURITY.md)
 
 ## Research Collaborations
 
-Welcome to raise any interesting research ideas on model compression techniques and feel free to reach us (inc.maintainers@intel.com). Look forward to our collaborations on Intel Neural Compressor!
+Welcome to raise any interesting research ideas on model compression techniques and feel free to reach us ([inc.maintainers@intel.com](mailto:inc.maintainers@intel.com)). Look forward to our collaborations on Intel Neural Compressor!
```

### Comparing `neural_compressor-2.1.1/neural_compressor.egg-info/SOURCES.txt` & `neural_compressor-2.2/neural_compressor.egg-info/SOURCES.txt`

 * *Files 12% similar despite different names*

```diff
@@ -125,21 +125,24 @@
 neural_compressor/adaptor/tensorflow.py
 neural_compressor/adaptor/tensorflow.yaml
 neural_compressor/adaptor/tensorflow_itex.yaml
 neural_compressor/adaptor/keras_utils/__init__.py
 neural_compressor/adaptor/keras_utils/conv2d.py
 neural_compressor/adaptor/keras_utils/dense.py
 neural_compressor/adaptor/keras_utils/depthwise_conv2d.py
+neural_compressor/adaptor/keras_utils/pool2d.py
 neural_compressor/adaptor/keras_utils/quantizer.py
 neural_compressor/adaptor/keras_utils/separable_conv2d.py
 neural_compressor/adaptor/mxnet_utils/__init__.py
 neural_compressor/adaptor/mxnet_utils/util.py
 neural_compressor/adaptor/ox_utils/__init__.py
 neural_compressor/adaptor/ox_utils/calibration.py
+neural_compressor/adaptor/ox_utils/calibrator.py
 neural_compressor/adaptor/ox_utils/quantizer.py
+neural_compressor/adaptor/ox_utils/smooth_quant.py
 neural_compressor/adaptor/ox_utils/util.py
 neural_compressor/adaptor/ox_utils/operators/__init__.py
 neural_compressor/adaptor/ox_utils/operators/activation.py
 neural_compressor/adaptor/ox_utils/operators/argmax.py
 neural_compressor/adaptor/ox_utils/operators/attention.py
 neural_compressor/adaptor/ox_utils/operators/binary_op.py
 neural_compressor/adaptor/ox_utils/operators/concat.py
@@ -148,24 +151,29 @@
 neural_compressor/adaptor/ox_utils/operators/embed_layernorm.py
 neural_compressor/adaptor/ox_utils/operators/gather.py
 neural_compressor/adaptor/ox_utils/operators/gavgpool.py
 neural_compressor/adaptor/ox_utils/operators/gemm.py
 neural_compressor/adaptor/ox_utils/operators/lstm.py
 neural_compressor/adaptor/ox_utils/operators/matmul.py
 neural_compressor/adaptor/ox_utils/operators/maxpool.py
+neural_compressor/adaptor/ox_utils/operators/norm.py
 neural_compressor/adaptor/ox_utils/operators/ops.py
 neural_compressor/adaptor/ox_utils/operators/pad.py
 neural_compressor/adaptor/ox_utils/operators/pooling.py
+neural_compressor/adaptor/ox_utils/operators/reduce.py
 neural_compressor/adaptor/ox_utils/operators/resize.py
 neural_compressor/adaptor/ox_utils/operators/split.py
+neural_compressor/adaptor/ox_utils/operators/unary_op.py
 neural_compressor/adaptor/tf_utils/__init__.py
 neural_compressor/adaptor/tf_utils/graph_converter.py
 neural_compressor/adaptor/tf_utils/graph_converter_without_calib.py
 neural_compressor/adaptor/tf_utils/graph_util.py
 neural_compressor/adaptor/tf_utils/quantize_graph_common.py
+neural_compressor/adaptor/tf_utils/smooth_quant_calibration.py
+neural_compressor/adaptor/tf_utils/smooth_quant_scaler.py
 neural_compressor/adaptor/tf_utils/tf2onnx_converter.py
 neural_compressor/adaptor/tf_utils/util.py
 neural_compressor/adaptor/tf_utils/graph_rewriter/__init__.py
 neural_compressor/adaptor/tf_utils/graph_rewriter/graph_base.py
 neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/__init__.py
 neural_compressor/adaptor/tf_utils/graph_rewriter/bf16/bf16_convert.py
 neural_compressor/adaptor/tf_utils/graph_rewriter/generic/__init__.py
@@ -255,15 +263,17 @@
 neural_compressor/adaptor/tf_utils/transform_graph/bias_correction.py
 neural_compressor/adaptor/tf_utils/transform_graph/graph_transform_base.py
 neural_compressor/adaptor/tf_utils/transform_graph/insert_logging.py
 neural_compressor/adaptor/tf_utils/transform_graph/rerange_quantized_concat.py
 neural_compressor/adaptor/torch_utils/__init__.py
 neural_compressor/adaptor/torch_utils/bf16_convert.py
 neural_compressor/adaptor/torch_utils/hawq_metric.py
+neural_compressor/adaptor/torch_utils/mixed_precision.py
 neural_compressor/adaptor/torch_utils/model_wrapper.py
+neural_compressor/adaptor/torch_utils/pattern_detector.py
 neural_compressor/adaptor/torch_utils/smooth_quant.py
 neural_compressor/adaptor/torch_utils/symbolic_trace.py
 neural_compressor/adaptor/torch_utils/util.py
 neural_compressor/algorithm/__init__.py
 neural_compressor/algorithm/algorithm.py
 neural_compressor/algorithm/fast_bias_correction.py
 neural_compressor/algorithm/smooth_quant.py
@@ -275,14 +285,18 @@
 neural_compressor/compression/pruner/__init__.py
 neural_compressor/compression/pruner/criteria.py
 neural_compressor/compression/pruner/patterns.py
 neural_compressor/compression/pruner/pruners.py
 neural_compressor/compression/pruner/regs.py
 neural_compressor/compression/pruner/schedulers.py
 neural_compressor/compression/pruner/utils.py
+neural_compressor/compression/pruner/model_slim/__init__.py
+neural_compressor/compression/pruner/model_slim/auto_slim.py
+neural_compressor/compression/pruner/model_slim/pattern_analyzer.py
+neural_compressor/compression/pruner/model_slim/weight_slim.py
 neural_compressor/conf/__init__.py
 neural_compressor/conf/config.py
 neural_compressor/conf/dotdict.py
 neural_compressor/conf/pythonic_config.py
 neural_compressor/contrib/__init__.py
 neural_compressor/contrib/strategy/__init__.py
 neural_compressor/contrib/strategy/sigopt.py
@@ -332,14 +346,18 @@
 neural_compressor/experimental/common/metric.py
 neural_compressor/experimental/common/model.py
 neural_compressor/experimental/common/optimizer.py
 neural_compressor/experimental/common/postprocess.py
 neural_compressor/experimental/common/torch_utils.py
 neural_compressor/experimental/compression/__init__.py
 neural_compressor/experimental/compression/pruning.py
+neural_compressor/experimental/contrib/__init__.py
+neural_compressor/experimental/contrib/strategy/__init__.py
+neural_compressor/experimental/contrib/strategy/sigopt.py
+neural_compressor/experimental/contrib/strategy/tpe.py
 neural_compressor/experimental/data/__init__.py
 neural_compressor/experimental/data/dataloaders/__init__.py
 neural_compressor/experimental/data/dataloaders/base_dataloader.py
 neural_compressor/experimental/data/dataloaders/dataloader.py
 neural_compressor/experimental/data/dataloaders/default_dataloader.py
 neural_compressor/experimental/data/dataloaders/fetcher.py
 neural_compressor/experimental/data/dataloaders/mxnet_dataloader.py
@@ -362,15 +380,14 @@
 neural_compressor/experimental/data/transforms/imagenet_transform.py
 neural_compressor/experimental/data/transforms/tokenization.py
 neural_compressor/experimental/data/transforms/transform.py
 neural_compressor/experimental/export/__init__.py
 neural_compressor/experimental/export/qlinear2qdq.py
 neural_compressor/experimental/export/tf2onnx.py
 neural_compressor/experimental/export/torch2onnx.py
-neural_compressor/experimental/export/utils.py
 neural_compressor/experimental/metric/__init__.py
 neural_compressor/experimental/metric/bleu.py
 neural_compressor/experimental/metric/bleu_util.py
 neural_compressor/experimental/metric/coco_label_map.py
 neural_compressor/experimental/metric/coco_tools.py
 neural_compressor/experimental/metric/evaluate_squad.py
 neural_compressor/experimental/metric/f1.py
@@ -394,14 +411,29 @@
 neural_compressor/experimental/pytorch_pruner/__init__.py
 neural_compressor/experimental/pytorch_pruner/logger.py
 neural_compressor/experimental/pytorch_pruner/patterns.py
 neural_compressor/experimental/pytorch_pruner/prune_utils.py
 neural_compressor/experimental/pytorch_pruner/pruner.py
 neural_compressor/experimental/pytorch_pruner/pruning.py
 neural_compressor/experimental/pytorch_pruner/scheduler.py
+neural_compressor/experimental/strategy/__init__.py
+neural_compressor/experimental/strategy/auto_mixed_precision.py
+neural_compressor/experimental/strategy/basic.py
+neural_compressor/experimental/strategy/bayesian.py
+neural_compressor/experimental/strategy/exhaustive.py
+neural_compressor/experimental/strategy/mse.py
+neural_compressor/experimental/strategy/mse_v2.py
+neural_compressor/experimental/strategy/random.py
+neural_compressor/experimental/strategy/strategy.py
+neural_compressor/experimental/strategy/utils/__init__.py
+neural_compressor/experimental/strategy/utils/constant.py
+neural_compressor/experimental/strategy/utils/tuning_sampler.py
+neural_compressor/experimental/strategy/utils/tuning_space.py
+neural_compressor/experimental/strategy/utils/tuning_structs.py
+neural_compressor/experimental/strategy/utils/utility.py
 neural_compressor/metric/__init__.py
 neural_compressor/metric/bleu.py
 neural_compressor/metric/bleu_util.py
 neural_compressor/metric/coco_label_map.py
 neural_compressor/metric/coco_tools.py
 neural_compressor/metric/evaluate_squad.py
 neural_compressor/metric/f1.py
@@ -411,14 +443,36 @@
 neural_compressor/model/keras_model.py
 neural_compressor/model/model.py
 neural_compressor/model/mxnet_model.py
 neural_compressor/model/nets_factory.py
 neural_compressor/model/onnx_model.py
 neural_compressor/model/tensorflow_model.py
 neural_compressor/model/torch_model.py
+neural_compressor/profiling/__init__.py
+neural_compressor/profiling/parser/__init__.py
+neural_compressor/profiling/parser/factory.py
+neural_compressor/profiling/parser/parser.py
+neural_compressor/profiling/parser/result.py
+neural_compressor/profiling/parser/onnx_parser/__init__.py
+neural_compressor/profiling/parser/onnx_parser/factory.py
+neural_compressor/profiling/parser/onnx_parser/parser.py
+neural_compressor/profiling/parser/tensorflow_parser/__init__.py
+neural_compressor/profiling/parser/tensorflow_parser/factory.py
+neural_compressor/profiling/parser/tensorflow_parser/parser.py
+neural_compressor/profiling/profiler/__init__.py
+neural_compressor/profiling/profiler/factory.py
+neural_compressor/profiling/profiler/profiler.py
+neural_compressor/profiling/profiler/onnxrt_profiler/__init__.py
+neural_compressor/profiling/profiler/onnxrt_profiler/factory.py
+neural_compressor/profiling/profiler/onnxrt_profiler/profiler.py
+neural_compressor/profiling/profiler/onnxrt_profiler/utils.py
+neural_compressor/profiling/profiler/tensorflow_profiler/__init__.py
+neural_compressor/profiling/profiler/tensorflow_profiler/factory.py
+neural_compressor/profiling/profiler/tensorflow_profiler/profiler.py
+neural_compressor/profiling/profiler/tensorflow_profiler/utils.py
 neural_compressor/strategy/__init__.py
 neural_compressor/strategy/auto.py
 neural_compressor/strategy/auto_mixed_precision.py
 neural_compressor/strategy/basic.py
 neural_compressor/strategy/bayesian.py
 neural_compressor/strategy/conservative.py
 neural_compressor/strategy/exhaustive.py
@@ -429,23 +483,19 @@
 neural_compressor/strategy/strategy.py
 neural_compressor/strategy/utils/__init__.py
 neural_compressor/strategy/utils/constant.py
 neural_compressor/strategy/utils/tuning_sampler.py
 neural_compressor/strategy/utils/tuning_space.py
 neural_compressor/strategy/utils/tuning_structs.py
 neural_compressor/strategy/utils/utility.py
-neural_compressor/template/__init__.py
-neural_compressor/template/api_doc_example.py
-neural_compressor/template/graph_optimization.yaml
-neural_compressor/template/pruning.yaml
-neural_compressor/template/ptq.yaml
-neural_compressor/template/qat.yaml
 neural_compressor/utils/__init__.py
 neural_compressor/utils/collect_layer_histogram.py
 neural_compressor/utils/constant.py
 neural_compressor/utils/create_obj_from_config.py
 neural_compressor/utils/kl_divergence.py
 neural_compressor/utils/load_huggingface.py
 neural_compressor/utils/logger.py
+neural_compressor/utils/neural_insights_utils.py
 neural_compressor/utils/options.py
 neural_compressor/utils/pytorch.py
-neural_compressor/utils/utility.py
+neural_compressor/utils/utility.py
+neural_compressor/utils/weights_details.py
```

