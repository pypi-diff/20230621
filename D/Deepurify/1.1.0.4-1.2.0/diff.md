# Comparing `tmp/Deepurify-1.1.0.4-py3-none-any.whl.zip` & `tmp/Deepurify-1.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,35 +1,36 @@
-Zip file size: 53037 bytes, number of entries: 33
+Zip file size: 57908 bytes, number of entries: 34
 -rw-r--r--  2.0 unx     7545 b- defN 23-May-11 06:32 Deepurify/IOUtils.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Feb-24 04:15 Deepurify/__init__.py
--rw-r--r--  2.0 unx    10314 b- defN 23-Jun-15 05:10 Deepurify/clean_func.py
--rw-r--r--  2.0 unx    10726 b- defN 23-Jun-15 05:10 Deepurify/cli.py
--rw-r--r--  2.0 unx    14498 b- defN 23-Jun-03 06:00 Deepurify/funcs.py
--rw-r--r--  2.0 unx     4026 b- defN 23-Mar-04 01:02 Deepurify/CallGenesTools/CallGenesUtils.py
+-rw-r--r--  2.0 unx    13439 b- defN 23-Jun-21 04:20 Deepurify/clean_func.py
+-rw-r--r--  2.0 unx    13986 b- defN 23-Jun-20 02:40 Deepurify/cli.py
+-rw-r--r--  2.0 unx    14636 b- defN 23-Jun-18 06:27 Deepurify/funcs.py
+-rw-r--r--  2.0 unx     3872 b- defN 23-Jun-19 04:37 Deepurify/CallGenesTools/CallGenesUtils.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Feb-24 04:09 Deepurify/CallGenesTools/__init__.py
+-rw-r--r--  2.0 unx     8542 b- defN 23-Jun-19 04:32 Deepurify/CallGenesTools/prodigal.py
 -rw-r--r--  2.0 unx    13167 b- defN 23-Mar-02 02:09 Deepurify/DataTools/DataUtils.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Feb-24 04:09 Deepurify/DataTools/__init__.py
--rw-r--r--  2.0 unx    14410 b- defN 23-Mar-06 02:34 Deepurify/FilterBinsTools/FilterUtils.py
+-rw-r--r--  2.0 unx    15310 b- defN 23-Jun-20 03:05 Deepurify/FilterBinsTools/FilterUtils.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Feb-24 04:09 Deepurify/FilterBinsTools/__init__.py
 -rw-r--r--  2.0 unx     1267 b- defN 23-Feb-24 06:09 Deepurify/LabelContigTools/GaussianLearnContigDist.py
--rw-r--r--  2.0 unx    35507 b- defN 23-Mar-18 02:02 Deepurify/LabelContigTools/LabelBinUtils.py
+-rw-r--r--  2.0 unx    35846 b- defN 23-Jun-16 07:29 Deepurify/LabelContigTools/LabelBinUtils.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Feb-24 04:09 Deepurify/LabelContigTools/__init__.py
 -rw-r--r--  2.0 unx     6665 b- defN 23-Feb-24 06:10 Deepurify/Model/Convolutions.py
 -rw-r--r--  2.0 unx      404 b- defN 23-Feb-24 06:10 Deepurify/Model/Embedding.py
--rw-r--r--  2.0 unx    13972 b- defN 23-Jun-15 05:59 Deepurify/Model/EncoderModels.py
+-rw-r--r--  2.0 unx    13972 b- defN 23-Jun-15 08:14 Deepurify/Model/EncoderModels.py
 -rw-r--r--  2.0 unx    11469 b- defN 23-Apr-29 02:40 Deepurify/Model/FormerLayers.py
 -rw-r--r--  2.0 unx     4568 b- defN 23-Feb-24 06:11 Deepurify/Model/Loss.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Feb-24 04:09 Deepurify/Model/__init__.py
--rw-r--r--  2.0 unx     7661 b- defN 23-Mar-09 07:58 Deepurify/SelectMAGsTools/SelectionUitls.py
+-rw-r--r--  2.0 unx     7715 b- defN 23-Jun-21 01:37 Deepurify/SelectMAGsTools/SelectionUitls.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Feb-24 04:09 Deepurify/SelectMAGsTools/__init__.py
 -rw-r--r--  2.0 unx    13115 b- defN 23-Jun-15 04:28 Deepurify/SeqProcessTools/SequenceDataset.py
 -rw-r--r--  2.0 unx    15973 b- defN 23-Jun-15 04:22 Deepurify/SeqProcessTools/SequenceUtils.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Feb-24 04:09 Deepurify/SeqProcessTools/__init__.py
 -rw-r--r--  2.0 unx     1451 b- defN 23-Feb-24 04:09 Deepurify/TrainTools/Scheduler.py
 -rw-r--r--  2.0 unx    13962 b- defN 23-May-11 07:13 Deepurify/TrainTools/TrainUtils.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Feb-24 04:09 Deepurify/TrainTools/__init__.py
--rw-r--r--  2.0 unx      434 b- defN 23-Jun-15 06:22 Deepurify-1.1.0.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-15 06:22 Deepurify-1.1.0.4.dist-info/WHEEL
--rw-r--r--  2.0 unx       48 b- defN 23-Jun-15 06:22 Deepurify-1.1.0.4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       10 b- defN 23-Jun-15 06:22 Deepurify-1.1.0.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2915 b- defN 23-Jun-15 06:22 Deepurify-1.1.0.4.dist-info/RECORD
-33 files, 204199 bytes uncompressed, 48283 bytes compressed:  76.4%
+-rw-r--r--  2.0 unx      432 b- defN 23-Jun-21 06:49 Deepurify-1.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-21 06:49 Deepurify-1.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       48 b- defN 23-Jun-21 06:49 Deepurify-1.2.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       10 b- defN 23-Jun-21 06:49 Deepurify-1.2.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2998 b- defN 23-Jun-21 06:49 Deepurify-1.2.0.dist-info/RECORD
+34 files, 220484 bytes uncompressed, 53026 bytes compressed:  76.0%
```

## zipnote {}

```diff
@@ -15,14 +15,17 @@
 
 Filename: Deepurify/CallGenesTools/CallGenesUtils.py
 Comment: 
 
 Filename: Deepurify/CallGenesTools/__init__.py
 Comment: 
 
+Filename: Deepurify/CallGenesTools/prodigal.py
+Comment: 
+
 Filename: Deepurify/DataTools/DataUtils.py
 Comment: 
 
 Filename: Deepurify/DataTools/__init__.py
 Comment: 
 
 Filename: Deepurify/FilterBinsTools/FilterUtils.py
@@ -78,23 +81,23 @@
 
 Filename: Deepurify/TrainTools/TrainUtils.py
 Comment: 
 
 Filename: Deepurify/TrainTools/__init__.py
 Comment: 
 
-Filename: Deepurify-1.1.0.4.dist-info/METADATA
+Filename: Deepurify-1.2.0.dist-info/METADATA
 Comment: 
 
-Filename: Deepurify-1.1.0.4.dist-info/WHEEL
+Filename: Deepurify-1.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: Deepurify-1.1.0.4.dist-info/entry_points.txt
+Filename: Deepurify-1.2.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: Deepurify-1.1.0.4.dist-info/top_level.txt
+Filename: Deepurify-1.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: Deepurify-1.1.0.4.dist-info/RECORD
+Filename: Deepurify-1.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## Deepurify/clean_func.py

```diff
@@ -7,119 +7,148 @@
 
 def cleanMAGs(
     input_bin_folder_path: str,
     output_bin_folder_path: str,
     bin_suffix: str,
     gpu_num: int,
     batch_size_per_gpu: int,
-    num_worker_per_device: int,
+    num_threads_per_device: int,
     overlapping_ratio=0.5,
     cutSeqLength=8192,
-    num_cpus_call_genes=12,
+    num_threads_call_genes=12,
     hmm_acc_cutoff=0.6,
     hmm_align_ratio_cutoff=0.4,
     estimated_completeness_threshold=0.55,
     seq_length_threshold=550000,
-    checkM_parallel_num=1,
-    num_cpus_per_checkm=12,
-    dfs_or_greedy="dfs",
-    topK=3,
+    checkM_process_num=1,
+    num_threads_per_checkm=12,
+    topk_or_greedy="topk",
+    topK_num=3,
     temp_output_folder: Union[str, None] = None,
     output_bins_meta_info_path: Union[str, None] = None,
     info_files_path: Union[str, None] = None,
     modelWeightPath: Union[str, None] = None,
     taxoVocabPath: Union[str, None] = None,
     taxoTreePath: Union[str, None] = None,
     taxoName2RepNormVecPath: Union[str, None] = None,
     hmmModelPath: Union[str, None] = None,
     model_config: Union[Dict, None] = None,
-    stop_at_step2: bool = False
+    self_evaluate: bool = False
 ):
     """
 
     The main function to clean the MAGs.
 
     Args:
-        input_bin_folder_path (str): The folder of input MAGs.
+        input_bin_folder_path (str): The folder of input MAGs
 
-        output_bin_folder_path (str): The folder uses to output cleaned MAGs.
+        output_bin_folder_path (str): The folder used to output cleaned MAGs.
 
-        bin_suffix (str): The bin suffix of MAGs.
+        bin_suffix (str): The bin suffix of MAG files.
 
-        gpu_num (int): The number of GPUs would be used. 0 means to use CPU. (ATTENTION: CPU is significantly slower !!!!) Better to provide at least one GPU.
-
-        batch_size_per_gpu (int): The number of sequences would be loaded to one GPU. It is useless if gpu_num is 0.
-
-        num_worker_per_device (int): The number of workers for one GPU or CPU. The number of batch_size_per_gpu would divide this value for per worker.
-
-        overlapping_ratio (float): The overlapping ratio would be used if the length of contig exceeds the cutSeqLength (8192). Defaults to 0.5.
-
-        cutSeqLength (int, optional): The length to cut the contig if the length of it longer than this value. Defaults to 8192. (ATTENTION: 8192 is the maximum length during training.)
-
-        num_cpus_call_genes (int, optional): The number of threads to call genes. Defaults to 12.
-
-        hmm_acc_cutoff (float, optional): The threshold when the hmm model decides to treat the called gene's sequence as SCG. Defaults to 0.6.
-
-        hmm_align_ratio_cutoff (float, optional): The threshold of alignment coverage when the called gene's sequence aligned to the SCG. Defaults to 0.4.
-
-        estimated_completeness_threshold (float, optional): The threshold of estimated completeness for filtering bins generated by applying those SCGs. Defaults to 0.55.
-
-        seq_length_threshold (int, optional): The threshold of a MAG's contigs' total length for filtering generated MAGs after applying SCGs.  Defaults to 550000.
-
-        checkM_parallel_num (int, optional): The number of processes to run CheckM simultaneously. Defaults to 1.
-
-        num_cpus_per_checkm (int, optional): The number of threads to run a CheckM process. Defaults to 12.
-
-        dfs_or_greedy (str, optional): Depth first searching or greedy searching to label a contig. Defaults to "dfs".
-
-        topK (int, optional): The Top-k nodes that have maximum cosine similarity with the contig encoded vector would be searched (Useless for greedy search). Defaults to 3.
-
-        temp_output_folder (Union[str, None], optional): The path of a folder to store temporary files. Defaults to None.
-        If the path is None, it would be built in the paraent folder of 'input_bin_folder_path'.
-
-        output_bins_meta_info_path (Union[str, None], optional): The path of a txt file that uses to record the meta informations (evaluated result) of final cleaned MAGs. Defaults to None.
-        If the path is None, then the file would be built under the 'output_bin_folder_path'.
-
-        info_files_path (Union[str, None]): The path of DeepurifyInfoFiles folder. Defaults to None.
-        If the path is None, please make sure you have set the environment variable 'DeepurifyInfoFiles'. 
+        gpu_num (int): The number of GPUs to be used can be specified. Defaults to 0.
+        If you set it to 0, the code will utilize the CPU. 
+        However, please note that using the CPU can result in significantly slower processing speed. 
+        It is recommended to provide at least one GPU for better performance.
+
+        batch_size_per_gpu (int): The batch size per GPU determines the number of sequences that will be loaded onto each GPU. 
+        This parameter is only applicable if the --gpu_num option is set to a value greater than 0. 
+        The default value is 2, meaning that two sequences will be loaded per GPU batch.
+        The batch size for CPU is 2.
+
+        num_threads_per_device (int): The number of threads per GPU or CPU determines the parallelism level during contigs' inference stage. 
+        If the value of --gpu_num is greater than 0, each GPU will have a set number of threads to do inference. 
+        Similarly, if --gpu_num is set to 0 and the code is running on CPU, the specified number of threads will be used. 
+        By default, the number of threads per GPU or CPU is set to 1. 
+        The --batch_size_per_gpu value will be divided by the number of threads to determine the batch size per thread.
+
+        overlapping_ratio (float): The --overlapping_ratio is a parameter used when the length of a contig exceeds the specified --cut_seq_length. 
+        By default, the overlapping ratio is set to 0.5. 
+        This means that when a contig is longer than the --cut_seq_length, it will be split into overlapping subsequences with 50\%\ overlap between consecutive subsequences.
+
+        cutSeqLength (int, optional): The --cut_seq_length parameter determines the length at which a contig will be cut if its length exceeds this value. 
+        The default setting is 8192, which is also the maximum length allowed during training. 
+        If a contig's length surpasses this threshold, it will be divided into smaller subsequences with lengths equal to or less than the cut_seq_length.
+        
+        num_threads_call_genes (int, optional): The number of threads to call genes. Defaults to 12.
+
+        hmm_acc_cutoff (float, optional): If the acc score and the aligned ratio assigned by the HMM model for a gene sequence exceeds this threshold, 
+        it would be considered as a single-copy gene. It is set to 0.6 by default.
+
+        hmm_align_ratio_cutoff (float, optional): If the acc score and the aligned ratio assigned by the HMM model for a gene sequence exceeds this threshold, 
+        it would be considered as a single-copy gene. It is set to 0.4 by default.
+
+        estimated_completeness_threshold (float, optional): The --estimate_completeness_threshold is used as a criterion for filtering MAGs that are generated 
+        by applying specific single-copy genes (SCGs). The default threshold is set to 0.55. 
+        MAGs with an estimated completeness score equal to or higher than this threshold will be considered for further analysis or inclusion, 
+        while those falling below the threshold may be filtered out.
+
+        seq_length_threshold (int, optional): The threshold for the total length of a MAG's contigs is used to filter generated MAGs after applying single-copy genes (SCGs). 
+        The default threshold is set to 550,000, which represents the total length of the contigs in base pairs (bp). 
+        MAGs with a total contig length equal to or greater than this threshold will be considered for further analysis or inclusion, 
+        while MAGs with a total contig length below the threshold may be filtered out.
+
+        checkM_process_num (int, optional): The number of processes to run CheckM simultaneously. Defaults to 1.
+
+        num_threads_per_checkm (int, optional): The number of threads to run a single CheckM process. Defaults to 12.
+
+        topk_or_greedy (str, optional): Topk searching or greedy searching to label a contig. Defaults to 'topk'.
+        The contig is assigned a label based on the top-k most relevant or similar taxonomic lineages. 
+        The specific number of lineages considered for labeling can be determined by the value of k.
+
+        topK_num (int, optional): During the top-k searching approach, the default behavior is to search for the top-k taxon nodes that exhibit the highest cosine 
+        similarity with the contig's encoded vector. 
+        By default, the value of k is set to 3, meaning that the three most similar nodes in terms of cosine similarity will be considered for labeling the contig. 
+        Please note that this parameter does not have any effect when using the greedy search approach (topK_num=1).
+
+        temp_output_folder (Union[str, None], optional): The temporary files generated during the process can be stored in a specified folder path. 
+        By default, if no path is provided (i.e., set to None), the temporary files will be stored in the parent folder of the 'input_bin_folder_path' location. 
+        However, you have the option to specify a different folder path to store these temporary files if needed.
+
+        output_bins_meta_info_path (Union[str, None], optional): The path of a text file can be provided to record the meta information, including the evaluated results, of the final cleaned MAGs. 
+        By default, if no path is specified (i.e., set to None), the file will be created under the 'output_bin_folder_path' directory. 
+        However, you have the flexibility to specify a different file path if desired.
+
+        info_files_path (Union[str, None]): The DeepurifyInfoFiles is essential for running Deepurify. 
+        By default, if no path is provided (i.e., set to None), it is expected that the environment variable 'DeepurifyInfoFiles' has been set to point to the appropriate folder. 
+        Please ensure that the 'DeepurifyInfoFiles' environment variable is correctly configured if the path is not explicitly provided.
 
         modelWeightPath (Union[str, None], optional): The path of model weight. (In DeepurifyInfoFiles folder) Defaults to None.
 
         taxoVocabPath (Union[str, None], optional): The path of taxon vocabulary. (In DeepurifyInfoFiles folder) Defaults to None.
 
         taxoTreePath (Union[str, None], optional): The path of taxonomic tree. (In DeepurifyInfoFiles folder) Defaults to None.
 
         taxoName2RepNormVecPath (Union[str, None], optional): The path of taxonomic lineage encoded vectors. (In DeepurifyInfoFiles folder) Defaults to None.
 
         hmmModelPath (Union[str, None], optional): The path of SCGs' hmm file. (In DeepurifyInfoFiles folder) Defaults to None.
 
         model_config (Union[Dict, None], optional): The config of model. See the TrainScript.py to find more information. Defaults to None.
         It would be used if you trained a another model with different model_config. Please set this variable equal with None at present.
 
-        stop_at_step2 (bool, optional): If stop deepurify at step 2 workflow. Defaults to False. 
-        This parameter is appropriate to calculate the metrics if you know which contig is clean and which is contaminated under the simulated dataset if this variable is True.
-        For a MAG, we would only output contigs containing the core lineage label at distinct taxonomic ranks (with different cosine similarity threshold.) if this variable is True.
-        The outputs are all in the folder of /temp_output_folder/FilterOutput/ PATH. 
-        We would not evaluate the cleaned MAGs at different taxonomic ranks. 
-        You should independently evaluate the outcomes from various taxonomic ranks and select the best output from the cleaned MAGs. 
+        self_evaluate (bool, optional): Evaluate the results by the user. Defaults to False. 
+        Set to True if you have knowledge of clean and contaminated contigs in the simulated dataset or you want to evaluate the outcomes by yourself.
+        We would remove the outlier contigs and only keep clean contigs with different cosine similarity threshold for a MAG if this variable is True.
+        The outputs will be stored in the following folder path: /temp_output_folder/FilterOutput/
+        You should independently evaluate the outcomes from various similarity threshold and select the best output from the cleaned MAGs.
     """
 
     print("##################################")
     print("###  WELCOME TO USE DEEPURIFY  ###")
     print("##################################")
     print()
     assert batch_size_per_gpu <= 20, "batch_size_per_gpu must smaller or equal with 20."
-    assert num_worker_per_device <= 4, "num_worker_per_device must smaller or equal with 4."
+    assert num_threads_per_device <= 4, "num_threads_per_device must smaller or equal with 4."
 
     if "/" == input_bin_folder_path[-1]:
         input_bin_folder_path = input_bin_folder_path[0:-1]
 
     filesFolder = os.path.split(input_bin_folder_path)[0]
     if temp_output_folder is None:
-        temp_output_folder = os.path.join(filesFolder, "DeepurifyTempOut")
+        temp_output_folder = os.path.join(filesFolder, "DeepurifyTempFiles")
 
     if output_bins_meta_info_path is None:
         output_bins_meta_info_path = os.path.join(output_bin_folder_path, "MetaInfo.txt")
 
     if gpu_num == 0:
         gpu_work_ratio = []
     else:
@@ -149,15 +178,15 @@
         "The variable mer3Path or mer4Path is None. Please check this file if it is in 'DeepurifyInfoFiles' folder.")
     assert modelWeightPath is not None, ValueError(
         "The variable modelWeightPath is None. Please check this file if it is in 'DeepurifyInfoFiles' folder.")
     assert taxoVocabPath is not None, ValueError(
         "The variable taxoVocabPath is None. Please check this file if it is in 'DeepurifyInfoFiles' folder.")
     assert taxoTreePath is not None, ValueError(
         "The variable taxoTreePath is None. Please check this file if it is in 'DeepurifyInfoFiles' folder.")
-    if stop_at_step2 is False:
+    if self_evaluate is False:
         assert hmmModelPath is not None, ValueError(
             "The variable hmmModelPath is None. Please check this file if it is in 'DeepurifyInfoFiles' folder.")
 
     if os.path.exists(filesFolder) is False:
         print("Your input folder is not exist.")
         sys.exit(1)
 
@@ -169,25 +198,25 @@
         modelWeightPath=modelWeightPath,
         hmmModelPath=hmmModelPath,
         taxoVocabPath=taxoVocabPath,
         taxoTreePath=taxoTreePath,
         taxoName2RepNormVecPath=taxoName2RepNormVecPath,
         gpus_work_ratio=gpu_work_ratio,
         batch_size_per_gpu=batch_size_per_gpu,
-        num_worker_per_device=num_worker_per_device,
+        num_threads_per_device=num_threads_per_device,
         bin_suffix=bin_suffix,
         mer3Path=mer3Path,
         mer4Path=mer4Path,
         overlapping_ratio=overlapping_ratio,
         cutSeqLength=cutSeqLength,
-        num_cpus_call_genes=num_cpus_call_genes,
+        num_threads_call_genes=num_threads_call_genes,
         ratio_cutoff=hmm_align_ratio_cutoff,
         acc_cutoff=hmm_acc_cutoff,
         estimated_completeness_threshold=estimated_completeness_threshold,
         seq_length_threshold=seq_length_threshold,
-        checkM_parallel_num=checkM_parallel_num,
-        num_cpus_per_checkm=num_cpus_per_checkm,
-        dfsORgreedy=dfs_or_greedy,
-        topK=topK,
+        checkM_process_num=checkM_process_num,
+        num_threads_per_checkm=num_threads_per_checkm,
+        topkORgreedy=topk_or_greedy,
+        topK=topK_num,
         model_config=model_config,
-        stop_at_step2=stop_at_step2
+        self_evaluate=self_evaluate
     )
```

## Deepurify/cli.py

```diff
@@ -36,15 +36,15 @@
                 vocab_dict[word] = k
                 k += 1
     return vocab_dict
 
 
 def cli():
     myparser = argparse.ArgumentParser(
-        prog=os.path.basename(sys.argv[0]), description="Deepurify is a tool to improving the quality of MAGs by decontaminating."
+        prog=os.path.basename(sys.argv[0]), description="Deepurify is a tool to improving the quality of MAGs."
     )
     subparsers = myparser.add_subparsers(dest="command")
 
     clean_parser = subparsers.add_parser("clean", help="Filtering the contamination in MAGs.")
 
     # Add parameters
     clean_parser.add_argument(
@@ -52,120 +52,167 @@
         "--input_path",
         required=True,
         help="The folder of input MAGs.")
     clean_parser.add_argument(
         "-o",
         "--output_path",
         required=True,
-        help="The folder uses to output cleaned MAGs.")
+        help="The folder used to output cleaned MAGs.")
     clean_parser.add_argument(
         "--bin_suffix",
         required=True,
-        help="The bin suffix of MAGs",
+        help="The bin suffix of MAG files.",
         type=str)
     clean_parser.add_argument(
         "--gpu_num",
-        default=0,
-        help="The number of GPUs would be used. 0 means to use CPU. (ATTENTION: CPU is significantly slower !!!!) Better to provide at least one GPU.",
+        default=1,
+        help="""The number of GPUs to be used can be specified. Defaults to 1.
+        If you set it to 0, the code will utilize the CPU. 
+        However, please note that using the CPU can result in significantly slower processing speed. 
+        It is recommended to provide at least one GPU for better performance.""",
         type=int
     )
     clean_parser.add_argument(
         "--batch_size_per_gpu",
-        default=1,
-        help="The batch size for per GPU. The number of sequences would be loaded to one GPU. It is useless if --gpu_num is 0.",
+        default=2,
+        help="""The batch size per GPU determines the number of sequences that will be loaded onto each GPU. 
+        This parameter is only applicable if the --gpu_num option is set to a value greater than 0. 
+        The default value is 2, meaning that two sequences will be loaded per GPU batch.
+        The batch size for CPU is 2.
+        """,
         type=int)
     clean_parser.add_argument(
-        "--num_worker_per_device",
+        "--num_threads_per_device",
         default=1,
         type=int,
-        help="The number of workers for one GPU or CPU. The number of batch_size_per_gpu would divide this value for per worker."
+        help="""The number of threads per GPU or CPU determines the parallelism level during contigs' inference stage. 
+        If the value of --gpu_num is greater than 0, each GPU will have a set number of threads to do inference. 
+        Similarly, if --gpu_num is set to 0 and the code is running on CPU, the specified number of threads will be used. 
+        By default, the number of threads per GPU or CPU is set to 1. 
+        The --batch_size_per_gpu value will be divided by the number of threads to determine the batch size per thread.
+        """
     )
 
     ### optional ###
     clean_parser.add_argument(
         "--overlapping_ratio",
         default=0.5,
         type=float,
-        help="The overlapping ratio if the length of contig exceeds the --cut_seq_length. Defaults to 0.5."
+        help="""The --overlapping_ratio is a parameter used when the length of a contig exceeds the specified --cut_seq_length. 
+        By default, the overlapping ratio is set to 0.5. 
+        This means that when a contig is longer than the --cut_seq_length, it will be split into overlapping subsequences with 50\%\ overlap between consecutive subsequences.
+        """
     )
     clean_parser.add_argument(
         "--cut_seq_length",
         default=8192,
         type=int,
-        help="The length to cut the contig if the length of it longer than this value. Defaults to 8192. (ATTENTION: 8192 is the maximum length during training.)")
+        help="""The --cut_seq_length parameter determines the length at which a contig will be cut if its length exceeds this value. 
+        The default setting is 8192, which is also the maximum length allowed during training. 
+        If a contig's length surpasses this threshold, it will be divided into smaller subsequences with lengths equal to or less than the cut_seq_length.
+        """)
     clean_parser.add_argument(
-        "--num_cpus_call_genes",
+        "--num_threads_call_genes",
         default=12,
         type=int,
         help="The number of threads to call genes. Defaults to 12.")
     clean_parser.add_argument(
         "--hmm_acc_cutoff",
         default=0.6,
         type=float,
-        help="The threshold when the hmm model decides to treat the called gene's sequence as SCG. Defaults to 0.6.",
+        help="""If the acc score and the aligned ratio assigned by the HMM model for a gene sequence exceeds this threshold, it would be considered as a single-copy gene.
+        It is set to 0.6 by default.
+        """,
     )
     clean_parser.add_argument(
         "--hmm_align_ratio_cutoff",
         default=0.4,
         type=float,
-        help="The threshold of alignment coverage when the called gene's sequence aligned to the SCG. Defaults to 0.4.",
+        help="""If the acc score and the aligned ratio assigned by the HMM model for a gene sequence exceeds this threshold, it would be considered as a single-copy gene.
+        It is set to 0.4 by default.
+        """,
     )
     clean_parser.add_argument(
         "--estimate_completeness_threshold",
         default=0.55,
         type=float,
-        help="The threshold of estimated completeness for filtering MAGs generated by applying those SCGs. Defaults to 0.55.",
+        help="""The --estimate_completeness_threshold is used as a criterion for filtering MAGs that are generated by applying specific single-copy genes (SCGs). 
+        The default threshold is set to 0.55. 
+        MAGs with an estimated completeness score equal to or higher than this threshold will be considered for further analysis or inclusion, 
+        while those falling below the threshold may be filtered out.
+        """,
     )
     clean_parser.add_argument(
         "--seq_length_threshold",
         default=550000,
         type=int,
-        help="The threshold of a MAG's contigs' total length for filtering generated MAGs after applying SCGs.  Defaults to 550000.",
+        help="""The threshold for the total length of a MAG's contigs is used to filter generated MAGs after applying single-copy genes (SCGs). 
+        The default threshold is set to 550,000, which represents the total length of the contigs in base pairs (bp). 
+        MAGs with a total contig length equal to or greater than this threshold will be considered for further analysis or inclusion, 
+        while MAGs with a total contig length below the threshold may be filtered out.
+        """,
     )
     clean_parser.add_argument(
-        "--checkM_parallel_num",
+        "--checkM_process_num",
         default=1,
         choices=[1, 2, 3, 6],
         type=int,
         help="The number of processes to run CheckM simultaneously. Defaults to 1.")
     clean_parser.add_argument(
-        "--num_cpus_per_checkm",
+        "--num_threads_per_checkm",
         default=12,
         type=int,
-        help="The number of threads to run a CheckM process. Defaults to 12.")
+        help="The number of threads to run a single CheckM process. Defaults to 12.")
     clean_parser.add_argument(
-        "--dfs_or_greedy",
-        default="dfs",
-        choices=["dfs", "greedy"],
+        "--topk_or_greedy_search",
+        default="topk",
+        choices=["topk", "greedy"],
         type=str,
-        help="Depth first searching or greedy searching to label a contig. Defaults to 'dfs'."
+        help="""Topk searching or greedy searching to label a contig. Defaults to 'topk'.
+        The contig is assigned a label based on the top-k most relevant or similar taxonomic lineages. 
+        The specific number of lineages considered for labeling can be determined by the value of k.
+        """
     )
     clean_parser.add_argument(
-        "--topK",
+        "--topK_num",
         default=3,
         type=int,
-        help="The Top-k nodes that have maximum cosine similarity with the contig encoded vector would be searched (Useless for greedy search). Defaults to 3.")
+        help="""
+        During the top-k searching approach, the default behavior is to search for the top-k nodes that exhibit the highest cosine similarity with the contig's encoded vector. 
+        By default, the value of k is set to 3, meaning that the three most similar nodes in terms of cosine similarity will be considered for labeling the contig. 
+        Please note that this parameter does not have any effect when using the greedy search approach (topK_num=1).
+        """)
     clean_parser.add_argument(
         "--temp_output_folder",
         default=None,
         type=str,
-        help="The path of a folder to store temporary files. Defaults to None. If the path is None, it would be built in the paraent folder of 'input_bin_folder_path'.",
+        help="""
+        The temporary files generated during the process can be stored in a specified folder path. 
+        By default, if no path is provided (i.e., set to None), the temporary files will be stored in the parent folder of the '--input_path' location. 
+        However, you have the option to specify a different folder path to store these temporary files if needed.
+        """,
     )
     clean_parser.add_argument(
         "--output_bins_meta_info_path",
         default=None,
         type=str,
-        help="The path of a txt file that uses to record the meta informations (evaluated result) of final cleaned MAGs. Defaults to None." +
-        "If the path is None, then the file would be built under the 'output_bin_folder_path'.",
+        help="""
+        The path of a text file can be provided to record the meta information, including the evaluated results, of the final cleaned MAGs. 
+        By default, if no path is specified (i.e., set to None), the file will be created under the "--output_path" directory. 
+        However, you have the flexibility to specify a different file path if desired.
+        """,
     )
     clean_parser.add_argument(
         "--info_files_path",
         default=None,
-        help="The path of DeepurifyInfoFiles folder. Defaults to None." +
-        "If the path is None, please make sure you have set the environment variable 'DeepurifyInfoFiles'.",
+        help="""
+        The DeepurifyInfoFiles is essential for running Deepurify. 
+        By default, if no path is provided (i.e., set to None), it is expected that the environment variable 'DeepurifyInfoFiles' has been set to point to the appropriate folder. 
+        Please ensure that the 'DeepurifyInfoFiles' environment variable is correctly configured if the path is not explicitly provided.
+        """,
         type=str
     )
     clean_parser.add_argument(
         "--model_weight_path",
         default=None,
         type=str,
         help="The path of model weight. (In DeepurifyInfoFiles folder) Defaults to None.")
@@ -190,30 +237,28 @@
     clean_parser.add_argument(
         "--hmm_model_path",
         default=None,
         type=str,
         help="The path of SCGs' hmm file. (In DeepurifyInfoFiles folder) Defaults to None.",
     )
     clean_parser.add_argument(
-        "--stop_at_step2",
+        "--self_evaluate",
         default=False,
         type=bool,
         choices=[True, False],
-        help="""
-        If stop deepurify at step 2 workflow. Defaults to False. 
-        This parameter is appropriate to calculate the metrics if you know which contig is clean and which is contaminated under the simulated dataset if this variable is True.
-        For a MAG, we would only output contigs containing the core lineage label at distinct taxonomic ranks (with different cosine similarity threshold.) if this variable is True.
-        The outputs are all in the folder of /temp_output_folder/FilterOutput/ PATH. 
-        We would not evaluate the cleaned MAGs at different taxonomic ranks. 
-        You should independently evaluate the outcomes from various taxonomic ranks and select the best output from the cleaned MAGs. 
+        help="""Evaluate the results by the user. Defaults to False. 
+        Set to True if you have knowledge of clean and contaminated contigs in the simulated dataset or you want to evaluate the outcomes by yourself.
+        We would remove the outlier contigs and only keep clean contigs with different cosine similarity threshold for a MAG if this variable is True.
+        The outputs will be stored in the following folder path: /temp_output_folder/FilterOutput/
+        You should independently evaluate the outcomes from various similarity threshold and select the best output from the cleaned MAGs.
         """)
 
     #### build parser ####
     bulid_parser = subparsers.add_parser(
-        "build", help="Build the files like taxonomy tree and the taxonomy vocabulary for training.")
+        "build", help="(Do not use this command at present.) Build the files like taxonomy tree and the taxonomy vocabulary for training.")
     # Add parameter
     bulid_parser.add_argument(
         "-i",
         "--input_taxo_lineage_weight_file_path",
         required=True,
         type=str,
         help="The path of the taxonomic lineages weights file. This file has two columns. " +
@@ -239,44 +284,44 @@
     if args.command == "clean":
         cleanMAGs(
             args.input_path,
             args.output_path,
             args.bin_suffix,
             args.gpu_num,
             args.batch_size_per_gpu,
-            args.num_worker_per_device,
+            args.num_threads_per_device,
             args.overlapping_ratio,
             args.cut_seq_length,
-            args.num_cpus_call_genes,
+            args.num_threads_call_genes,
             args.hmm_acc_cutoff,
             args.hmm_align_ratio_cutoff,
             args.estimate_completeness_threshold,
             args.seq_length_threshold,
-            args.checkM_parallel_num,
-            args.num_cpus_per_checkm,
-            args.dfs_or_greedy,
-            args.topK,
+            args.checkM_process_num,
+            args.num_threads_per_checkm,
+            args.topk_or_greedy_search,
+            args.topK_num,
             args.temp_output_folder,
             args.output_bins_meta_info_path,
             args.info_files_path,
             args.model_weight_path,
             args.taxo_vocab_path,
             args.taxo_tree_path,
             args.taxo_lineage_vector_file_path,
             args.hmm_model_path,
             None,
-            args.stop_at_step2
+            args.self_evaluate
         )
 
     elif args.command == "build":
         taxo_tree = bulid_tree(args.input_weight_file_path)
         writePickle(args.output_tree_path, taxo_tree)
         vocab = build_taxo_vocabulary(args.input_weight_file_path)
         with open(args.output_vocabulary_path, "w") as wh:
             for word, index in vocab.items():
                 wh.write(word+"\t"+str(index) + "\n")
     else:
         print("#################################")
         print("### RUN THE DEEPURIFY PROJECT ###")
         print("#################################")
         print()
-        print("Please use 'deepurify -h' or 'deepurify clean -h' or 'deepurify build -h' for helping.")
+        print("Please use 'deepurify -h' or 'deepurify clean -h' for helping.")
```

## Deepurify/funcs.py

```diff
@@ -15,14 +15,17 @@
 from Deepurify.LabelContigTools.LabelBinUtils import (buildTextsRepNormVector,
                                                       labelBinsFolder)
 from Deepurify.Model.EncoderModels import SequenceCLIP
 from Deepurify.SelectMAGsTools.SelectionUitls import findBestBinsAfterFiltering
 from Deepurify.CallGenesTools.CallGenesUtils import splitListEqually
 
 
+index2Taxo = {1: "T1_filter", 2: "T2_filter", 3: "T3_filter", 4: "T4_filter", 5: "T5_filter", 6: "T6_filter"}
+
+
 def runLabelFilterSplitBins(
     inputBinFolder: str,
     tempFileOutFolder: str,
     outputBinFolder: str,
     outputBinsMetaFilePath: str,
     modelWeightPath: str,
     hmmModelPath: str,
@@ -30,30 +33,30 @@
     taxoTreePath: str,
     taxoName2RepNormVecPath: str,
     bin_suffix: str,
     mer3Path: str,
     mer4Path: str,
     gpus_work_ratio: List[float],
     batch_size_per_gpu: List[float],
-    num_worker_per_device: int,
+    num_threads_per_device: int,
     overlapping_ratio: float,
     cutSeqLength: int,
-    num_cpus_call_genes: int,
+    num_threads_call_genes: int,
     ratio_cutoff: float,
     acc_cutoff: float,
     estimated_completeness_threshold: float,
     seq_length_threshold: int,
-    checkM_parallel_num: int,
-    num_cpus_per_checkm: int,
-    dfsORgreedy: str,
+    checkM_process_num: int,
+    num_threads_per_checkm: int,
+    topkORgreedy: str,
     topK: int,
     model_config: Dict,
-    stop_at_step2=False
+    self_evaluate=False
 ):
-    print("Labeling taxonomic lineage for each contig in bins...")
+    print("Estimating Taxonomic Similarity by Labeling Lineage for Each Contig in the MAG.")
     print()
     if os.path.exists(tempFileOutFolder) is False:
         os.mkdir(tempFileOutFolder)
     annotOutputFolder = os.path.join(tempFileOutFolder, "AnnotOutput")
     num_gpu = len(gpus_work_ratio)
     if os.path.exists(annotOutputFolder) is False:
         os.mkdir(annotOutputFolder)
@@ -92,28 +95,28 @@
             num_lstm_layer=model_config["num_lstm_layers"],
             IRB_layers=model_config["IRB_num"],
             expand=model_config["expand"],
             feature_dim=model_config["feature_dim"],
             drop_connect_ratio=0.0,
             dropout=0.0,
         )
-        print("DO NOT FIND taxoName2RepNormVecPath FILE. Start to build taxoName2RepNormVecPath file. ")
+        print("Warning, DO NOT FIND taxoName2RepNormVecPath FILE. Start to build taxoName2RepNormVecPath file. ")
         state = torch.load(modelWeightPath, map_location="cpu")
         model.load_state_dict(state, strict=True)
         with torch.no_grad():
             buildTextsRepNormVector(taxo_tree, model, taxo_vocabulary, "cpu", taxoName2RepNormVecPath)
     processList = []
     error_queue = Queue()
     if num_gpu == 0:
         binFilesList = os.listdir(inputBinFolder)
         totalNum = len(binFilesList)
         nextIndex = 0
-        for i in range(num_worker_per_device):
-            if i != (num_worker_per_device) - 1:
-                cutFileLength = totalNum // num_worker_per_device + 1
+        for i in range(num_threads_per_device):
+            if i != (num_threads_per_device) - 1:
+                cutFileLength = totalNum // num_threads_per_device + 1
                 curDataFilesList = binFilesList[nextIndex: nextIndex + cutFileLength]
                 nextIndex += cutFileLength
             else:
                 curDataFilesList = binFilesList[nextIndex:]
             processList.append(
                 Process(
                     target=labelBinsFolder,
@@ -123,75 +126,74 @@
                         "cpu",
                         modelWeightPath,
                         mer3Path,
                         mer4Path,
                         taxoVocabPath,
                         taxoTreePath,
                         taxoName2RepNormVecPath,
-                        8,
+                        2,
                         6,
                         bin_suffix,
                         curDataFilesList,
                         2,
                         overlapping_ratio,
                         cutSeqLength,
-                        dfsORgreedy,
+                        topkORgreedy,
                         topK,
                         error_queue,
                         model_config,
                     ),
                 )
             )
             print("Processer {} has {} files.".format(i, len(curDataFilesList)))
             processList[-1].start()
     else:
         assert sum(gpus_work_ratio) == 1.0
         for b in batch_size_per_gpu:
-            assert b % num_worker_per_device == 0, "The batch size number in batch_size_per_gpu can not divide num_worker_per_device."
+            assert b % num_threads_per_device == 0, f"The batch size number: {b} in batch_size_per_gpu can not divide num_threads_per_device: {num_threads_per_device}."
         gpus = ["cuda:" + str(i) for i in range(num_gpu)]
         binFilesList = os.listdir(inputBinFolder)
         totalNum = len(binFilesList)
         nextIndex = 0
-        for i in range(num_gpu * num_worker_per_device):
-            if i != (num_gpu * num_worker_per_device) - 1:
-                cutFileLength = int(totalNum * gpus_work_ratio[i //
-                                    num_worker_per_device] / num_worker_per_device + 0.0) + 1
+        for i in range(num_gpu * num_threads_per_device):
+            if i != (num_gpu * num_threads_per_device) - 1:
+                cutFileLength = int(totalNum * gpus_work_ratio[i // num_threads_per_device] / num_threads_per_device + 0.0) + 1
                 curDataFilesList = binFilesList[nextIndex: nextIndex + cutFileLength]
                 nextIndex += cutFileLength
             else:
                 curDataFilesList = binFilesList[nextIndex:]
             processList.append(
                 Process(
                     target=labelBinsFolder,
                     args=(
                         inputBinFolder,
                         annotOutputFolder,
-                        gpus[i // num_worker_per_device],
+                        gpus[i // num_threads_per_device],
                         modelWeightPath,
                         mer3Path,
                         mer4Path,
                         taxoVocabPath,
                         taxoTreePath,
                         taxoName2RepNormVecPath,
-                        batch_size_per_gpu[i // num_worker_per_device] // num_worker_per_device,
+                        batch_size_per_gpu[i // num_threads_per_device] // num_threads_per_device,
                         6,
                         bin_suffix,
                         curDataFilesList,
                         2,
                         overlapping_ratio,
                         cutSeqLength,
-                        dfsORgreedy,
+                        topkORgreedy,
                         topK,
                         error_queue,
                         model_config,
                     ),
                 )
             )
             print("Processer {} has {} files in device {} .".format(
-                i, len(curDataFilesList), gpus[i // num_worker_per_device]))
+                i, len(curDataFilesList), gpus[i // num_threads_per_device]))
             processList[-1].start()
 
     # error collection
     queue_len = 0
     n = len(processList)
     while True:
         if not error_queue.empty():
@@ -215,44 +217,47 @@
     if os.path.exists(filterOutputFolder) is False:
         os.mkdir(filterOutputFolder)
 
     temp_folder_path = os.path.join(tempFileOutFolder, "CalledGenes")
     if os.path.exists(temp_folder_path) is False:
         os.mkdir(temp_folder_path)
 
-    if stop_at_step2 is False:
+    originalBinsCheckMPath = None
+    if self_evaluate is False:
         print("\n")
         print("Starting Call Genes...")
-        callMarkerGenes(inputBinFolder, temp_folder_path, num_cpus_call_genes, hmmModelPath, bin_suffix)
+        callMarkerGenes(inputBinFolder, temp_folder_path, num_threads_call_genes, hmmModelPath, bin_suffix)
+        print("Starting Run CheckM...")
+        runCheckMsingle(inputBinFolder, os.path.join(filterOutputFolder, "original_checkm.txt"),
+                    num_threads_per_checkm * checkM_process_num, bin_suffix)
+        originalBinsCheckMPath = os.path.join(filterOutputFolder, "original_checkm.txt")
 
     print("\n")
     print("Starting Filter Contaminations and Separate Bins...")
     filterContaminationFolder(
         annotOutputFolder,
         inputBinFolder,
         temp_folder_path,
         filterOutputFolder,
         bin_suffix,
         ratio_cutoff,
         acc_cutoff,
         estimated_completeness_threshold,
         seq_length_threshold,
-        stop_at_step2=stop_at_step2
+        originalBinsCheckMPath,
+        self_evaluate=self_evaluate
     )
 
-    if stop_at_step2:
+    if self_evaluate:
         print()
         return
 
     print("\n")
     print("Starting Run CheckM...")
-    runCheckMsingle(inputBinFolder, os.path.join(filterOutputFolder, "original_checkm.txt"),
-                    num_cpus_per_checkm * checkM_parallel_num, bin_suffix)
-    runCheckMParall(filterOutputFolder, bin_suffix, checkM_parallel_num, num_cpus_per_checkm)
-    originalBinsCheckMPath = os.path.join(filterOutputFolder, "original_checkm.txt")
+    runCheckMParall(filterOutputFolder, bin_suffix, checkM_process_num, num_threads_per_checkm)
 
     print("\n")
     print("Starting Gather Result...")
     wh = open(outputBinsMetaFilePath, "w")
     binFilesList = os.listdir(inputBinFolder)
     tN = len(binFilesList)
     for i, binFileName in enumerate(binFilesList):
@@ -319,18 +324,14 @@
                 print("############################################")
                 raise RuntimeError("binFolder: {}, Checkm Result Path: {}".format(binsFolder, checkmResFilePath))
             runCheckMsingle(binsFolder, checkmResFilePath, num_cpu, bin_suffix, repTime + 1)
     # res.wait()
     res.kill()
 
 
-index2Taxo = {1: "phylum_filter", 2: "class_filter", 3: "order_filter",
-              4: "family_filter", 5: "genus_filter", 6: "species_filter"}
-
-
 def runCheckMForSixFilter(filterFolder, indexList: List, num_checkm_cpu: int, bin_suffix: str):
     for i in indexList:
         binsFolder = os.path.join(filterFolder, index2Taxo[i])
         files = os.listdir(binsFolder)
         n = 0
         copyList = []
         for file in files:
```

## Deepurify/CallGenesTools/CallGenesUtils.py

```diff
@@ -1,11 +1,12 @@
 import os
 from multiprocessing import Process
 from subprocess import Popen
 from typing import List
+from Deepurify.CallGenesTools.prodigal import ProdigalRunner
 
 from Deepurify.IOUtils import readFasta
 
 
 def splitListEqually(input_list: List, num_parts: int) -> List[List[object]]:
     n = len(input_list)
     step = n // num_parts + 1
@@ -13,35 +14,27 @@
     for i in range(num_parts):
         curList = input_list[i * step: (i + 1) * step]
         if curList:
             out_list.append(curList)
     return out_list
 
 
-def runProgidalSingle(bin_path: str, ouputFAA_path: str) -> None:
-    if os.path.exists(ouputFAA_path):
+def runProgidalSingle(binName, bin_path: str, output_faa_folder_path: str) -> None:
+    outFAA_path = os.path.join(output_faa_folder_path, binName + ".faa")
+    if os.path.exists(outFAA_path):
         return
-    name2seq = readFasta(bin_path)
-    length = 0
-    mode = "single"
-    for key, seq in name2seq.items():
-        length += len(seq)
-    if length <= 100000:
-        mode = "meta"
-    res = Popen("prodigal -p {} -q -m  -g 11 -a {} -i {} > /dev/null".format(mode, ouputFAA_path, bin_path), shell=True)
-    res.wait()
-    res.kill()
+    runner = ProdigalRunner(binName, output_faa_folder_path)
+    runner.run(bin_path)
 
 
 def subProcessProgidal(files: List[str], bin_folder_path: str, output_faa_folder_path: str) -> None:
     for file in files:
         binName = os.path.splitext(file)[0]
         bin_path = os.path.join(bin_folder_path, file)
-        outFAA_path = os.path.join(output_faa_folder_path, binName + ".faa")
-        runProgidalSingle(bin_path, outFAA_path)
+        runProgidalSingle(binName, bin_path, output_faa_folder_path)
 
 
 def runProgidalFolder(bin_folder_path: str, output_faa_folder_path: str, num_cpu: int, bin_suffix: str) -> None:
     files = os.listdir(bin_folder_path)
     bin_files = []
     for file in files:
         if os.path.splitext(file)[-1][1:] == bin_suffix:
```

## Deepurify/FilterBinsTools/FilterUtils.py

```diff
@@ -1,17 +1,23 @@
 import math
 import os
 import sys
 from copy import deepcopy
 from multiprocessing import Process
-from typing import Dict, List, Set, Tuple
+from typing import Dict, List, Set, Tuple, Union
 
-from Deepurify.IOUtils import (readAnnotResult, readFasta, readHMMFile,
-                               writeAnnot2BinNames, writeFasta)
-from Deepurify.LabelContigTools.LabelBinUtils import getBestMultiLabelsForFiltering
+import numpy as np
+
+from Deepurify.IOUtils import (readAnnotResult, readCheckMResultAndStat,
+                               readFasta, readHMMFile, writeAnnot2BinNames,
+                               writeFasta)
+from Deepurify.LabelContigTools.LabelBinUtils import \
+    getBestMultiLabelsForFiltering
+
+index2Taxo = {1: "T1_filter", 2: "T2_filter", 3: "T3_filter", 4: "T4_filter", 5: "T5_filter", 6: "T6_filter"}
 
 
 def summedLengthCal(
         name2seq: Dict[str, str]) -> int:
     summedLength = 0
     for _, seq in name2seq.items():
         summedLength += len(seq)
@@ -57,21 +63,29 @@
             curSet.add(info[0])
             splitContigSetList.append(curSet)
             curDict = dict()
             curDict.update(info[1])
             splitRecordGenes.append(curDict)
 
 
+def summedRecord(recordList):
+    summedValue = 0.0
+    for num, _ in recordList:
+        summedValue += num
+    return summedValue
+
+
 def splitContigs(
     contigName2seq: Dict[str, str],
     gene2contigNames: Dict[str, List[str]],
     contigName2_gene2num: Dict[str, Dict[str, int]],
     replication_times_threashold: int,
     estimate_completeness_threshold: float,
-    core: bool
+    core: bool,
+    thre: float = 0.72
 ) -> List[Dict[str, str]]:
     c1 = deepcopy(contigName2seq)
     c2 = deepcopy(gene2contigNames)
     c3 = deepcopy(contigName2_gene2num)
     contigSeqPair = []
     for contigName, seq in contigName2seq.items():
         contigSeqPair.append((contigName, len(seq)))
@@ -138,40 +152,65 @@
         if ratio not in ratioSet and score not in scoreSet:
             filtedContigList.append((curContig2seq, ratio, score))
             ratioSet.add(ratio)
             scoreSet.add(score)
 
     filtedContigList = sorted(filtedContigList, key=lambda x: x[-1], reverse=True)
     first = filtedContigList[0]
-    if first[1] <= 0.72:
+    if first[1] <= thre :
         return splitContigs(c1,
                             c2,
                             c3,
                             replication_times_threashold + 1,
                             estimate_completeness_threshold,
-                            core)
+                            core,
+                            thre)
     else:
         result = []
         for i, infoPair in enumerate(filtedContigList):
             if infoPair[1] >= estimate_completeness_threshold or i == 0:
                 result.append(infoPair[0])
         return result
 
 
+def modify(contigName2annot, coreNames):
+    N = 0.0
+    n = 0.0
+    recordCount = [[0.0, ""] for _ in range(len(coreNames))]
+    for _, annotLabel in contigName2annot.items():
+        for i, coreName in enumerate(coreNames):
+            if coreName == annotLabel:
+                n += 1
+                recordCount[i][0] += 1
+                recordCount[i][1] = coreName
+        N += 1
+    if (n / N + 0.0) >= 0.8:
+        sortedRecordCount = list(sorted(recordCount, key=lambda x: x[0]))
+        while len(sortedRecordCount) > 1 and (summedRecord(sortedRecordCount) / N + 0.0) > 0.68:
+            sortedRecordCount.pop(0)
+        newCoreNames = [coreNames[0]]
+        for _, coreTaxo in sortedRecordCount:
+            if coreTaxo != newCoreNames[0]:
+                newCoreNames.append(coreTaxo)
+        coreNames = deepcopy(newCoreNames)
+    return coreNames
+
+
 def filterContaminationOneBin(
     annotBinPath: str,
     binFastaPath: str,
     hmmFilePath: str,
     outputFastaFolder: str,
     taxoLevel: int,
     ratio_cutoff: float,
     acc_cutoff: float,
     estimate_completeness_threshold: float,
     seq_length_threshold: int,
-    stop_at_step2=False
+    originalBinsCheckMPath,
+    self_evaluate=False
 ) -> None:
     assert 1 <= taxoLevel <= 6, ValueError("The taxoLevel must between 1 to 6.")
     assert 0.4 <= ratio_cutoff, ValueError("The ratio_cutoff value must bigger than 0.4")
     assert 0.6 <= acc_cutoff, ValueError("acc_cutoff must bigger than 0.6")
 
     contigName2annot, contigName2probs = readAnnotResult(annotBinPath)
     contigName2seq = readFasta(binFastaPath)
@@ -187,45 +226,22 @@
     coreNames = []
     for core in coreList:
         coreNames.append("@".join(core[1:]))
 
     filtedContigName2seq = {}
     annot2_contigName2seq = {}
 
-    def summedRecord(recordList):
-        summedValue = 0.0
-        for num, _ in recordList:
-            summedValue += num
-        return summedValue
-
     if taxoLevel == 6:
-        N = 0.0
-        n = 0.0
-        recordCount = [[0.0, ""] for _ in range(len(coreNames))]
-        for _, annotLabel in contigName2annot.items():
-            for i, coreName in enumerate(coreNames):
-                if coreName == annotLabel:
-                    n += 1
-                    recordCount[i][0] += 1
-                    recordCount[i][1] = coreName
-            N += 1
-        if (n / N + 0.0) >= 0.8:
-            sortedRecordCount = list(sorted(recordCount, key=lambda x: x[0]))
-            while len(sortedRecordCount) > 1 and (summedRecord(sortedRecordCount) / N + 0.0) > 0.68:
-                sortedRecordCount.pop(0)
-            newCoreNames = [coreNames[0]]
-            for _, coreTaxo in sortedRecordCount:
-                if coreTaxo != newCoreNames[0]:
-                    newCoreNames.append(coreTaxo)
-            coreNames = deepcopy(newCoreNames)
+        coreNames = modify(contigName2annot, coreNames)
 
     for key, seq in contigName2seq.items():
         for coreName in coreNames:
             if coreName in contigName2annot[key]:
                 filtedContigName2seq[key] = seq
+        
         if key not in filtedContigName2seq:
             curAnnot = "@".join(contigName2annot[key].split("@")[0:taxoLevel])
             if curAnnot not in annot2_contigName2seq:
                 newDict = dict()
                 newDict[key] = seq
                 annot2_contigName2seq[curAnnot] = newDict
             else:
@@ -233,69 +249,78 @@
                 curDict[key] = seq
 
     # write files
     binName = os.path.split(binFastaPath)[-1]
     annot2binNames = {}
     writeFasta(filtedContigName2seq, os.path.join(outputFastaFolder, binName))
 
-    if stop_at_step2:
+    if self_evaluate:
         return
 
     # using SCGs to exclude external contigs.
     gene2contigList, contigName2_gene2num = readHMMFile(hmmFilePath, ratio_cutoff, acc_cutoff)
     annot2binNames[coreNames[0]] = [binName]
     binNamePro, bin_suffix = os.path.splitext(binName)
-    filtedContigName2seqList = splitContigs(filtedContigName2seq, gene2contigList, contigName2_gene2num, 1, estimate_completeness_threshold, True)
-    # Write the split bins from core taxonomy
-    k = 0
+    
+    assert originalBinsCheckMPath is not None, ValueError("The checkm result of original MAGs is None.")
+    res = readCheckMResultAndStat(originalBinsCheckMPath)[0]
+    q = res[binNamePro]
+    
+    filtedContigName2seqList = \
+    splitContigs(filtedContigName2seq, gene2contigList, contigName2_gene2num, 1, estimate_completeness_threshold, True, getThre(q[0], q[1], taxoLevel))
+    
+    idx_k = 0
     for coreName2seqFilter in filtedContigName2seqList:
         summedLength = summedLengthCal(coreName2seqFilter)
         if summedLength >= seq_length_threshold:
-            annot2binNames[coreNames[0]].append(binNamePro + "_Core_" + str(k) + bin_suffix)
-            writeFasta(coreName2seqFilter, os.path.join(outputFastaFolder, binNamePro + "_Core_" + str(k) + bin_suffix))
-            k += 1
-    # Write the split bins do not from the core taxonomy
-    k = 0
+            annot2binNames[coreNames[0]].append(binNamePro + "___" + str(idx_k) + bin_suffix)
+            writeFasta(coreName2seqFilter, os.path.join(outputFastaFolder, binNamePro + "___" + str(idx_k) + bin_suffix))
+            idx_k += 1
+
     gene2contigList, contigName2_gene2num = readHMMFile(hmmFilePath, ratio_cutoff, acc_cutoff)
     for annot, noCoreContigName2seq in annot2_contigName2seq.items():
         if summedLengthCal(noCoreContigName2seq) >= seq_length_threshold:
             if annot not in annot2binNames:
-                annot2binNames[annot] = [binNamePro + "_NoCore_" + str(k) + bin_suffix]
+                annot2binNames[annot] = [binNamePro + "___" + str(idx_k) + bin_suffix]
             else:
-                annot2binNames[annot].append(binNamePro + "_NoCore_" + str(k) + bin_suffix)
-            writeFasta(noCoreContigName2seq, os.path.join(outputFastaFolder, binNamePro + "_NoCore_" + str(k) + bin_suffix))
-            k += 1
+                annot2binNames[annot].append(binNamePro + "___" + str(idx_k) + bin_suffix)
+            writeFasta(noCoreContigName2seq, os.path.join(outputFastaFolder, binNamePro + "___" + str(idx_k) + bin_suffix))
+            idx_k += 1
+        
         curFilteredList = splitContigs(noCoreContigName2seq, gene2contigList, contigName2_gene2num, 1, estimate_completeness_threshold, False)
         for noCoreName2seqFilter in curFilteredList:
             summedLength = summedLengthCal(noCoreName2seqFilter)
             if summedLength >= seq_length_threshold:
                 if annot not in annot2binNames:
-                    annot2binNames[annot] = [binNamePro + "_NoCore_" + str(k) + bin_suffix]
+                    annot2binNames[annot] = [binNamePro + "___" + str(idx_k) + bin_suffix]
                 else:
-                    annot2binNames[annot].append(binNamePro + "_NoCore_" + str(k) + bin_suffix)
-                writeFasta(noCoreName2seqFilter, os.path.join(outputFastaFolder, binNamePro + "_NoCore_" + str(k) + bin_suffix))
-                k += 1
+                    annot2binNames[annot].append(binNamePro + "___" + str(idx_k) + bin_suffix)
+                writeFasta(noCoreName2seqFilter, os.path.join(outputFastaFolder, binNamePro + "___" + str(idx_k) + bin_suffix))
+                idx_k += 1
+
     writeAnnot2BinNames(annot2binNames, os.path.join(outputFastaFolder, binNamePro + "_BinNameToLineage.ann"))
 
 
-index2Taxo = {1: "phylum_filter", 2: "class_filter", 3: "order_filter", 4: "family_filter", 5: "genus_filter", 6: "species_filter"}
+def sigmoid(z):
+    return 1/(1 + np.exp(-z))
 
 
 def subProcessFilter(
     annotBinFolder: str,
     oriBinFolder: str,
     hmmOutFolder: str,
     outputFolder: str,
     bin_suffix: str,
     i: int,
     ratio_cutoff: float,
     acc_cutoff: float,
     estimate_completeness_threshold: float,
     seq_length_threshold: int,
-    stop_at_step2=False
+    originalBinsCheckMPath,
+    self_evaluate=False
 ):
     binFiles = os.listdir(oriBinFolder)
     N = len(binFiles)
     for j, binFastaName in enumerate(binFiles):
         binName, suffix = os.path.splitext(binFastaName)
         hmmFilePath = os.path.join(hmmOutFolder, binName + ".HMM.txt")
         if suffix[1:] != bin_suffix:
@@ -314,29 +339,31 @@
             hmmFilePath,
             os.path.join(outputFolder, index2Taxo[i + 1]),
             taxoLevel=i + 1,
             ratio_cutoff=ratio_cutoff,
             acc_cutoff=acc_cutoff,
             estimate_completeness_threshold=estimate_completeness_threshold,
             seq_length_threshold=seq_length_threshold,
-            stop_at_step2=stop_at_step2
+            originalBinsCheckMPath = originalBinsCheckMPath,
+            self_evaluate=self_evaluate
         )
 
 
 def filterContaminationFolder(
     annotBinFolderInput: str,
     oriBinFolder: str,
     hmmOutFolder: str,
     outputFolder: str,
     bin_suffix: str,
     ratio_cutoff: float,
     acc_cutoff: float,
     estimate_completeness_threshold: float,
     seq_length_threshold: int,
-    stop_at_step2=False
+    originalBinsCheckMPath: Union[str, None],
+    self_evaluate=False
 ):
     for i in range(6):
         if os.path.exists(os.path.join(outputFolder, index2Taxo[i + 1])) is False:
             os.mkdir(os.path.join(outputFolder, index2Taxo[i + 1]))
     res = []
     for i in range(6):
         res.append(
@@ -349,14 +376,27 @@
                     outputFolder,
                     bin_suffix,
                     i,
                     ratio_cutoff,
                     acc_cutoff,
                     estimate_completeness_threshold,
                     seq_length_threshold,
-                    stop_at_step2,
+                    originalBinsCheckMPath,
+                    self_evaluate,
                 ),
             )
         )
         res[-1].start()
     for p in res:
         p.join()
+
+
+def getThre(comp, conta, taxoLevel): 
+    v1 = (conta - 10.) / 100.
+    v2 = (comp - 50.) / 100.
+    a = math.log(sigmoid(v1 * 0.9 + v2 * 0.3 - taxoLevel * 0.0125) + 1., 4.35)
+    thre = 1.0 - a
+    if thre > 0.77:
+        thre = 0.77
+    elif thre < 0.67:
+        thre = 0.67
+    return thre
```

## Deepurify/LabelContigTools/LabelBinUtils.py

```diff
@@ -12,14 +12,30 @@
                                readPickle, readVocabulary, writeAnnotResult,
                                writePickle)
 from Deepurify.Model.EncoderModels import SequenceCLIP
 from Deepurify.SeqProcessTools.SequenceUtils import (
     ConvertSeqToImageTensorMoreFeatures, ConvertTextToIndexTensor)
 
 
+def gatherValues(v1, t2, num_labels):
+    """
+    t1 = torch.randn([3, 4])
+    t2 = torch.randn([3, 5, 4])
+    print(gatherValues(t1, t2, 5))
+    t2v = t2.view(15, 4)
+    print(t1 @ t2v.T)
+    """
+    b1 = v1.size(0)
+    b2 = t2.size(0)
+    assert b1 == b2, ValueError("Batch size is not equal.")
+    dotTensor = torch.tensordot(v1, t2, dims=([1], [2])).permute([0, 2, 1])  # [b1, num_labels, b2]
+    index = torch.arange(b1).expand([num_labels, b1]).transpose(1, 0).unsqueeze(-1).to(dotTensor.device)
+    return torch.gather(dotTensor, dim=-1, index=index).squeeze(-1)
+
+
 def buildTextsRepNormVector(taxo_tree: Dict, model: nn.Module, vocabulary: Dict[str, int], device: str, outputPath: str) -> Dict:
     text2repV = {}
 
     def inner(cur_taxo_tree: Dict, cur_text: str):
         if cur_taxo_tree["TaxoLevel"] != "genus":
             for child in cur_taxo_tree["Children"]:
                 this_name = child["Name"]
@@ -46,15 +62,14 @@
 
 def taxonomyLabelGreedySearch(
     visRepVector: torch.Tensor,
     cur_anntotated: str,
     taxo_tree: Dict,
     annotated_level: int,
     text2repNormVector: Dict,
-    gather_func: Callable,
     logitNum: torch.Tensor,
     curMaxList: List,
     phy_fc: Union[None, nn.Module],
 ):
     """
     visRepVector: one dimension sequence reprentation, shape: [dimension]
     cur_annotated: current annotated string. Taxonomy levels are split by '@'
@@ -69,15 +84,15 @@
         curTextsNames = []
         curNextChild = []
         for child in taxo_tree["Children"]:
             curstackedTextsTensorList.append(text2repNormVector[child["Name"]])
             curTextsNames.append(child["Name"])
             curNextChild.append(child)
         textNorm = torch.stack(curstackedTextsTensorList, dim=0).unsqueeze(0)
-        innerSFT = torch.softmax(gather_func(visRepNorm, textNorm, len(curstackedTextsTensorList)).squeeze(0) * logitNum, dim=-1)
+        innerSFT = torch.softmax(gatherValues(visRepNorm, textNorm, len(curstackedTextsTensorList)).squeeze(0) * logitNum, dim=-1)
         if len(innerSFT.shape) == 2:
             innerSFT = innerSFT.squeeze(0)
         innerMaxIndex = innerSFT.argmax()
         phySFT = torch.softmax(phy_fc(visRepNorm), dim=-1)
         if len(phySFT.shape) == 2:
             phySFT = phySFT.squeeze(0)
         phyMaxIndex = phySFT.argmax()
@@ -92,15 +107,15 @@
             maxIndex = innerMaxIndex
             curMaxList.append(innerSFT[maxIndex].item())
         annotatedRes = curTextsNames[maxIndex]
         next_taxo_tree = curNextChild[maxIndex]
         if next_taxo_tree["TaxoLevel"] != index2Taxo[annotated_level]:
             visRepVector = visRepVector.squeeze(0)
             return taxonomyLabelGreedySearch(
-                visRepVector, annotatedRes + "@", next_taxo_tree, annotated_level, text2repNormVector, gather_func, logitNum, curMaxList, None
+                visRepVector, annotatedRes + "@", next_taxo_tree, annotated_level, text2repNormVector, logitNum, curMaxList, None
             )
         else:
             return annotatedRes
     else:
         curstackedTextsTensorList = []
         curTextsNames = []
         curNextChild = []
@@ -109,40 +124,39 @@
                 curstackedTextsTensorList.append(text2repNormVector[cur_anntotated + child])
                 curTextsNames.append(cur_anntotated + child)
             else:
                 curstackedTextsTensorList.append(text2repNormVector[cur_anntotated + child["Name"]])
                 curTextsNames.append(cur_anntotated + child["Name"])
                 curNextChild.append(child)
         textNorm = torch.stack(curstackedTextsTensorList, dim=0).unsqueeze(0)
-        innerSFT = torch.softmax(gather_func(visRepNorm, textNorm, len(curstackedTextsTensorList)).squeeze(0) * logitNum, dim=-1)
+        innerSFT = torch.softmax(gatherValues(visRepNorm, textNorm, len(curstackedTextsTensorList)).squeeze(0) * logitNum, dim=-1)
         if len(innerSFT.shape) == 2:
             innerSFT = innerSFT.squeeze(0)
         maxIndex = innerSFT.argmax()
         curMaxList.append(innerSFT[maxIndex].item())
         annotatedRes = curTextsNames[maxIndex]
         if len(curNextChild) != 0:
             next_taxo_tree = curNextChild[maxIndex]
             if next_taxo_tree["TaxoLevel"] != index2Taxo[annotated_level]:
                 visRepVector = visRepVector.squeeze(0)
                 return taxonomyLabelGreedySearch(
-                    visRepVector, annotatedRes + "@", next_taxo_tree, annotated_level, text2repNormVector, gather_func, logitNum, curMaxList, None
+                    visRepVector, annotatedRes + "@", next_taxo_tree, annotated_level, text2repNormVector, logitNum, curMaxList, None
                 )
             else:
                 return annotatedRes
         else:
             return annotatedRes
 
 
-def taxonomyLabelDFSSearch(
+def taxonomyLabelTopkSearch(
     result: List,
     visRepVector: torch.Tensor,
     cur_anntotated: str,
     taxo_tree: Dict,
     text2repNormVector: Dict,
-    gather_func: Callable,
     logitNum: torch.Tensor,
     phy_fc: Union[None, nn.Module],
     topK: int,
     probs: List[float],
     value=0.0,
     curLevel=0,
 ):
@@ -161,15 +175,15 @@
         curTextsNames = []
         curNextChild = []
         for child in taxo_tree["Children"]:
             curstackedTextsTensorList.append(text2repNormVector[child["Name"]])
             curTextsNames.append(child["Name"])
             curNextChild.append(child)
         textNorm = torch.stack(curstackedTextsTensorList, dim=0).unsqueeze(0)
-        innerSFT = torch.softmax(gather_func(visRepNorm, textNorm, len(curstackedTextsTensorList)).squeeze(0) * logitNum, dim=-1)
+        innerSFT = torch.softmax(gatherValues(visRepNorm, textNorm, len(curstackedTextsTensorList)).squeeze(0) * logitNum, dim=-1)
         if len(innerSFT.shape) == 2:
             innerSFT = innerSFT.squeeze(0)
         innerMaxIndex = innerSFT.argmax()
         phySFT = torch.softmax(phy_fc(visRepNorm), dim=-1)
         if len(phySFT.shape) == 2:
             phySFT = phySFT.squeeze(0)
         phyMaxIndex = phySFT.argmax()
@@ -184,21 +198,20 @@
             nextPairs.append((innerMaxIndex, curTextsNames[innerMaxIndex], curProb, curProb))
         for pair in nextPairs:
             next_taxo_tree = curNextChild[pair[0]]
             annotatedRes = pair[1]
             curProbs = [pair[2]]
             v = pair[3] * bouns[curLevel]
             visRepVector = visRepVector.squeeze(0)
-            taxonomyLabelDFSSearch(
+            taxonomyLabelTopkSearch(
                 result,
                 visRepVector,
                 annotatedRes + "@",
                 next_taxo_tree,
                 text2repNormVector,
-                gather_func,
                 logitNum,
                 None,
                 topK,
                 curProbs,
                 v,
                 curLevel + 1,
             )
@@ -211,15 +224,15 @@
                 curstackedTextsTensorList.append(text2repNormVector[cur_anntotated + child])
                 curTextsNames.append(cur_anntotated + child)
             else:
                 curstackedTextsTensorList.append(text2repNormVector[cur_anntotated + child["Name"]])
                 curTextsNames.append(cur_anntotated + child["Name"])
                 curNextChild.append(child)
         textNorm = torch.stack(curstackedTextsTensorList, dim=0).unsqueeze(0)
-        innerSFT = torch.softmax(gather_func(visRepNorm, textNorm, len(curstackedTextsTensorList)).squeeze(0) * logitNum, dim=-1)
+        innerSFT = torch.softmax(gatherValues(visRepNorm, textNorm, len(curstackedTextsTensorList)).squeeze(0) * logitNum, dim=-1)
         thisTopK = topK
         if curLevel <= 2:
             thisTopK = topK - 1
         if len(innerSFT.shape) == 2:
             innerSFT = innerSFT.squeeze(0)
         if innerSFT.shape[-1] >= thisTopK:
             topValues, topIndices = torch.topk(innerSFT, thisTopK, dim=-1)
@@ -241,21 +254,20 @@
         if len(curNextChild) != 0:
             for pair in nextPairs:
                 next_taxo_tree = curNextChild[pair[0]]
                 annotatedRes = pair[1]
                 curProbs = pair[2]
                 v = pair[3] * bouns[curLevel]
                 visRepVector = visRepVector.squeeze(0)
-                taxonomyLabelDFSSearch(
+                taxonomyLabelTopkSearch(
                     result,
                     visRepVector,
                     annotatedRes + "@",
                     next_taxo_tree,
                     text2repNormVector,
-                    gather_func,
                     logitNum,
                     None,
                     topK,
                     curProbs,
                     v,
                     curLevel + 1,
                 )
@@ -526,44 +538,42 @@
 
 def subProcessLabelGreedySearch(
     inputVectorList: List[torch.Tensor],
     names: List[str],
     taxo_tree: Dict,
     annotated_level: int,
     text2repNormVector: Dict[str, torch.Tensor],
-    func: Callable,
     logitNum: torch.Tensor,
     phy_fc: nn.Module,
 ):
     name2res = {}
     for i, inputVector in enumerate(inputVectorList):
         curMaxList = []
         with torch.no_grad():
             anntotated_res = taxonomyLabelGreedySearch(
-                inputVector, "", taxo_tree, annotated_level, text2repNormVector, func, logitNum, curMaxList, phy_fc
+                inputVector, "", taxo_tree, annotated_level, text2repNormVector, logitNum, curMaxList, phy_fc
             )
         name2res[names[i]] = (anntotated_res, curMaxList)
     return name2res
 
 
-def subProcessLabelDFSSearch(
+def subProcessLabelTopkSearch(
     inputVectorList: List[torch.Tensor],
     names: List[str],
     taxo_tree: Dict,
     text2repNormVector: Dict[str, torch.Tensor],
-    func: Callable,
     logitNum: torch.Tensor,
     phy_fc: nn.Module,
     topK: int,
 ):
     name2res = {}
     with torch.no_grad():
         for i, inputVector in enumerate(inputVectorList):
             result = []
-            taxonomyLabelDFSSearch(result, inputVector, "", taxo_tree, text2repNormVector, func, logitNum, phy_fc, topK, None, 1.0)
+            taxonomyLabelTopkSearch(result, inputVector, "", taxo_tree, text2repNormVector, logitNum, phy_fc, topK, None, 1.0)
             sortedRes = list(sorted(result, key=lambda x: x[2], reverse=True))
             n = len(sortedRes)
             k = n // 4 * 3
             if k == 0:
                 k = n
             result = sortedRes[0:k]
             annotNames = []
@@ -574,15 +584,15 @@
                 annotNames.append(pair[0])
                 annotProbs.append(pair[1])
                 annotScore.append(pair[2])
                 annotTextNormTensors.append(pair[3])
             inputVector = inputVector.unsqueeze(0)
             visRepNorm = inputVector / inputVector.norm(dim=-1, keepdim=True)
             textNorm = torch.stack(annotTextNormTensors, dim=0).unsqueeze(0)
-            innerSFT = torch.softmax(func(visRepNorm, textNorm, len(annotTextNormTensors)).squeeze(0) * logitNum, dim=-1)
+            innerSFT = torch.softmax(gatherValues(visRepNorm, textNorm, len(annotTextNormTensors)).squeeze(0) * logitNum, dim=-1)
             if len(innerSFT.shape) >= 2:
                 innerSFT = innerSFT.squeeze(0)
             annotScore = np.array(annotScore, dtype=np.float32)
             annotScore = torch.softmax(torch.from_numpy(annotScore), dim=-1)
             innerSFT = innerSFT + annotScore
             innerMaxIndex = innerSFT.argmax()
             name2res[names[i]] = (annotNames[innerMaxIndex], annotProbs[innerMaxIndex])
@@ -603,15 +613,15 @@
     annotated_level=6,
     num_cpu=6,
     overlapping_ratio=0.5,
     cutSeqLength=8192,
     th="",
     n="",
     binName="",
-    dfsORgreedy="dfs",
+    topkORgreedy="topk",
     topK=3,
 ):
     pid = str(os.getpid())
     if isinstance(binFasta, str):
         name2seq = readFasta(binFasta)
     elif isinstance(binFasta, dict):
         name2seq = binFasta
@@ -669,40 +679,38 @@
     for i, name in enumerate(names):
         name2contigLen[name] = len(name2seq[name])
     phy_fc = model.phy_fc.to("cpu")
     processList = []
     step = len(names) // num_cpu + 1
     with ThreadPoolExecutor(max_workers=num_cpu) as t:
         for i in range(num_cpu):
-            if dfsORgreedy.lower() == "greedy":
+            if topkORgreedy.lower() == "greedy":
                 p = t.submit(
                     subProcessLabelGreedySearch,
                     visRepVectorList[step * i: step * (i + 1)],
                     names[step * i: step * (i + 1)],
                     taxo_tree,
                     annotated_level,
                     text2repNormVector,
-                    model.gatherValues,
                     logitNum,
                     phy_fc,
                 )
-            elif dfsORgreedy.lower() == "dfs":
+            elif topkORgreedy.lower() == "topk":
                 p = t.submit(
-                    subProcessLabelDFSSearch,
+                    subProcessLabelTopkSearch,
                     visRepVectorList[step * i: step * (i + 1)],
                     names[step * i: step * (i + 1)],
                     taxo_tree,
                     text2repNormVector,
-                    model.gatherValues,
                     logitNum,
                     phy_fc,
                     topK,
                 )
             else:
-                raise ValueError("No Implement Other Searching Algorithms Besides DFS & Greedy Search.")
+                raise ValueError("No Implement Other Searching Algorithms Besides Top-K or Greedy Search.")
             processList.append(p)
         for async_res in as_completed(processList):
             name2res = async_res.result()
             for name, data in name2res.items():
                 name2Labeled[name] = data[0]
                 name2maxList[name] = data[1]
     # Reverse to original
@@ -724,15 +732,15 @@
     annotated_level=6,
     num_cpu=6,
     overlapping_ratio=0.5,
     cutSeqLength=8192,
     th="",
     n="",
     binName="",
-    dfsORgreedy="dfs",
+    topkORgreedy="topk",
     topK=3,
 ):
     name2annotated, name2maxList = labelBinFastaFile(
         inputPath,
         model,
         mer3_vocabulary,
         mer4_vocabulary,
@@ -745,15 +753,15 @@
         annotated_level=annotated_level,
         num_cpu=num_cpu,
         overlapping_ratio=overlapping_ratio,
         cutSeqLength=cutSeqLength,
         th=th,
         n=n,
         binName=binName,
-        dfsORgreedy=dfsORgreedy,
+        topkORgreedy=topkORgreedy,
         topK=topK,
     )
     outputPath = os.path.join(outputFolder, binName + ".txt")
     writeAnnotResult(outputPath, name2annotated, name2maxList)
 
 
 def labelBinsFolder(
@@ -769,15 +777,15 @@
     batch_size=6,
     annotated_level=6,
     bin_suffix="fasta",
     filesList=None,
     num_cpu=6,
     overlapping_ratio=0.5,
     cutSeqLength=8192,
-    dfsORgreedy="dfs",
+    topkORgreedy="topk",
     topK=3,
     error_queue=None,
     model_config=None,
 ):
     try:
         assert bin_suffix.lower() != "txt" or bin_suffix.lower() != ".txt"
         files = os.listdir(inputBinFolder)
@@ -848,15 +856,15 @@
                     annotated_level,
                     num_cpu,
                     overlapping_ratio,
                     cutSeqLength,
                     i,
                     num_files,
                     binName,
-                    dfsORgreedy,
+                    topkORgreedy,
                     topK,
                 )
         error_queue.put(None)
         if c == 0:
             raise ValueError("Can not find any MAGs in this folder. Please check your bin_suffix or MAGs' PATH !!!!")
     except:
         traceback.print_exc()
```

## Deepurify/SelectMAGsTools/SelectionUitls.py

```diff
@@ -2,16 +2,16 @@
 import os
 from copy import deepcopy
 from shutil import copy
 from typing import Dict, List, Tuple
 
 from Deepurify.IOUtils import readBinName2Annot, readCheckMResultAndStat, readPickle
 
-index2Taxo = {1: "phylum_filter", 2: "class_filter", 3: "order_filter", 4: "family_filter", 5: "genus_filter", 6: "species_filter"}
 
+index2Taxo = {1: "T1_filter", 2: "T2_filter", 3: "T3_filter", 4: "T4_filter", 5: "T5_filter", 6: "T6_filter"}
 
 class Tree:
     def __init__(self, binName: str, annotName: str, qualityValues: Tuple[float, float, str], core: List[bool]) -> None:
         self.binName = binName
         self.annotName = annotName
         self.qualityValues = qualityValues
         self.core = core
@@ -22,15 +22,15 @@
     def insert(self, binName: str, annotName: str, qualityValues: Tuple[float, float, str], core: List[bool]):
         node = Tree(binName, annotName, qualityValues, core)
         self.childern.append(node)
         return node
 
 
 def getScore(qualityValues: Tuple[float, float, str]) -> float:
-    score = qualityValues[0] - 5.0 * qualityValues[1]
+    score = qualityValues[0] - qualityValues[1]
     if qualityValues[-1] == "HighQuality":
         score += 200.0
     elif qualityValues[-1] == "MediumQuality":
         score += 100.0
     return score
 
 
@@ -111,15 +111,15 @@
         elif highQNum == 1 and mediumQNum == 0 and tree.qualityValues[-1] == "HighQuality":
             if getScore(tree.qualityValues) > getScore(tree.pathBests[0][0]):
                 tree.pathBests = [(tree.qualityValues, tree)]
 
 
 def getScoreForLowquality(
         qualityValues: Tuple[float, float, str]) -> float:
-    score = qualityValues[0] - 3.0 * qualityValues[1]
+    score = qualityValues[0] - qualityValues[1]
     return score
 
 
 def findCoreOutput(
         node: Tree,
         res: List):
     res.append((getScoreForLowquality(node.qualityValues), node))
@@ -150,32 +150,34 @@
             root.annotName = child.annotName
             break
     outInfo = []
     n = len(root.pathBests)
     _, suffix = os.path.splitext(binFileName)
     # It has medium or high quality bins
     if n > 0:
-        for qualityValues, treeNode in root.pathBests:
+        for k, (qualityValues, treeNode) in enumerate(root.pathBests):
             level = len(treeNode.core) - 1
             curBinName = treeNode.binName
-            outName = curBinName + "_" + str(level) + suffix
+            perfix = curBinName.split("___")[0]
+            outName = perfix + "_" + str(k) + suffix
             outInfo.append((outName, qualityValues, treeNode.annotName))
             if level == 0:
                 copy(os.path.join(inputFileFolder, binFileName), os.path.join(outputPath, outName))
             else:
                 copy(os.path.join(deepurifyFolderPath, "FilterOutput", index2Taxo[level], treeNode.binName + suffix),
                      os.path.join(outputPath, outName))
     # this bin dose not have medium or high quality separated bins.
     else:
         res = []
         findCoreOutput(root, res)
         res = list(sorted(res, key=lambda x: x[0], reverse=True))
         coreLeaf = res[0][1]
         curBinName = coreLeaf.binName
+        perfix = curBinName.split("___")[0]
         l = len(coreLeaf.core) - 1
-        outName = curBinName + "_" + str(l) + suffix
+        outName = perfix + "_0" + suffix
         outInfo.append((outName, coreLeaf.qualityValues, coreLeaf.annotName))
         if l == 0:
             copy(os.path.join(inputFileFolder, binFileName), os.path.join(outputPath, outName))
         else:
             copy(os.path.join(deepurifyFolderPath, "FilterOutput", index2Taxo[l], coreLeaf.binName + suffix), os.path.join(outputPath, outName))
     return outInfo
```

## Comparing `Deepurify-1.1.0.4.dist-info/RECORD` & `Deepurify-1.2.0.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,33 +1,34 @@
 Deepurify/IOUtils.py,sha256=fjc1BRxVjbDCITJFWurwhT1cV_05xe4usC3AYPVPeIo,7545
 Deepurify/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-Deepurify/clean_func.py,sha256=7rYBesdKk8h36-oQHqmYarlDZkLpb5kyXrYg2QzH6eU,10314
-Deepurify/cli.py,sha256=ZE9qLsXZNhglzsrz2Mo9Huh7USMPVJ_LmnVmZm1tKto,10726
-Deepurify/funcs.py,sha256=7d6ZOR5PhpEgDeHlLH5_JeIAKyG-TFm9vkf6NJWSR8s,14498
-Deepurify/CallGenesTools/CallGenesUtils.py,sha256=3LHtECkmv7JBwwk3KXcGORMHImkM4yjoqbmcPLem1zo,4026
+Deepurify/clean_func.py,sha256=EBdABYyO8WJQBG-a_vXV7to6s5lBLJCnn5Ur5rIkna8,13439
+Deepurify/cli.py,sha256=_GLaOX3XuK6GDtbRU8LSlpEUDSua-lzi7l0fmePmVsg,13986
+Deepurify/funcs.py,sha256=-yALbKjdTkopBxObkmflQLAb4-FHgsyBvlQb5jItnbg,14636
+Deepurify/CallGenesTools/CallGenesUtils.py,sha256=Qww0FtZijQF4-X_8nj7mfG5PVV4at5wTmwzIXifc3AU,3872
 Deepurify/CallGenesTools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+Deepurify/CallGenesTools/prodigal.py,sha256=H8WlNV0aOljm0boS_7IqkXBZNMUMfyqdbY3MUmGIrzo,8542
 Deepurify/DataTools/DataUtils.py,sha256=ID6AKtc71qy9BUfXx1cShvkhDv4bfXwxqXY3szHChNc,13167
 Deepurify/DataTools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-Deepurify/FilterBinsTools/FilterUtils.py,sha256=swzYEGJPgCMQd2PA9SbpYB00Dfm7UHM0kzs31Opv1LU,14410
+Deepurify/FilterBinsTools/FilterUtils.py,sha256=g2ESTtsMpe0S3B-w_1jJxyMArXiZuP3S_D-KO1HuTno,15310
 Deepurify/FilterBinsTools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 Deepurify/LabelContigTools/GaussianLearnContigDist.py,sha256=as0_fD574bud_UBauZkDGhfVMm7oMBOgKXhOkZlr4jo,1267
-Deepurify/LabelContigTools/LabelBinUtils.py,sha256=fr3Ub1ylCyGt9lRkOIgyC9cpKPUIPTLte1QQ8JXJIjI,35507
+Deepurify/LabelContigTools/LabelBinUtils.py,sha256=jy726K5K_TZZLqbiRpoKT5jFjPDHkunrNBCChJOVl3I,35846
 Deepurify/LabelContigTools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 Deepurify/Model/Convolutions.py,sha256=8DfDkpV6V6dyFZr1ayXp0h26Bs3NCX2EKeTHZY3-EF0,6665
 Deepurify/Model/Embedding.py,sha256=KVjbnXhmtkeZVm3qj5DnQNasf8NWtKuTBBDA3HWmMdc,404
 Deepurify/Model/EncoderModels.py,sha256=81veXRCU4TslfrZf_PEDASpAHlVqQGbPxP3I5ozpxms,13972
 Deepurify/Model/FormerLayers.py,sha256=XFDCVMFgeIYPbm4oTZbFjvVmUz9KxeLuoqz4JVP8vaI,11469
 Deepurify/Model/Loss.py,sha256=uHFmaz3GKFpsi9euygMogToVhowi6casOwO-fiWLMOU,4568
 Deepurify/Model/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-Deepurify/SelectMAGsTools/SelectionUitls.py,sha256=hzoQqck5V661nVt8pWgD32ZNcmb5U5lCtf3p9Bt3i88,7661
+Deepurify/SelectMAGsTools/SelectionUitls.py,sha256=6btPJo_saTApOdm56yUc5e3WUv9RBpxYHuQOWCNWLYQ,7715
 Deepurify/SelectMAGsTools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 Deepurify/SeqProcessTools/SequenceDataset.py,sha256=m7n6X04zikrCr_jiDVj7uJ1J-MgSklBPsEDibzZRXJU,13115
 Deepurify/SeqProcessTools/SequenceUtils.py,sha256=U27DLiaaIv61UKD15-zOwt3S9q61MAwXqzni65n0t-M,15973
 Deepurify/SeqProcessTools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 Deepurify/TrainTools/Scheduler.py,sha256=tEFqVdN9BysboQh7rMoBVAOtu8X27DeHY7vsWfqRe7c,1451
 Deepurify/TrainTools/TrainUtils.py,sha256=c7Bloj2Upxq_ViwpncVmtCregh5NHqIFUCZmijHL-kM,13962
 Deepurify/TrainTools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-Deepurify-1.1.0.4.dist-info/METADATA,sha256=cnlc6K_1OIa3nnoxqEuhxu9ZiMEnAeKi8u0z_-GVP3Y,434
-Deepurify-1.1.0.4.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-Deepurify-1.1.0.4.dist-info/entry_points.txt,sha256=gEdIWoK91-vl9bePt-ev9A4fejyBpeMu_XeIvWI95gk,48
-Deepurify-1.1.0.4.dist-info/top_level.txt,sha256=rCukIGjPiSmqy0QF2_ycXuZN1WRx4Of2u2r0B7bQbeE,10
-Deepurify-1.1.0.4.dist-info/RECORD,,
+Deepurify-1.2.0.dist-info/METADATA,sha256=TL3wLsLnji9EkAOo-CP-q-QpNxL-rGMcEhXA98aFp-Y,432
+Deepurify-1.2.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+Deepurify-1.2.0.dist-info/entry_points.txt,sha256=gEdIWoK91-vl9bePt-ev9A4fejyBpeMu_XeIvWI95gk,48
+Deepurify-1.2.0.dist-info/top_level.txt,sha256=rCukIGjPiSmqy0QF2_ycXuZN1WRx4Of2u2r0B7bQbeE,10
+Deepurify-1.2.0.dist-info/RECORD,,
```

