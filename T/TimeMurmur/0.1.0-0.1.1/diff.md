# Comparing `tmp/TimeMurmur-0.1.0-py3-none-any.whl.zip` & `tmp/TimeMurmur-0.1.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,28 +1,28 @@
-Zip file size: 26885 bytes, number of entries: 26
--rw-rw-rw-  2.0 fat     3045 b- defN 23-Mar-02 15:08 TimeMurmur/Model.py
--rw-rw-rw-  2.0 fat    17355 b- defN 23-Mar-20 21:08 TimeMurmur/Murmur.py
+Zip file size: 27524 bytes, number of entries: 26
+-rw-rw-rw-  2.0 fat     3191 b- defN 23-Apr-20 15:15 TimeMurmur/Model.py
+-rw-rw-rw-  2.0 fat    21726 b- defN 23-Jun-21 21:38 TimeMurmur/Murmur.py
 -rw-rw-rw-  2.0 fat     3311 b- defN 22-Nov-26 15:26 TimeMurmur/Optimizer.py
 -rw-rw-rw-  2.0 fat       27 b- defN 22-Oct-05 13:30 TimeMurmur/__init__.py
 -rw-rw-rw-  2.0 fat     1785 b- defN 22-Dec-11 16:37 TimeMurmur/basis_functions/FourierBasisFunction.py
--rw-rw-rw-  2.0 fat     2958 b- defN 22-Oct-25 18:32 TimeMurmur/basis_functions/LinearBasisFunction.py
+-rw-rw-rw-  2.0 fat     2958 b- defN 23-Apr-13 17:23 TimeMurmur/basis_functions/LinearBasisFunction.py
 -rw-rw-rw-  2.0 fat       27 b- defN 22-Oct-05 13:30 TimeMurmur/basis_functions/__init__.py
 -rw-rw-rw-  2.0 fat     3295 b- defN 22-Oct-22 15:46 TimeMurmur/builder/BuildArAxis.py
 -rw-rw-rw-  2.0 fat      425 b- defN 22-Oct-05 13:30 TimeMurmur/builder/BuildHierarchy.py
 -rw-rw-rw-  2.0 fat     1218 b- defN 22-Oct-25 01:39 TimeMurmur/builder/BuildIdAxis.py
--rw-rw-rw-  2.0 fat     3388 b- defN 22-Nov-28 21:45 TimeMurmur/builder/BuildPanelAxis.py
+-rw-rw-rw-  2.0 fat     3396 b- defN 23-Apr-12 14:39 TimeMurmur/builder/BuildPanelAxis.py
 -rw-rw-rw-  2.0 fat     7602 b- defN 22-Dec-11 17:17 TimeMurmur/builder/BuildTimeAxis.py
--rw-rw-rw-  2.0 fat    19107 b- defN 23-Mar-16 14:20 TimeMurmur/builder/Builder.py
+-rw-rw-rw-  2.0 fat    19154 b- defN 23-Apr-06 18:22 TimeMurmur/builder/Builder.py
 -rw-rw-rw-  2.0 fat      963 b- defN 22-Oct-30 14:10 TimeMurmur/builder/ExtractFeatures.py
--rw-rw-rw-  2.0 fat     4057 b- defN 23-Mar-16 18:51 TimeMurmur/builder/PreProcess.py
--rw-rw-rw-  2.0 fat     6802 b- defN 23-Mar-20 21:08 TimeMurmur/builder/Transformer.py
+-rw-rw-rw-  2.0 fat     4057 b- defN 23-Apr-06 18:57 TimeMurmur/builder/PreProcess.py
+-rw-rw-rw-  2.0 fat     8422 b- defN 23-Apr-19 15:54 TimeMurmur/builder/Transformer.py
 -rw-rw-rw-  2.0 fat       27 b- defN 22-Oct-05 13:30 TimeMurmur/builder/__init__.py
 -rw-rw-rw-  2.0 fat     9607 b- defN 22-Oct-30 14:45 TimeMurmur/utils/FeatureExtraction.py
 -rw-rw-rw-  2.0 fat       27 b- defN 22-Oct-05 13:30 TimeMurmur/utils/__init__.py
 -rw-rw-rw-  2.0 fat     1184 b- defN 22-Oct-05 13:30 TimeMurmur/utils/series_utils.py
--rw-rw-rw-  2.0 fat     3098 b- defN 22-Dec-11 16:08 TimeMurmur/utils/utility_functions.py
--rw-rw-rw-  2.0 fat     1088 b- defN 23-Mar-20 21:09 TimeMurmur-0.1.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     4691 b- defN 23-Mar-20 21:09 TimeMurmur-0.1.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Mar-20 21:09 TimeMurmur-0.1.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 23-Mar-20 21:09 TimeMurmur-0.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2279 b- defN 23-Mar-20 21:09 TimeMurmur-0.1.0.dist-info/RECORD
-26 files, 97469 bytes uncompressed, 23161 bytes compressed:  76.2%
+-rw-rw-rw-  2.0 fat     3110 b- defN 23-Apr-06 18:18 TimeMurmur/utils/utility_functions.py
+-rw-rw-rw-  2.0 fat     1088 b- defN 23-Jun-21 21:41 TimeMurmur-0.1.1.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     4715 b- defN 23-Jun-21 21:41 TimeMurmur-0.1.1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-21 21:41 TimeMurmur-0.1.1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 23-Jun-21 21:41 TimeMurmur-0.1.1.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     2279 b- defN 23-Jun-21 21:41 TimeMurmur-0.1.1.dist-info/RECORD
+26 files, 103697 bytes uncompressed, 23800 bytes compressed:  77.0%
```

## zipnote {}

```diff
@@ -57,23 +57,23 @@
 
 Filename: TimeMurmur/utils/series_utils.py
 Comment: 
 
 Filename: TimeMurmur/utils/utility_functions.py
 Comment: 
 
-Filename: TimeMurmur-0.1.0.dist-info/LICENSE
+Filename: TimeMurmur-0.1.1.dist-info/LICENSE
 Comment: 
 
-Filename: TimeMurmur-0.1.0.dist-info/METADATA
+Filename: TimeMurmur-0.1.1.dist-info/METADATA
 Comment: 
 
-Filename: TimeMurmur-0.1.0.dist-info/WHEEL
+Filename: TimeMurmur-0.1.1.dist-info/WHEEL
 Comment: 
 
-Filename: TimeMurmur-0.1.0.dist-info/top_level.txt
+Filename: TimeMurmur-0.1.1.dist-info/top_level.txt
 Comment: 
 
-Filename: TimeMurmur-0.1.0.dist-info/RECORD
+Filename: TimeMurmur-0.1.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## TimeMurmur/Model.py

```diff
@@ -1,11 +1,12 @@
 # -*- coding: utf-8 -*-
 import warnings
 import numpy as np
 import lightgbm as gbm
+from catboost import Pool, CatBoostRegressor
 # import optuna.integration.lightgbm as gbm
 warnings.filterwarnings("ignore")
 
 
 class Model:
 
     def __init__(self,
@@ -15,15 +16,16 @@
                  min_child_samples=5,
                  num_leaves=50,
                  num_iterations=50,
                  return_proba=False,
                  boosting_params=None,
                  scale_pos_weights=None,
                  is_unbalance=True,
-                 alpha=None):
+                 alpha=None,
+                 num_threads=1):
         self.objective = objective
         self.return_proba = return_proba
         if boosting_params is None:
             self.boosting_params = {
                                     "objective": objective,
                                     "metric": metric,
                                     # "tweedie_variance_power": 1.7,
@@ -33,15 +35,16 @@
                                     'linear_tree': False,
                                     'learning_rate': learning_rate,
                                     'min_child_samples': min_child_samples,
                                     'num_leaves': num_leaves,
                                     'num_iterations': num_iterations,
                                     'alpha': alpha,
                                     'scale_pos_weights': scale_pos_weights,
-                                    'is_unbalance': is_unbalance
+                                    'is_unbalance': is_unbalance,
+                                    'num_threads': num_threads
                                 }
         else:
             self.boosted_params = boosting_params
             
     def build_model(self):
         if self.objective == 'regression' or self.objective == 'quantile' or self.objective == 'mape':
             model_obj = gbm.LGBMRegressor(**self.boosting_params)
@@ -62,13 +65,14 @@
             validation_set = dataset[dataset[split] == 'Test'].drop(split, axis=1)
             train_set = dataset[dataset[split] == 'Train'].drop(split, axis=1)
             test_X = validation_set.drop('Murmur Target', axis=1)
             test_y = validation_set['Murmur Target']
             eval_set = [(test_X, test_y)]
         train_X = train_set.drop('Murmur Target', axis=1)
         train_y = train_set['Murmur Target']
+
         del train_set
         if validation_set is not None:
             del validation_set
         return train_X, train_y, eval_set
```

## TimeMurmur/Murmur.py

```diff
@@ -59,20 +59,23 @@
             labels=None,
             floor_bind=False,
             scale_type='standard',
             basis_difference=False,
             linear_test_window=None,
             seasonal_dummy=False,
             outlier_cap=None,
-            ts_features=False
+            ts_features=False,
+            sample_weights=None,
+            num_threads=1
             ):
         self.difference = difference
         self.scale = scale
         self.linear_trend = linear_trend
         self.id_column = id_column
+        self.date_column = date_column
         self.target_column = target_column
         if freq == 'auto':
             dates = df[date_column].drop_duplicates().sort_values()
             freq = infer_freq(dates)
         self.builder = Builder(target_column=target_column,
                           id_column=id_column,
                           date_column=date_column,
@@ -109,15 +112,15 @@
                 id_feature_columns = [i for i in id_feature_columns if i != id_column]
                 process_dataset = process_dataset.drop(id_feature_columns, axis=1)
 
         else:
             id_dataset = None
         time_dataset = self.builder.build_time_axis(df,
                                                time_exogenous=time_exogenous)
-        dataset = self.builder.build_dataset(process_dataset,
+        self.dataset = self.builder.build_dataset(process_dataset,
                                         id_axis=id_dataset,
                                         time_axis=time_dataset)
         drop_columns = [id_column, date_column, target_column]
         self.run_dict = self.builder.run_dict
         
         model = Model(objective=objective,
                       metric=metric,
@@ -125,93 +128,96 @@
                       min_child_samples=min_child_samples,
                       num_leaves=num_leaves,
                       num_iterations=num_iterations,
                       return_proba=return_proba,
                       boosting_params=boosting_params,
                       scale_pos_weights=scale_pos_weights,
                       is_unbalance=is_unbalance,
-                      alpha=alpha)
+                      alpha=alpha,
+                      num_threads=num_threads)
         self.model_obj = model.build_model()
         self.run_dict['global']['model'] = model.boosting_params
         if categorical_columns is None:
             cat_features = ['Murmur ID']
         else:
             cat_features = list(set(categorical_columns + ['Murmur ID']))
-        if 'murmur_seasonal_dummy' in dataset.columns:
+        if 'murmur_seasonal_dummy' in self.dataset.columns:
             cat_features += ['murmur_seasonal_dummy']
         self.run_dict['global']['cat_features'] = cat_features
-        dataset = dataset.sort_values(by=['Murmur ID', date_column])
-        self.train_X = dataset.drop(drop_columns+['Murmur Target', 'Murmur Data Split'], axis=1)
+        self.dataset = self.dataset.sort_values(by=['Murmur ID', date_column])
+        self.train_X = self.dataset.drop(drop_columns+['Murmur Target', 'Murmur Data Split'], axis=1)
         eval_set = []
-        train_y = dataset['Murmur Target']
+        train_y = self.dataset['Murmur Target']
         if labels is not None:
             train_y = labels
         if not eval_set:
             early_stopping_rounds = None
         self.columns = self.train_X.columns
         # self.train_X = dataset
         self.train_y = train_y
         self.model_obj.fit(self.train_X,
                            train_y, 
                             eval_set=eval_set,
                             early_stopping_rounds=early_stopping_rounds, 
-                           categorical_feature=cat_features)
+                           categorical_feature=cat_features,
+                           sample_weight=sample_weights)
         fitted = self.model_obj.predict(self.train_X)
-        fitted_df = dataset[['Murmur ID', id_column, date_column, target_column, 'Murmur Target']]
+        fitted_df = self.dataset[['Murmur ID', id_column, date_column, target_column, 'Murmur Target']]
         fitted_df['Predictions'] = fitted
         if self.scale or self.difference or self.linear_trend:
             fitted_df = self.unscale(fitted_df)
         if self.floor is not None:
             fitted_df['Predictions'] = fitted_df['Predictions'].clip(lower=self.floor)
         return fitted_df
     
     def ar_predict(self, forecast_horizon, pred_X):
         date_column = self.run_dict['global']['Date Column']
         id_column = 'Murmur ID'
         final_predicted = []
-        self.look = []
+        self.pred_X = []
         print('Running Recursive Predictions')
         for i in tqdm(range(forecast_horizon)):
             refined_pred_X = pred_X.groupby('Murmur ID').nth(i)
             for _, dataset in self.run_dict['global']['AR Datasets'].items(): 
                 refined_pred_X = refined_pred_X.merge(dataset,
                                                       on=['Murmur ID',
                                                           date_column],
                                                       how='left')
             self.refined_pred_X = refined_pred_X
-            self.look.append(refined_pred_X)
+            self.pred_X.append(refined_pred_X)
             model_X = refined_pred_X.drop([date_column], axis=1)
             predicted = self.model_obj.predict(model_X[list(self.columns)])
             predicted = pd.DataFrame(predicted, columns=['Predictions'])
             predicted[id_column] = refined_pred_X[id_column].values
             predicted[date_column] = refined_pred_X[date_column].values
             self.builder.build_future_ar_axis(predicted)
             final_predicted.append(predicted)
+        self.pred_X = pd.concat(self.pred_X)
         return pd.concat(final_predicted)
     
     def predict(self, 
                 forecast_horizon,
                 time_exogenous=None,
                 panel_exogenous=None,
                 predict_proba=False
                 ):
         date_column = self.run_dict['global']['Date Column']
         id_column = self.run_dict['global']['ID Column']
         drop_columns = [date_column]
         pred_X = self.builder.build_future_dataset(forecast_horizon,
                                                    time_exogenous=time_exogenous,
                                                    panel_exogenous=panel_exogenous)
-        self.pred_X = pred_X
         if self.run_dict['global']['AR Datasets'] is not None:
             predicted_df = self.ar_predict(forecast_horizon,
                                         pred_X)
             predicted_df = predicted_df.merge(self.run_dict['global']['ID Mapping'],
                                               on='Murmur ID',
                                               how='left')
         else:
+            self.pred_X = pred_X
             if predict_proba:
                 predicted = self.model_obj.predict_proba(pred_X[list(self.columns)])
                 predicted = predicted[:, 0]
             else:
                 predicted = self.model_obj.predict(pred_X[list(self.columns)])
             predicted_df = pred_X[['Murmur ID', date_column]]
             predicted_df = predicted_df.merge(self.run_dict['global']['ID Mapping'],
@@ -224,153 +230,225 @@
             predicted_df['Predictions'] = predicted_df['Predictions'].clip(lower=self.floor)
         print('All Done!')
         return predicted_df
 
     def inverse_transform(self, df):
         scaler = self.run_dict['local'][df['Murmur ID'].iloc[0]]['scaler']
         df['Unscaled Predictions'] = df['Predictions']
+        linear_trend = self.run_dict['local'][df['Murmur ID'].iloc[0]]['trend']
         if self.target_column in df.columns:
-            # if self.scale:
-            #     actuals = df['Murmur Target'].values
-            # else:
-            actuals = df[self.target_column].values
+            if linear_trend:
+                actuals = df['Murmur Target'].values
+                linear = True
+            else:
+                actuals = df[self.target_column].values
+                linear = False
             preds = scaler.inverse_transform(df['Predictions'].values.reshape(-1,1),
-                                             actuals=actuals)
+                                             actuals=actuals,
+                                             linear=linear)
         else:
             preds = scaler.inverse_transform(df['Predictions'].values.reshape(-1,1))
         if any(np.isnan(preds)):
-            raise ValueError('Nan found when Inverse Transforming, use a more stable transformer such as "standard"')
+            print('Nan found when Inverse Transforming, use a more stable transformer such as "standard"')
         df['Predictions'] = preds
         return df
     
     def unscale(self, forecast_df):
         return forecast_df.groupby('Murmur ID').apply(self.inverse_transform)
 
-    def Explain(self):
-        explainer = shap.TreeExplainer(self.model_obj)
-        shap_values = explainer.shap_values(self.train_X)
-        return explainer, shap_values
-
-    # def Optimize(cls, y, seasonality, n_folds, test_size=None):
-    #     optimizer = Optimize(y, Murmur, seasonality, n_folds, test_size)
-    #     optimized = optimizer.fit()
-    #     optimized['ar'] = list(range(1, int(optimized['ar']) + 1))
-    #     optimized['n_estimators'] = int(optimized['n_estimators'])
-    #     optimized['num_leaves'] = int(optimized['num_leaves'])
-    #     optimized['n_basis'] = int(optimized['n_basis'])
-    #     optimized['fourier_order'] = int(optimized['fourier_order'])
-    #     return optimized
-
-    def Evaluate(self,
-                 fitted,
-                 predicted,
-                 test_df,
-                 dropna=False):
-        id_column = self.run_dict['global']['ID Column']
+    def explain_predictions(self, predicted_df):
+        pred_X = self.pred_X[self.train_X.columns]
+        self.explainer = shap.TreeExplainer(self.model_obj)
+        shap_values = self.explainer.shap_values(pred_X)
+        shap_values = pd.DataFrame(shap_values, columns=pred_X.columns)
+    
+        shap_values = shap_values.rename({'Murmur ID': 'Level'}, axis=1)
+    
+        shap_values[self.date_column] = self.pred_X[self.date_column].values
+        shap_values['Murmur ID'] = self.pred_X['Murmur ID'].values
+        shap_values = shap_values.merge(self.run_dict['global']['ID Mapping'],
+                                        on='Murmur ID')
+        # shap_values[self.date_column] = shap_values[self.date_column].dt.date
+        shap_values['Predictions'] = predicted_df['Predictions']
+
+        scale = (shap_values['Predictions'].values / shap_values.iloc[:, :-4].sum(axis=1).values)
+        shap_values.iloc[:, :-4] = shap_values.iloc[:, :-4].mul(scale, axis=0)
+        def rename_column(column_name, tags=None):
+            if 'basis' in column_name:
+                return 'Trend'
+            if 'calc' in column_name:
+                return 'TimeSeriesFeatures'
+            if 'fourier' in column_name:
+                return 'Seasonality'
+            if 'ar_' in column_name:
+                return 'LaggedData'
+            if tags is not None:
+                matching = [s for s in tags if column_name.split('_')[0] in s]
+                if matching:
+                    return matching
+            return column_name
+        shap_values.columns = [rename_column(i) for i in shap_values.columns]
+        shap_values = shap_values.groupby(level=0, axis=1).sum()
+        return shap_values
+
+    def explain_fitted(self, fitted_df):
+        pred_X = self.train_X
+        self.explainer = shap.TreeExplainer(self.model_obj)
+        shap_values = self.explainer.shap_values(pred_X)
+        shap_values = pd.DataFrame(shap_values, columns=pred_X.columns)
+    
+        shap_values = shap_values.rename({'Murmur ID': 'Level'}, axis=1)
+        shap_values[self.date_column] = self.dataset[self.date_column].values
+        shap_values['Murmur ID'] = self.dataset['Murmur ID'].values
+        shap_values = shap_values.merge(self.run_dict['global']['ID Mapping'],
+                                        on='Murmur ID')
+        # shap_values[self.date_column] = shap_values[self.date_column].dt.date
+        shap_values['Predictions'] = fitted_df['Predictions'].values
+
+        scale = (shap_values['Predictions'].values / shap_values.iloc[:, :-4].sum(axis=1).values)
+        shap_values.iloc[:, :-4] = shap_values.iloc[:, :-4].mul(scale, axis=0)
+        def rename_column(column_name, tags=None):
+            if 'basis' in column_name:
+                return 'Trend'
+            if 'calc' in column_name:
+                return 'TimeSeriesFeatures'
+            if 'fourier' in column_name:
+                return 'Seasonality'
+            if 'ar_' in column_name:
+                return 'LaggedData'
+            if tags is not None:
+                matching = [s for s in tags if column_name.split('_')[0] in s]
+                if matching:
+                    return matching
+            return column_name
+        shap_values.columns = [rename_column(i) for i in shap_values.columns]
+        shap_values = shap_values.groupby(level=0, axis=1).sum()
+        return shap_values
+
+    def plot_explanations(self,
+                          ts_id=None,
+                          murmur_id=None,
+                          level=None,
+                          predicted_shap_vals=None,
+                          fitted_shap_vals=None):
+        if (murmur_id is None and ts_id is None) and level is None:
+            raise ValueError('Must pass a level or time series ID')
+        if (fitted_shap_vals is None and predicted_shap_vals is None):
+            raise ValueError('Must pass a fitted or predicted shap val DF')
         date_column = self.run_dict['global']['Date Column']
+        if murmur_id is not None:
+            id_column = 'Murmur ID'
+            ts_id = murmur_id
+        else:
+            id_column = self.run_dict['global']['ID Column']
         target_column = self.run_dict['global']['Target Column']
-        seasonal_period = self.run_dict['global']['main_seasonal_period']
-        merge_on = [id_column, date_column]
-        predicted = predicted.merge(test_df[merge_on + [target_column]],
-                                    on=merge_on)
-        grouped_df = predicted.groupby(id_column)
-        def grouped_smape(df):
-            return smape(df[target_column], df['Predictions'])
-        def grouped_mse(df):
-            return mse(df[target_column], df['Predictions'])
-        def grouped_mape(df):
-            return mape(df[target_column], df['Predictions'])
-        mape_df = grouped_df.apply(grouped_mape)
-        feature_df = get_features(fitted,
-                                  id_column,
-                                  target_column,
-                                  seasonal_period,
-                                  scale_data=False)
-        print(f'Mean SMAPE: {np.mean(grouped_df.apply(grouped_smape))}')
-        print(f'Mean MAPE: {100*np.mean(mape_df)}')
-        print(f'Mean MSE: {np.mean(grouped_df.apply(grouped_mse))}')
-        feature_df = feature_df.merge(mape_df.reset_index(),
-                                      on=id_column)
-        feature_df = feature_df.drop(id_column, axis=1)
-        import statsmodels.api as sm
-        feature_df = sm.add_constant(feature_df, prepend=False)
-        if dropna:
-            feature_df = feature_df.dropna()
-        mod = sm.OLS(feature_df[0], feature_df.drop(0, axis=1))
-        res = mod.fit()
-        print(res.summary())
-        return
+        if predicted_shap_vals is not None:
+            plot_cols = [i for i in predicted_shap_vals.columns if i not in ['Predictions',
+                                                                              'Murmur ID',
+                                                                              self.id_column,
+                                                                              self.date_column]]
+        else:
+            plot_cols = [i for i in fitted_shap_vals.columns if i not in ['Predictions',
+                                                                              'Murmur ID',
+                                                                              self.id_column,
+                                                                              self.date_column]]
+        if level == 'all':
+            if fitted_shap_vals is not None:
+                fitted_shap_vals = fitted_shap_vals.groupby(self.date_column)[plot_cols].sum().reset_index()
+            if predicted_shap_vals is not None:
+                predicted_shap_vals = predicted_shap_vals.groupby(self.date_column)[plot_cols].sum().reset_index()
+        else:
+            if fitted_shap_vals is not None:
+                fitted_shap_vals = fitted_shap_vals[fitted_shap_vals[id_column] == ts_id]
+            if predicted_shap_vals is not None:
+                predicted_shap_vals = predicted_shap_vals[predicted_shap_vals[id_column] == ts_id]
+        if predicted_shap_vals is not None and fitted_shap_vals is not None:
+            vals = pd.concat([fitted_shap_vals, predicted_shap_vals])
+            vals = vals.set_index(self.date_column)[plot_cols]
+            vals.plot(kind='bar', stacked=True)
+        elif fitted_shap_vals is not None:
+            fitted_shap_vals = fitted_shap_vals.set_index(self.date_column)[plot_cols]
+            self.fitted_shap_vals = fitted_shap_vals
+            fitted_shap_vals.plot(kind='bar', stacked=True)
+        elif predicted_shap_vals is not None:
+            predicted_shap_vals = predicted_shap_vals.set_index(self.date_column)[plot_cols]
+            predicted_shap_vals.plot(kind='bar', stacked=True)
 
     def plot(self,
              fitted,
              ts_id=None,
              murmur_id=None,
              level=None,
              predicted=None,
              upper_fitted=None,
              upper_predicted=None,
              lower_fitted=None,
              lower_predicted=None):
+        fig, ax = plt.subplots()
         if (murmur_id is None and ts_id is None) and level is None:
             raise ValueError('Must pass a level or time series ID')
         date_column = self.run_dict['global']['Date Column']
         if murmur_id is not None:
             id_column = 'Murmur ID'
             ts_id = murmur_id
         else:
             id_column = self.run_dict['global']['ID Column']
         target_column = self.run_dict['global']['Target Column']
         if level == 'all':
             refined_df = fitted.groupby(date_column)[[target_column,'Predictions']].sum().reset_index()
+
         else:
             refined_df = fitted[fitted[id_column] == ts_id]
-        plt.plot(refined_df[date_column],
-                 refined_df['Predictions'],  
-                 color='lightseagreen',
-                 label='Fitted')
+
+        ax.plot(refined_df[date_column],
+                  refined_df['Predictions'],  
+                  color='lightseagreen',
+                  label='Fitted')
         if predicted is not None:
             if level is not None:
                 predicted = predicted.groupby(date_column)[['Predictions']].sum().reset_index()
             else:
                 predicted = predicted[predicted[id_column] == ts_id]
-            plt.plot(predicted[date_column],
-                     predicted['Predictions'],
-                     color='lightseagreen',
-                     linestyle='dashed',
-                     label='Predicted')
-        plt.plot(refined_df[date_column],
+            ax.plot(predicted[date_column],
+                      predicted['Predictions'],
+                      color='lightseagreen',
+                      linestyle='dashed',
+                      label='Predicted')
+        ax.plot(refined_df[date_column],
                   refined_df[target_column],
                   color='navy',
                   label='Target')
         if upper_fitted is not None and lower_fitted is not None:
             if level is not None:
                 upper_df = upper_fitted.groupby(date_column)[[target_column,'Predictions']].sum().reset_index()
                 lower_df = lower_fitted.groupby(date_column)[[target_column,'Predictions']].sum().reset_index()
             else:
                 upper_df = upper_fitted[upper_fitted[id_column] == ts_id]
                 lower_df = lower_fitted[lower_fitted[id_column] == ts_id]
-            plt.fill_between(x=refined_df[date_column],
-                             y1=upper_df['Predictions'],
-                             y2=lower_df['Predictions'],
-                             color='lightseagreen',
-                             alpha=.1)
+            ax.fill_between(x=refined_df[date_column],
+                              y1=upper_df['Predictions'],
+                              y2=lower_df['Predictions'],
+                              color='lightseagreen',
+                              alpha=.1)
         if upper_predicted is not None and lower_predicted is not None:
             if level is not None:
                 upper_df = upper_predicted.groupby(date_column)[['Predictions']].sum().reset_index()
                 lower_df = lower_predicted.groupby(date_column)[['Predictions']].sum().reset_index()
             else:
                 upper_df = upper_predicted[upper_predicted[id_column] == ts_id]
                 lower_df = lower_predicted[lower_predicted[id_column] == ts_id]
-            plt.fill_between(x=upper_df[date_column],
-                             y1=upper_df['Predictions'],
-                             y2=lower_df['Predictions'],
-                             color='lightseagreen',
-                             linestyle='dashed',
-                             alpha=.25)
-        if ts_id is not None:
-            plt.title = ts_id
+            ax.fill_between(x=upper_df[date_column],
+                              y1=upper_df['Predictions'],
+                              y2=lower_df['Predictions'],
+                              color='lightseagreen',
+                              linestyle='dashed',
+                              alpha=.25)
+
         plt.legend()
+
         plt.show()
 
     def plot_importance(self, **kwargs):
         import lightgbm as gbm
         gbm.plot_importance(self.model_obj, **kwargs)
+
+
```

## TimeMurmur/builder/BuildPanelAxis.py

```diff
@@ -21,15 +21,15 @@
         self.n_basis = n_basis
         self.decay = decay
         self.basis_difference = basis_difference
         self.weighted = weighted
     
     def get_piecewise(self, y, n_basis, ts_id):
         if n_basis >= len(y):
-            n_basis = len(y) - 1
+            n_basis = max(1, len(y) - 1)
         lbf = LinearBasisFunction(n_changepoints=n_basis,
                                   decay=self.decay,
                                   weighted=self.weighted,
                                   basis_difference=self.basis_difference)
         basis = lbf.get_basis(y)
         self.run_dict['local'][ts_id][f'{n_basis}_function'] = lbf
         self.run_dict['local'][ts_id][f'{n_basis}_basis'] = basis
```

## TimeMurmur/builder/Builder.py

```diff
@@ -70,15 +70,15 @@
         self.floor_bind = floor_bind
         self.floor = floor
         self.outlier_cap = outlier_cap
         self.ts_features = ts_features
         self.run_dict = {}
         self.run_dict['local'] = {}
         self.run_dict['global'] = {}
-        self.run_dict['global']['last date'] = []
+        self.run_dict['global']['last date'] = {}
         self.run_dict['global']['categorical_encoder'] = {}
         self.run_dict['global']['IDs with Trend'] = []
         if self.seasonal_period is not None:
             self.run_dict['global']['main_seasonal_period'] = self.seasonal_period[0]
         else:
             self.run_dict['global']['main_seasonal_period'] = 0
         self.run_dict['global']['id_axis'] = None
@@ -225,15 +225,16 @@
     def build_future_ar_axis(self, predicted_dataset):
         ar_builder = ArAxis(self.run_dict,
                             self.freq)
         ar_builder.build_future_axis(predicted_dataset, self.ar)
         return
         
     def build_panel_axis(self, dataset):
-        self.run_dict['global']['last date'].append(dataset[self.date_column].max())
+        ts_id = dataset['Murmur ID'].iloc[0]
+        self.run_dict['global']['last date'][ts_id] = dataset[self.date_column].max()
         panel_builder = PanelAxis(run_dict=self.run_dict,
                                   n_basis=self.n_basis,
                                   decay=self.decay,
                                   weighted=self.weighted,
                                   seasonal_period=self.seasonal_period,
                                   basis_difference=self.basis_difference)
         panel_axis = panel_builder.build_axis(dataset)
```

## TimeMurmur/builder/Transformer.py

```diff
@@ -1,9 +1,12 @@
 # -*- coding: utf-8 -*-
 import numpy as np
+import pandas as pd
+import matplotlib.pyplot as plt
+from statsmodels.tsa.stattools import kpss
 from scipy import stats, special
 from sklearn.preprocessing import (StandardScaler, MinMaxScaler, RobustScaler,
                                    PowerTransformer, MaxAbsScaler, QuantileTransformer)
 
 
 class log:
 
@@ -17,14 +20,40 @@
         transformed = np.log(y)
         return transformed
 
     def inverse_transform(self, y):
         transformed = np.exp(y)
         return transformed
 
+class RobustBoxCox:
+    def __init__(self):
+        self.minmax = None
+        self.boxcox = None
+
+    def fit(self, y):
+        self.minmax = MinMaxScaler(feature_range=(1, 100))
+        self.minmax.fit(y.reshape(-1,1))
+        trans_y = self.minmax.transform(y.reshape(-1,1))
+        self.boxcox = PowerTransformer()
+        self.boxcox.fit(trans_y.reshape(-1,1))
+
+    def transform(self, y):
+        y_copy = y.copy()
+        trans_y = self.minmax.transform(y_copy.reshape(-1,1))
+        transformed = self.boxcox.transform(trans_y.reshape(-1,1))
+        return transformed
+
+    def inverse_transform(self, y):
+        y_copy = y.copy().reshape(-1,1)
+        inv_transf = self.minmax.inverse_transform(self.boxcox.inverse_transform(y_copy))
+        if not np.isfinite(inv_transf.reshape(-1)).all():
+            inds = np.where(np.isnan(inv_transf))
+            inv_transf[inds] = np.nanmean(inv_transf)
+        return inv_transf
+
 
 class MurmurScaler:
 
     def __init__(self,
                  scaler,
                  scale,
                  difference,
@@ -45,28 +74,29 @@
         self.predict = None
         factory_mapping = {'standard': StandardScaler(),
                            'minmax': MinMaxScaler(),
                            'maxabs': MaxAbsScaler(),
                            'robust': RobustScaler(),
                            'quantile': QuantileTransformer(),
                            'boxcox': PowerTransformer(method='box-cox'),
-                           'log': log()
+                           'log': log(),
+                           'robust_boxcox': RobustBoxCox()
                             }
         self.transformer = factory_mapping[self.scaler]
 
     def fit(self, y):
         if self.scale:
             self.transformer.fit(y)
 
     def get_deterministic_trend(self, y):
         trend_line, self.linear, slope, intercept, penalty = self.linear_test(y)
-        if self.linear or self.linear_trend==True:
+        if self.linear:
             self.run_dict['global']['IDs with Trend'].append(self.ts_id)
             series_level = np.mean(trend_line)
-            trend_line = trend_line# - series_level
+            trend_line = trend_line - series_level
             y = np.subtract(y.reshape((-1,)),
                             trend_line)
             self.run_dict['local'][self.ts_id]['trend']['trend_line'] = trend_line
             self.run_dict['local'][self.ts_id]['trend']['slope'] = slope
             self.run_dict['local'][self.ts_id]['trend']['intercept'] = trend_line[0]
             self.run_dict['local'][self.ts_id]['trend']['penalty'] = penalty
             self.run_dict['local'][self.ts_id]['trend']['series_level'] = series_level
@@ -74,19 +104,22 @@
 
     def linear_test(self, y):
         y = y
         xi = np.arange(1, len(y) + 1)
         # xi = xi**2
         slope, intercept, r_value, p_value, std_err = stats.linregress(xi,y.reshape(-1, ))
         trend_line = slope*xi + intercept
+        if self.linear_trend is True:
+            linear = True
+            return trend_line, linear, slope, intercept, r_value
         if self.seasonal_period is not None:
             required_len = 1.5 * max(self.seasonal_period)
         else:
             required_len = 6
-        if self.linear_trend and len(y) > required_len:
+        if self.linear_trend == 'auto' and len(y) > required_len:
             if self.linear_test_window is not None:
                 n_bins = self.linear_test_window
             else:
                 n_bins = (1 + len(y)**(1/3) * 2)
             splitted_array = np.array_split(y.reshape(-1,), int(n_bins))
             mean_splits = np.array([np.mean(i) for i in splitted_array])
             asc_array = np.sort(mean_splits)
@@ -105,55 +138,64 @@
             linear = False
         # slope = slope * r_value
         return trend_line, linear, slope, intercept, r_value
 
     def transform(self, y):
         if self.linear_trend:
             y = self.get_deterministic_trend(y)
-        else:
-            self.run_dict['local'][self.ts_id]['trend']['trend_line'] = None
         if self.scale:
             y = self.transformer.transform(y.reshape(-1, 1))
+        if self.difference == 'auto':
+            if kpss(y, nlags=1)[1] < .1:
+                self.difference = True
+            else:
+                self.difference = False
         if self.difference is not None and self.difference:
             self.run_dict['local'][self.ts_id]['undifference'] = y[0]
             self.run_dict['local'][self.ts_id]['last_y'] = y[-1]
             y = np.diff(y, n=1, axis=0)
             y = np.append(0, y)
         return y
 
     def retrend_predicted(self, y):
         slope = self.run_dict['local'][self.ts_id]['trend']['slope']
         intercept = self.run_dict['local'][self.ts_id]['trend']['intercept']
         penalty = self.run_dict['local'][self.ts_id]['trend']['penalty']
         fit_trend = self.run_dict['local'][self.ts_id]['trend']['trend_line']
         n = len(fit_trend)
-        # series_level = self.run_dict['local'][ts_id]['trend']['series_level']
         linear_trend = [i for i in range(0, len(y))]
         linear_trend = np.reshape(linear_trend, (len(linear_trend), 1))
         linear_trend += n + 1
-        linear_trend = np.multiply(linear_trend, slope*penalty) + intercept
-        linear_trend = linear_trend# - series_level
+        linear_trend = np.multiply(linear_trend, slope) + intercept
         y = np.add(y.reshape(-1), np.reshape(linear_trend, (-1,)))
         return y
 
     def retrend_fitted(self, y):
         trend = self.run_dict['local'][self.ts_id]['trend']['trend_line']
         y = np.add(y.reshape(-1), trend)
         return y
 
     def inverse_transform(self, y, **kwargs):
         if self.difference is not None and self.difference:
             if self.predict is None:
                 actuals = kwargs['actuals']
-                # actuals = actuals + self.run_dict['local'][self.ts_id]['undifference']
+                if kwargs['linear'] and self.scale:
+                    actuals = self.transformer.inverse_transform(actuals.reshape((-1,1)))
+                # undiff = self.run_dict['local'][self.ts_id]['undifference']
+                # undiff = self.transformer.inverse_transform(undiff.reshape((-1,1)))
+                # print(actuals)
+                # actuals = actuals + float(undiff)
+                # print(y, kwargs['actuals'])
                 if self.scale:
                     y = self.transformer.inverse_transform(y.reshape((-1,1)))
-                y = np.append(actuals[0], actuals)[:-1] + y.reshape((-1, ))
                 # y_0 = self.run_dict['local'][self.ts_id]['undifference']
-                # y = np.r_[y_0, y[1:].reshape((-1,))].cumsum()
+                # y = np.r_[y_0, y.reshape((-1,))].cumsum()[1:]
+                y = np.append(actuals[0], actuals)[:-1] + y.reshape((-1, ))
+                if self.scale:
+                    y = y - (y[0] - float(actuals[0]))
             else:
                 y_0 = self.run_dict['local'][self.ts_id]['last_y']
                 y = np.r_[y_0, y.reshape((-1,))].cumsum()[1:]
                 if self.scale:
                     y = self.transformer.inverse_transform(y.reshape((-1,1)))
                 # y_0 = y[-1]
         elif self.scale:
```

## TimeMurmur/utils/utility_functions.py

```diff
@@ -17,15 +17,15 @@
     last_date = dates.iloc[-1]
     future_index = pd.date_range(last_date,
                                  periods=forecast_horizon + 1,
                                  freq=freq)[1:]
     return future_index
 
 def future_index(dataset, run_dict, freq, forecast_horizon):
-    ts_id = dataset.index[0]
+    ts_id = dataset['Murmur ID'].iloc[0]
     date_column = run_dict['global']['Date Column']
     last_date = run_dict['global']['last date'][ts_id]
     future_index = pd.date_range(last_date,
                                  periods=forecast_horizon + 1,
                                  freq=freq)[1:]
     future_df = pd.DataFrame(future_index, columns=[date_column])
     future_df['Murmur ID'] = ts_id
```

## Comparing `TimeMurmur-0.1.0.dist-info/LICENSE` & `TimeMurmur-0.1.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `TimeMurmur-0.1.0.dist-info/METADATA` & `TimeMurmur-0.1.1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: TimeMurmur
-Version: 0.1.0
+Version: 0.1.1
 Summary: Time series forecasting at scale with LightGBM
 Author: Tyler Blume
 Author-email: tblume@mail.USF.edu
 Keywords: forecasting,time series,lightgbm
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
@@ -14,14 +14,15 @@
 Requires-Dist: pandas
 Requires-Dist: statsmodels
 Requires-Dist: scikit-learn
 Requires-Dist: optuna
 Requires-Dist: scipy
 Requires-Dist: matplotlib
 Requires-Dist: lightgbm
+Requires-Dist: catboost
 Requires-Dist: thymeboost
 Requires-Dist: shap
 
 # TimeMurmur
 Requires the forecast period is the same for all time series.
 ## Quickstart
 ```
```

## Comparing `TimeMurmur-0.1.0.dist-info/RECORD` & `TimeMurmur-0.1.1.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,26 @@
-TimeMurmur/Model.py,sha256=5T9CAG9mjsUmVV29tF-m2FdkIASwrVMVeA5JKb8H-ow,3045
-TimeMurmur/Murmur.py,sha256=b7pBYyfYYtifZvjA8eJTlCTTe_ihp1TmN0ZpIKIMfI4,17355
+TimeMurmur/Model.py,sha256=fYwjBBDjsBsLoQj1XOmjXAVCDl1hLdWuR63pVp0M_VA,3191
+TimeMurmur/Murmur.py,sha256=5vkaunDpm00SIlfO6glhHBVCZFbOpNDWCfJz8BlWzLE,21726
 TimeMurmur/Optimizer.py,sha256=7sicRO2bVeLUCDX7ba8dhR7aNLcpgOwE1HHm6hRTNlQ,3311
 TimeMurmur/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
 TimeMurmur/basis_functions/FourierBasisFunction.py,sha256=Z5ONw7IN_HB6MnyEqm6aDLOnfMy4E72GTEr5yM7BAAA,1785
 TimeMurmur/basis_functions/LinearBasisFunction.py,sha256=A5NIV3kcRNm-rUIwcIPCtYPvtkgmHW4tw1xhsGIPmgw,2958
 TimeMurmur/basis_functions/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
 TimeMurmur/builder/BuildArAxis.py,sha256=2T8G8VqISOnl8sZ5TFqsrIgxqSjv-U1EUWW1WL9Wn2E,3295
 TimeMurmur/builder/BuildHierarchy.py,sha256=hvJfawGP3HsKI1527EMQp70uIkdE8K9FdTFWOxtsNGo,425
 TimeMurmur/builder/BuildIdAxis.py,sha256=MVaqd7KPu6HH9E-FTP8MLYQCKAvg_3exi0rxXahfw9o,1218
-TimeMurmur/builder/BuildPanelAxis.py,sha256=hz02k_aj9GNHQqQC3pX4M0aS9ZdDrYCQaGvPV0yeF3U,3388
+TimeMurmur/builder/BuildPanelAxis.py,sha256=shsydQziaZfoMUFqmwZSZCPtrISgP8NnIIooj4f8RTg,3396
 TimeMurmur/builder/BuildTimeAxis.py,sha256=1OgdCMeqr52i57eHRkt8c3l3L968VBptIzal0tbm_O0,7602
-TimeMurmur/builder/Builder.py,sha256=MAgXTebJGLiUK83uTptPOa-C1WD8Tt4rYFnggIL1O8Q,19107
+TimeMurmur/builder/Builder.py,sha256=a-3_-TzF_U0RdZ-OeWZE9LRQgfR_XM5XXvvtlWn9pg4,19154
 TimeMurmur/builder/ExtractFeatures.py,sha256=3ff_q5Ody5WPq40VPb56oJpxX1SPo6zDVqNTippOUiU,963
 TimeMurmur/builder/PreProcess.py,sha256=Epx9xiDAvvMYqb7qtrovYUB7f8u9bdsvw4Ve1Xusm3A,4057
-TimeMurmur/builder/Transformer.py,sha256=P4R-Y1nsqq5bu_Dphf7NbqAnv_sO06q-PsLxBnnd6BQ,6802
+TimeMurmur/builder/Transformer.py,sha256=RRFMBuqVej9g7q5UU20d0lai1hKJV4fob8SUO4FZX2g,8422
 TimeMurmur/builder/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
 TimeMurmur/utils/FeatureExtraction.py,sha256=Pnre_KgFoQi5VrVyA6SH_JlJ4GCnfHtRNvCrjB-RePY,9607
 TimeMurmur/utils/__init__.py,sha256=M9j1NuexsmXRO0O_LLRNrLF7fEfPAHWIxm90hC6gV3I,27
 TimeMurmur/utils/series_utils.py,sha256=P153Yi_P4DhpjFPnIRJPfu6OYHT3pe7BUnHKHzdzfWk,1184
-TimeMurmur/utils/utility_functions.py,sha256=NxVcWUOQym4RF5fUGsEnCOBdfRCcmi01_cq_X5MPfbY,3098
-TimeMurmur-0.1.0.dist-info/LICENSE,sha256=PKTFxJnxvWRgz1XsBhnki8iKznsV6TFyR305UdmXoeY,1088
-TimeMurmur-0.1.0.dist-info/METADATA,sha256=5rVkjYBUzB9cN3ZJBwA7_TDLbyUAC60slFxbsTvaKuQ,4691
-TimeMurmur-0.1.0.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
-TimeMurmur-0.1.0.dist-info/top_level.txt,sha256=P2PDgK20ZHd9EywUDWXFr6a1710Uc0hiGLEtHGdaXhk,11
-TimeMurmur-0.1.0.dist-info/RECORD,,
+TimeMurmur/utils/utility_functions.py,sha256=ie1pRCLwSijQJS_MoY8J7BcgExmh809aVtWNDSkJYWs,3110
+TimeMurmur-0.1.1.dist-info/LICENSE,sha256=PKTFxJnxvWRgz1XsBhnki8iKznsV6TFyR305UdmXoeY,1088
+TimeMurmur-0.1.1.dist-info/METADATA,sha256=BhVLrdUBtviXM72bQNS2IKQlMZctV1ra4oCFItwdOMk,4715
+TimeMurmur-0.1.1.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
+TimeMurmur-0.1.1.dist-info/top_level.txt,sha256=P2PDgK20ZHd9EywUDWXFr6a1710Uc0hiGLEtHGdaXhk,11
+TimeMurmur-0.1.1.dist-info/RECORD,,
```

